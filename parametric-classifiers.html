<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>11 Parametric classifiers | UBC Stat 406 Worksheets</title>
<meta name="author" content="Daniel J. McDonald and Matías Salibán-Barrera">
<meta name="description" content="As we discussed in class, what is commonly referred to as classification can be thought of as prediction, when the responses are classes and we use a particular loss function (the 0-1 loss we...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="11 Parametric classifiers | UBC Stat 406 Worksheets">
<meta property="og:type" content="book">
<meta property="og:url" content="https://ubc-stat.github.io/stat-406-worksheets/parametric-classifiers.html">
<meta property="og:description" content="As we discussed in class, what is commonly referred to as classification can be thought of as prediction, when the responses are classes and we use a particular loss function (the 0-1 loss we...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="11 Parametric classifiers | UBC Stat 406 Worksheets">
<meta name="twitter:description" content="As we discussed in class, what is commonly referred to as classification can be thought of as prediction, when the responses are classes and we use a particular loss function (the 0-1 loss we...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.0/transition.js"></script><script src="libs/bs3compat-0.3.0/tabs.js"></script><script src="libs/bs3compat-0.3.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">UBC Stat 406 Worksheets</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Stat 406 Worksheets</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Module 0 – Review</li>
<li><a class="" href="predictions-using-a-linear-model.html"><span class="header-section-number">1</span> Predictions using a linear model</a></li>
<li class="book-part">Module 1 – Model Selection</li>
<li><a class="" href="predictions-using-a-linear-model-continued.html"><span class="header-section-number">2</span> Predictions using a linear model (continued)</a></li>
<li><a class="" href="cross-validation-concerns.html"><span class="header-section-number">3</span> Cross-validation concerns</a></li>
<li><a class="" href="comparing-models.html"><span class="header-section-number">4</span> Comparing models</a></li>
<li class="book-part">Module 2 – Regression</li>
<li><a class="" href="ridge-regression.html"><span class="header-section-number">5</span> Ridge regression</a></li>
<li><a class="" href="lasso.html"><span class="header-section-number">6</span> LASSO</a></li>
<li><a class="" href="non-parametric-regression.html"><span class="header-section-number">7</span> Non-parametric regression</a></li>
<li><a class="" href="kernel-regression-local-regression.html"><span class="header-section-number">8</span> Kernel regression / local regression</a></li>
<li><a class="" href="regression-trees.html"><span class="header-section-number">9</span> Regression trees</a></li>
<li><a class="" href="pruning-regression-trees-with-rpart.html"><span class="header-section-number">10</span> Pruning regression trees with rpart</a></li>
<li class="book-part">Module 3 – Classification</li>
<li><a class="active" href="parametric-classifiers.html"><span class="header-section-number">11</span> Parametric classifiers</a></li>
<li><a class="" href="qda.html"><span class="header-section-number">12</span> QDA</a></li>
<li><a class="" href="classification-trees.html"><span class="header-section-number">13</span> Classification Trees</a></li>
<li class="book-part">Module 4 – Modern techniques</li>
<li><a class="" href="bagging-for-regression.html"><span class="header-section-number">14</span> Bagging for regression</a></li>
<li><a class="" href="bagging-for-classification.html"><span class="header-section-number">15</span> Bagging for classification</a></li>
<li><a class="" href="random-forests.html"><span class="header-section-number">16</span> Random Forests</a></li>
<li><a class="" href="boosting-a-statistical-learning-perspective.html"><span class="header-section-number">17</span> Boosting (a Statistical Learning perspective)</a></li>
<li><a class="" href="what-is-adaboost-doing-really.html"><span class="header-section-number">18</span> What is Adaboost doing, really?</a></li>
<li><a class="" href="single-layer-neural-network.html"><span class="header-section-number">19</span> Single layer neural network</a></li>
<li class="book-part">Module 5 – Unsupervised learning</li>
<li><a class="" href="introduction.html"><span class="header-section-number">20</span> Introduction</a></li>
<li><a class="" href="clustering.html"><span class="header-section-number">21</span> Clustering</a></li>
<li><a class="" href="model-based-clustering.html"><span class="header-section-number">22</span> Model based clustering</a></li>
<li><a class="" href="hierarchical-clustering.html"><span class="header-section-number">23</span> Hierarchical clustering</a></li>
<li><a class="" href="references.html">References</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="alt-pca.html"><span class="header-section-number">A</span> PCA and alternating regression</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/ubc-stat/stat-406-worksheets">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="parametric-classifiers" class="section level1">
<h1>
<span class="header-section-number">11</span> Parametric classifiers<a class="anchor" aria-label="anchor" href="#parametric-classifiers"><i class="fas fa-link"></i></a>
</h1>
<p>As we discussed in class, what is commonly referred to as <em>classification</em>
can be thought of as prediction, when the responses are classes and we
use a particular loss function (the <em>0-1 loss</em> we discussed in class).
Furthermore, it is easy to show (which we did in class) that the optimal
classifier (in terms of minimizing the
expected misclassification error) is the one that assigns an observation
to the class with the highest probability of occuring, conditional to the
value of the observed explanatory variables.</p>
<!-- A related discussion about including costs of misclassification  -->
<!-- and the difference between prediction and classification can be found here: [http://www.fharrell.com/post/classification/](http://www.fharrell.com/post/classification/). -->
<p>Most (if not all) classification methods we will cover in this course can be
simply thought of as different approaches to estimate the conditional probability of
each class, conditional on the value of the explanatory variables. In
symbols: <code>P( G = g | X = x_0)</code>.
The obvious parallel with what we have done before in this class,
is that many (all?) regression methods we discussed in class are
different ways of estimating the conditional mean of the response
variable (conditional on the value of the explanatory
variables). Here we are
in fact estimating the whole conditional distribution of <code>G</code>
given <code>X = x_0</code>;
in symbols: <code>G | X = x_0</code>.</p>
<p>As in the regression case, there are different ways to estimate this
optimal predictor / classifier. Some will be model-based, some will
be non-parametric in nature. And some can be considered “restricted”
non-parametric methods (without relying on a model, but imposing some
other type of constrain on the shape of the classifier). The equivalent
methods for regression with continuous responses are: linear
or non-linear regression as model-based methods; kernel or local regression
as non-parametric methods; and splines or regression trees as
“constrained” (regularized?) non-parametric methods.</p>
<p>Below we first discuss model-based methods (Linear / Quadratic
Discriminant Analysis and logistic regression). non-parametric methods (nearest-neighbours and
classification trees) will be discussed later.</p>
<div id="linear-discriminant-analysis" class="section level2">
<h2>
<span class="header-section-number">11.1</span> Linear Discriminant Analysis<a class="anchor" aria-label="anchor" href="#linear-discriminant-analysis"><i class="fas fa-link"></i></a>
</h2>
<p>Probably the “second easiest approach”to estimate the above probability
(what would be the easiest one?) is to model the distribution of
the explanatory variables <strong>within</strong> each class (that is, to
model the distribution of <code>X | G = g</code>
for each possible class <code>g</code>).
These conditional distributions will then uniquely determine the
probabilities we need to estimate, as discussed above and in class.
In particular, one the simplest models we can use
for <code>X | G = g</code>
is a Normal (Gaussian) multivariate distribution.
As we saw in class, if we assume that the distribution of the features
for each class is Gaussian with a common covariance matrix across clases, then
it easy to show (<strong>and I strongly suggest that you do it</strong>) that the optimal
classifier (using the 0-1 loss function mentioned above) is a linear
function of the explanatory variables. The coefficients of this linear
function depend on the parameters of the assumed Gaussian distributions,
which can be estimated using MLE on the training set. Plugging these
parameter estimates in <code>P( G = g | X)</code>
provides a natural estimator of each of these conditional probabilities,
and thus we can compute an approximation to the optimal classifier.</p>
<p>The function <code>lda</code> in
the <code>MASS</code> library implements this simple classifier. We illustrate it
here on the rather simple and well-known vaso constriction data, available in the
<code>robustbase</code> package. More details, as usual, can be found on its
help page. The response variable takes two values (represented below
as <strong>blue</strong> and <strong>red</strong>), and there are only two
explanatory variables (which allows us to visualize our methods and results).</p>
<div class="sourceCode" id="cb141"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">vaso</span>, package <span class="op">=</span> <span class="st">"robustbase"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">Volume</span> <span class="op">~</span> <span class="va">Rate</span>,
  data <span class="op">=</span> <span class="va">vaso</span>, pch <span class="op">=</span> <span class="fl">19</span>, cex <span class="op">=</span> <span class="fl">1.5</span>,
  col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"red"</span>, <span class="st">"blue"</span><span class="op">)</span><span class="op">[</span><span class="va">Y</span> <span class="op">+</span> <span class="fl">1</span><span class="op">]</span>
<span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="30-lda-logit_files/figure-html/lda1-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>To train the LDA classifier we use the function <code>lda</code> as follows (note the
<strong>model-like</strong> syntax to indicate the response and explanatory variables):</p>
<div class="sourceCode" id="cb142"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span>
<span class="va">a.lda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">Volume</span> <span class="op">+</span> <span class="va">Rate</span>, data <span class="op">=</span> <span class="va">vaso</span><span class="op">)</span></code></pre></div>
<p>Now, given any value of the explanatory variables <code>(Volume, Rate)</code> we
can use the method <code>predict</code> on the object returned by <code><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda()</a></code> to
estimate the conditional probabilities of <strong>blue</strong> and <strong>red</strong>.</p>
<p>To visualize which regions of the feature space will be predicted to
contain <strong>blue</strong> points (and then obviously which areas will be
predicted to correspond to <strong>red</strong> responses) we will
construct a relatively fine 2-dimensional grid of posible values of
the explanatory variables (<code>(Volume, Rate)</code>):</p>
<div class="sourceCode" id="cb143"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">xvol</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">4</span>, length <span class="op">=</span> <span class="fl">200</span><span class="op">)</span>
<span class="va">xrat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">4</span>, length <span class="op">=</span> <span class="fl">200</span><span class="op">)</span>
<span class="va">the.grid</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/expand.grid.html">expand.grid</a></span><span class="op">(</span><span class="va">xvol</span>, <span class="va">xrat</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="va">the.grid</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Volume"</span>, <span class="st">"Rate"</span><span class="op">)</span></code></pre></div>
<p>and estimate the probabilities of the 2 classes for each point in this grid:</p>
<div class="sourceCode" id="cb144"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pr.lda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">a.lda</span>, newdata <span class="op">=</span> <span class="va">the.grid</span><span class="op">)</span><span class="op">$</span><span class="va">posterior</span></code></pre></div>
<p>Finally, we plot the corresponding “surface” of predictions for one class
(i.e. the conditional probabilites for that class as a function of
the explanatory variables):</p>
<div class="sourceCode" id="cb145"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/image.html">image</a></span><span class="op">(</span><span class="va">xrat</span>, <span class="va">xvol</span>, <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">pr.lda</span><span class="op">[</span>, <span class="fl">2</span><span class="op">]</span>, <span class="fl">200</span>, <span class="fl">200</span><span class="op">)</span>,
  col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/grDevices/palettes.html">terrain.colors</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>,
  ylab <span class="op">=</span> <span class="st">"Volume"</span>, xlab <span class="op">=</span> <span class="st">"Rate"</span>, main <span class="op">=</span> <span class="st">"LDA"</span>
<span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">Volume</span> <span class="op">~</span> <span class="va">Rate</span>,
  data <span class="op">=</span> <span class="va">vaso</span>, pch <span class="op">=</span> <span class="fl">19</span>, cex <span class="op">=</span> <span class="fl">1.5</span>,
  col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"red"</span>, <span class="st">"blue"</span><span class="op">)</span><span class="op">[</span><span class="va">Y</span> <span class="op">+</span> <span class="fl">1</span><span class="op">]</span>
<span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="30-lda-logit_files/figure-html/lda1.2-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>On the plot above higher numbers are shown with lighther colors
(dark green corresponds to very low conditional probabilities).</p>
<div id="further-considerations" class="section level3">
<h3>
<span class="header-section-number">11.1.1</span> Further considerations<a class="anchor" aria-label="anchor" href="#further-considerations"><i class="fas fa-link"></i></a>
</h3>
<p>This model-based approach to classification (LDA) is optimal
if the model is correct. The strongest assumption of this model
is, of course, the Gaussian conditional distribution of the vector of
explanatory variables: <code>X | G = g</code> has a <code>N( mu, Sigma)</code> distribution.
The second strongest assumption is
that of equal “shape” (in other words, that the covariance matrix
<code>Sigma</code> above does not depend on <code>g</code>). This latter assumption
can be relaxed slightly if
we assume instead that the features have a Gaussian distribution within each
class, but that the covariance matrix may be different across
classes.
In symbols, if we assume that
<code>X | G = g</code> has a <code>N( mu, Sigma_g)</code> distribution.
The corresponding optimal classifier is now a quadratic
function of the predictors (<strong>prove it!</strong>). The function <code>qda</code>
in the <code>MASS</code> library implements this classifier, and it
can be used just like <code>lda</code> (as usual, refer to its help page for details).</p>
<p>This approach can be used with any number of classes. Can you think of any limitations?</p>
</div>
</div>
<div id="logistic-regression-review" class="section level2">
<h2>
<span class="header-section-number">11.2</span> Logistic regression (Review)<a class="anchor" aria-label="anchor" href="#logistic-regression-review"><i class="fas fa-link"></i></a>
</h2>
<p>If we model the distribution of the features within each class using a
multivariate Gaussian distribution, then it is easy to see that the
boundaries between classes are linear functions of the features (<strong>verify this!</strong>)
Furthermore, the log of the odds ratio between classes is a linear
function. It is interesting to note that one can start with this last
assumption (instead of the full Gaussian model) and arrive at a
fully parametric model for the conditional distibution of the classes
given the features (see the class slides). The parameters can be
estimated using maximum likelihood. For two classes this is the
logistic regression model, which you may have seen in previous
courses.</p>
<p>We illustrate this on the <code>vaso</code> data as before. Since this is
a 2-class problem, we just need to fit a logistic regression model.
The function <code>glm</code> in <code>R</code> does it for us, we specify that we
want to fit such a model using the argument <code>family=binomial</code>.
Once we obtain parameter estimators (in the <code>glm</code> object <code>a</code> below),
we use the <code>predict</code> method to obtain predicted conditional
probabilities on the same grid we used before:</p>
<div class="sourceCode" id="cb146"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Y</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">vaso</span>, family <span class="op">=</span> <span class="va">binomial</span><span class="op">)</span>
<span class="va">pr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">a</span>, newdata <span class="op">=</span> <span class="va">the.grid</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></code></pre></div>
<p>We now plot the data and the <em>surface</em> of predicted probabilities for
blue points (higher probabilites are displayed with lighter colors).</p>
<div class="sourceCode" id="cb147"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/image.html">image</a></span><span class="op">(</span><span class="va">xrat</span>, <span class="va">xvol</span>, <span class="fu"><a href="https://rdrr.io/r/base/matrix.html">matrix</a></span><span class="op">(</span><span class="va">pr</span>, <span class="fl">200</span>, <span class="fl">200</span><span class="op">)</span>,
  col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/grDevices/palettes.html">terrain.colors</a></span><span class="op">(</span><span class="fl">100</span><span class="op">)</span>,
  ylab <span class="op">=</span> <span class="st">"Volume"</span>, xlab <span class="op">=</span> <span class="st">"Rate"</span>, main <span class="op">=</span> <span class="st">"Logistic"</span>
<span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/points.html">points</a></span><span class="op">(</span><span class="va">Volume</span> <span class="op">~</span> <span class="va">Rate</span>,
  data <span class="op">=</span> <span class="va">vaso</span>, pch <span class="op">=</span> <span class="fl">19</span>, cex <span class="op">=</span> <span class="fl">1.5</span>,
  col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"red"</span>, <span class="st">"blue"</span><span class="op">)</span><span class="op">[</span><span class="va">Y</span> <span class="op">+</span> <span class="fl">1</span><span class="op">]</span>
<span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="30-lda-logit_files/figure-html/logistic2-1.png" width="90%" style="display: block; margin: auto;"></div>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="pruning-regression-trees-with-rpart.html"><span class="header-section-number">10</span> Pruning regression trees with rpart</a></div>
<div class="next"><a href="qda.html"><span class="header-section-number">12</span> QDA</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#parametric-classifiers"><span class="header-section-number">11</span> Parametric classifiers</a></li>
<li>
<a class="nav-link" href="#linear-discriminant-analysis"><span class="header-section-number">11.1</span> Linear Discriminant Analysis</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#further-considerations"><span class="header-section-number">11.1.1</span> Further considerations</a></li></ul>
</li>
<li><a class="nav-link" href="#logistic-regression-review"><span class="header-section-number">11.2</span> Logistic regression (Review)</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/ubc-stat/stat-406-worksheets/blob/main/30-lda-logit.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/ubc-stat/stat-406-worksheets/edit/main/30-lda-logit.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>UBC Stat 406 Worksheets</strong>" was written by Daniel J. McDonald and Matías Salibán-Barrera. It was last built on 2021-09-17.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
