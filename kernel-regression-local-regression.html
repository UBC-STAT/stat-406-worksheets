<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>8 Kernel regression / local regression | UBC Stat 406 Worksheets</title>
<meta name="author" content="Daniel J. McDonald and Matías Salibán-Barrera">
<meta name="description" content="A different approach to estimating a regression function is based on recalling that the true regression function is f(a) = E(Y | X = a), the mean of the response variable Y conditional to the...">
<meta name="generator" content="bookdown 0.23 with bs4_book()">
<meta property="og:title" content="8 Kernel regression / local regression | UBC Stat 406 Worksheets">
<meta property="og:type" content="book">
<meta property="og:description" content="A different approach to estimating a regression function is based on recalling that the true regression function is f(a) = E(Y | X = a), the mean of the response variable Y conditional to the...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="8 Kernel regression / local regression | UBC Stat 406 Worksheets">
<meta name="twitter:description" content="A different approach to estimating a regression function is based on recalling that the true regression function is f(a) = E(Y | X = a), the mean of the response variable Y conditional to the...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">UBC Stat 406 Worksheets</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li class="book-part">Module 0 – Review</li>
<li><a class="" href="predictions-using-a-linear-model.html"><span class="header-section-number">1</span> Predictions using a linear model</a></li>
<li class="book-part">Module 1 – Model Selection</li>
<li><a class="" href="predictions-using-a-linear-model-continued.html"><span class="header-section-number">2</span> Predictions using a linear model (continued)</a></li>
<li><a class="" href="cross-validation-concerns.html"><span class="header-section-number">3</span> Cross-validation concerns</a></li>
<li><a class="" href="comparing-models.html"><span class="header-section-number">4</span> Comparing models</a></li>
<li class="book-part">Module 2 – Regression</li>
<li><a class="" href="ridge-regression.html"><span class="header-section-number">5</span> Ridge regression</a></li>
<li><a class="" href="lasso.html"><span class="header-section-number">6</span> LASSO</a></li>
<li><a class="" href="non-parametric-regression.html"><span class="header-section-number">7</span> Non-parametric regression</a></li>
<li><a class="active" href="kernel-regression-local-regression.html"><span class="header-section-number">8</span> Kernel regression / local regression</a></li>
<li><a class="" href="regression-trees.html"><span class="header-section-number">9</span> Regression trees</a></li>
<li><a class="" href="pruning-regression-trees-with-rpart.html"><span class="header-section-number">10</span> Pruning regression trees with rpart</a></li>
<li class="book-part">Module 3 – Classification</li>
<li><a class="" href="parametric-classifiers.html"><span class="header-section-number">11</span> Parametric classifiers</a></li>
<li><a class="" href="qda.html"><span class="header-section-number">12</span> QDA</a></li>
<li><a class="" href="classification-trees.html"><span class="header-section-number">13</span> Classification Trees</a></li>
<li><a class="" href="bagging-for-regression.html"><span class="header-section-number">14</span> Bagging for regression</a></li>
<li><a class="" href="bagging-for-classification.html"><span class="header-section-number">15</span> Bagging for classification</a></li>
<li><a class="" href="random-forests.html"><span class="header-section-number">16</span> Random Forests</a></li>
<li><a class="" href="boosting-a-statistical-learning-perspective.html"><span class="header-section-number">17</span> Boosting (a Statistical Learning perspective)</a></li>
<li><a class="" href="what-is-adaboost-doing-really.html"><span class="header-section-number">18</span> What is Adaboost doing, really?</a></li>
<li><a class="" href="single-layer-neural-network.html"><span class="header-section-number">19</span> Single layer neural network</a></li>
<li class="book-part">Module 5 – Unsupervised learning</li>
<li><a class="" href="introduction.html"><span class="header-section-number">20</span> Introduction</a></li>
<li><a class="" href="clustering.html"><span class="header-section-number">21</span> Clustering</a></li>
<li><a class="" href="model-based-clustering.html"><span class="header-section-number">22</span> Model based clustering</a></li>
<li><a class="" href="hierarchical-clustering.html"><span class="header-section-number">23</span> Hierarchical clustering</a></li>
<li><a class="" href="references.html">References</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="alt-pca.html"><span class="header-section-number">A</span> PCA and alternating regression</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="kernel-regression-local-regression" class="section level1">
<h1>
<span class="header-section-number">8</span> Kernel regression / local regression<a class="anchor" aria-label="anchor" href="#kernel-regression-local-regression"><i class="fas fa-link"></i></a>
</h1>
<p>A different approach to estimating a regression function is
based on recalling that the true regression function
is <em>f(a) = E(Y | X = a)</em>, the mean of the response variable <em>Y</em> <strong>conditional</strong>
to the event that the explanatory variable(s) <strong>X</strong> equal(s) <strong>a</strong>. If we
had lots of data, we could, in principle, think of the following
intuitively simple regression estimator: given <strong>c</strong>, consider all
observations (Y, <strong>X</strong>) in your training set that have <strong>X = c</strong>, and take
our estimated <em>f(c)</em> as
the average of the corresponding observed values of the response variable
Y. This would be a resonable estimator for E(Y | <strong>X</strong> = <strong>c</strong> ) (if
we had sufficient cases in our training data pairs for which <strong>X</strong> = <strong>c</strong>).</p>
<p>Although the simple approach above does not usually work in practice (because
we do not have enough training points with <strong>X</strong> = <strong>c</strong> for many values of <strong>c</strong>),
the idea can still be used to construct a regression estimator that works
<strong>locally</strong>, i.e. that given <strong>c</strong> uses the points in the training set
that have <strong>X close to c</strong> (you can think of this as <em>working in a neighbourhood</em> of
<strong>c</strong>). This family of regression estimators is called
<em>local regression</em>, or <em>kernel regression</em>. The latter name is based
on the fact that we will use a specific
family of functions (called kernels) to define which points
are <em>neighbours</em> and
how they will be used to estimate the regression function.
Note that these <em>kernel functions</em> are different from those used in
Support Vector Machines and other reproducible kernel Hilbert spaces methods.</p>
<p>Probably the simplest kernel regression estimator is to simply take
the average of the responses of the training points where the explanatory
variables are within <em>h</em> of the point of interest. This “window width” <em>h</em>
is called the <em>bandwidth</em>. We can use the function
<code>ksmooth</code> in package <code>KernSmooth</code> in <code>R</code> to do this (<strong>but it would be
a great exercise to write your own <code>R</code> function to do it</strong>). The code below
considers one specific explanatory variable for the air pollution data
(just for illustration purposes) and fits a local averages regression
estimator, with bandwidth equal to 50:</p>
<div class="sourceCode" id="cb95"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">dat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"data/rutgers-lib-30861_CSV-1.csv"</span>, header <span class="op">=</span> <span class="cn">TRUE</span>, sep <span class="op">=</span> <span class="st">","</span><span class="op">)</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">KernSmooth</span><span class="op">)</span>
<span class="va">x</span> <span class="op">&lt;-</span> <span class="va">dat</span><span class="op">$</span><span class="va">SO.</span>
<span class="va">y</span> <span class="op">&lt;-</span> <span class="va">dat</span><span class="op">$</span><span class="va">MORT</span>
<span class="va">h</span> <span class="op">&lt;-</span> <span class="fl">50</span>
<span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/ksmooth.html">ksmooth</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span>, kernel <span class="op">=</span> <span class="st">"box"</span>, bandwidth <span class="op">=</span> <span class="va">h</span>, n.points <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="st">"gray"</span>, cex <span class="op">=</span> <span class="fl">1.3</span>, xlab <span class="op">=</span> <span class="st">"SO."</span>, ylab <span class="op">=</span> <span class="st">"MORT"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">a</span><span class="op">$</span><span class="va">x</span>, <span class="va">a</span><span class="op">$</span><span class="va">y</span>, lwd <span class="op">=</span> <span class="fl">4</span>, col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="23-kernel-regression_files/figure-html/kernel0-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Note the gap in the estimated regression function. Why do you think
this happened?</p>
<p>If we increase the bandwidth from 50 to 60 we obtain the following estimated
regression function:</p>
<div class="sourceCode" id="cb96"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">h</span> <span class="op">&lt;-</span> <span class="fl">60</span>
<span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/ksmooth.html">ksmooth</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span>, kernel <span class="op">=</span> <span class="st">"box"</span>, bandwidth <span class="op">=</span> <span class="va">h</span>, n.points <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="st">"gray"</span>, cex <span class="op">=</span> <span class="fl">1.3</span>, xlab <span class="op">=</span> <span class="st">"SO."</span>, ylab <span class="op">=</span> <span class="st">"MORT"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">a</span><span class="op">$</span><span class="va">x</span>, <span class="va">a</span><span class="op">$</span><span class="va">y</span>, lwd <span class="op">=</span> <span class="fl">4</span>, col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="23-kernel-regression_files/figure-html/kernel0.1-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>This fit is still rather unsatisfactory. For example, note
how it looks like a <em>staircase</em>. The estimated regression curve
is fairly jagged, which is usually not how we expect the true regression
function to be. Can you explain why the above regression estimator looks like this?</p>
<p>As discussed in class, using a smoother kernel function
results in a smoother estimated regression function. The plot
below uses the same bandwidth as before, but the kernel function is
the standard gaussian density:</p>
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">h</span> <span class="op">&lt;-</span> <span class="fl">60</span>
<span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/ksmooth.html">ksmooth</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span>, kernel <span class="op">=</span> <span class="st">"normal"</span>, bandwidth <span class="op">=</span> <span class="va">h</span>, n.points <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="st">"gray"</span>, cex <span class="op">=</span> <span class="fl">1.3</span>, xlab <span class="op">=</span> <span class="st">"SO."</span>, ylab <span class="op">=</span> <span class="st">"MORT"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">a</span><span class="op">$</span><span class="va">x</span>, <span class="va">a</span><span class="op">$</span><span class="va">y</span>, lwd <span class="op">=</span> <span class="fl">4</span>, col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="23-kernel-regression_files/figure-html/kernel0.2-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Better properties for the estimated regression function are obtained
when one uses a smooth kernel with <em>compact support</em> (the support of
the gaussian density function is the whole real line and thus not
compact). The reasons for this (better kernel regression estimators
when the kernel has compact support) are rather technical and will
not be discussed here. A good technical reference for these topics is
the following, which is available on-line via the Library:</p>
<blockquote>
<p>Nonparametric and Semiparametric Models. (2004).
Hardle, W., Werwatz, A., Muller, M. and Sperlich, S.
Springer-Verlag Berlin Heidelberg.
DOI: <a href="http://doi.org/10.1007/978-3-642-17146-8">10.1007/978-3-642-17146-8</a></p>
</blockquote>
<p>In what follows we will use the <code>R</code> function <code>loess</code>
that implements this approach with a tri-cubic
kernel given by <em>k(a) = ( 1 - (|a|)^3 )^3</em> if <em>|a| &lt; 1</em>, and 0 otherwise.
The following plot compares this kernel with the gaussian one.
Since the important characteristics of a kernel are its shape and support set,
below I standardized both of them to reach the same maximum value (1):</p>
<div class="sourceCode" id="cb98"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">2</span>, <span class="fl">2</span>, length <span class="op">=</span> <span class="fl">100</span><span class="op">)</span>
<span class="va">tmp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html">dnorm</a></span><span class="op">(</span><span class="va">tt</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">tt</span>, <span class="va">tmp</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">tmp</span><span class="op">)</span>, ylab <span class="op">=</span> <span class="st">"Kernel"</span>, xlab <span class="op">=</span> <span class="st">""</span>, lwd <span class="op">=</span> <span class="fl">6</span>, type <span class="op">=</span> <span class="st">"l"</span>, col <span class="op">=</span> <span class="st">"gray40"</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span>
<span class="va">tmp</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">tt</span><span class="op">)</span><span class="op">^</span><span class="fl">3</span><span class="op">)</span><span class="op">^</span><span class="fl">3</span>
<span class="va">tmp</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">abs</a></span><span class="op">(</span><span class="va">tt</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">1</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fl">0</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">tt</span>, <span class="va">tmp</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">tmp</span><span class="op">)</span>, lwd <span class="op">=</span> <span class="fl">6</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="23-kernel-regression_files/figure-html/kernel.comp-1.png" width="90%" style="display: block; margin: auto;"></div>
<div id="fixed-versus-variable-bandwidths" class="section level3">
<h3>
<span class="header-section-number">8.0.1</span> Fixed versus variable bandwidths<a class="anchor" aria-label="anchor" href="#fixed-versus-variable-bandwidths"><i class="fas fa-link"></i></a>
</h3>
<p>As we discussed in class, fixed bandwidths may present problems in practice
when the density of the observed explanatory variables is not uniform
(i.e. there are <em>dense</em> regions where we have more observations and
<em>sparse</em> regions where there are fewer observations). A solution
to this is to use <em>variable bandwidths</em>, where at each point <em>c</em> we
take a bandwidth large enough to contain a pre-specified proportion
<em>alpha</em> of the data. The function <code>loess</code> implements this approach,
the desired proportion of observations in each neighbourhood is given
by the argument <code>span</code>.</p>
<p>When we apply this method (with <code>span = 0.5</code>, and <code>degree = 0</code> to indicate
we are using <em>local averages</em>) to the example above, we get the following
fit:</p>
<div class="sourceCode" id="cb99"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/loess.html">loess</a></span><span class="op">(</span><span class="va">MORT</span> <span class="op">~</span> <span class="va">SO.</span>, data <span class="op">=</span> <span class="va">dat</span>, span <span class="op">=</span> <span class="fl">0.5</span>, degree <span class="op">=</span> <span class="fl">0</span>, family <span class="op">=</span> <span class="st">"gaussian"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">MORT</span> <span class="op">~</span> <span class="va">SO.</span>, data <span class="op">=</span> <span class="va">dat</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="st">"gray"</span>, cex <span class="op">=</span> <span class="fl">1.3</span>, xlab <span class="op">=</span> <span class="st">"SO."</span>, ylab <span class="op">=</span> <span class="st">"MORT"</span><span class="op">)</span>
<span class="va">tmp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="va">a</span><span class="op">$</span><span class="va">x</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">a</span><span class="op">$</span><span class="va">x</span><span class="op">[</span><span class="va">tmp</span><span class="op">]</span>, <span class="va">a</span><span class="op">$</span><span class="va">fitted</span><span class="op">[</span><span class="va">tmp</span><span class="op">]</span>, lwd <span class="op">=</span> <span class="fl">4</span>, col <span class="op">=</span> <span class="st">"steelblue"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="23-kernel-regression_files/figure-html/loess.air-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Note, in particular, how the upper end of the estimated regression function
looks better than the approach discussed before using fixed bandwidths.</p>
<p>Although we have not yet discussed how to choose a bandwidth (either fixed
or variable) among the infinitely many possible ones, I expect you
to already know how this may be done.</p>
</div>
<div id="local-regression-versus-local-means" class="section level3">
<h3>
<span class="header-section-number">8.0.2</span> Local regression versus local means<a class="anchor" aria-label="anchor" href="#local-regression-versus-local-means"><i class="fas fa-link"></i></a>
</h3>
<p>As discussed in more detail in class, a better way to exploit the
approximating properties of a Taylor expansion, is to use it locally.
In particular, using kernels as above, we can estimate the regression
function using a linear function <em>locally</em> (corresponding to a
Taylor expansion of order 1), or a quadratic function (expansion of
order 2), etc. We will illustrate these points on the <code>ethanol</code>
data in package <code>SemiPar</code>. As usual, information about the data
can be found on its help page.</p>
<p>Below we load the data and compute a <em>local constant</em> (<code>degree = 0</code>) <em>regression
estimator</em>, where the response variable is <code>NOx</code> and the explanatory variable is
<code>E</code>. The span was arbitrarily set to 0.40 (but this will discussed in
more detail below).</p>
<div class="sourceCode" id="cb100"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">ethanol</span>, package <span class="op">=</span> <span class="st">"SemiPar"</span><span class="op">)</span>
<span class="co"># local constant</span>
<span class="va">span</span> <span class="op">&lt;-</span> <span class="fl">.4</span>
<span class="va">b0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/loess.html">loess</a></span><span class="op">(</span><span class="va">NOx</span> <span class="op">~</span> <span class="va">E</span>, data <span class="op">=</span> <span class="va">ethanol</span>, span <span class="op">=</span> <span class="va">span</span>, degree <span class="op">=</span> <span class="fl">0</span>, family <span class="op">=</span> <span class="st">"gaussian"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">NOx</span> <span class="op">~</span> <span class="va">E</span>, data <span class="op">=</span> <span class="va">ethanol</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="st">"gray"</span>, cex <span class="op">=</span> <span class="fl">1.3</span>, xlab <span class="op">=</span> <span class="st">"E"</span>, ylab <span class="op">=</span> <span class="st">"NOx"</span><span class="op">)</span>
<span class="va">tmp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="va">b0</span><span class="op">$</span><span class="va">x</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">b0</span><span class="op">$</span><span class="va">x</span><span class="op">[</span><span class="va">tmp</span><span class="op">]</span>, <span class="va">b0</span><span class="op">$</span><span class="va">fitted</span><span class="op">[</span><span class="va">tmp</span><span class="op">]</span>, lwd <span class="op">=</span> <span class="fl">4</span>, col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="23-kernel-regression_files/figure-html/kernel0.3-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Note how this regression estimator tends to not fit well the <em>tails</em> of the data
(i.e. the smallest and largest observed values of <code>E</code>). A better fit
is obtained with a <em>locally linear</em> estimator (<code>degree = 1</code>), shown below in red, over the
<em>locally constant</em> one (in blue):</p>
<div class="sourceCode" id="cb101"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># local linear</span>
<span class="va">span</span> <span class="op">&lt;-</span> <span class="fl">.4</span>
<span class="va">b1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/loess.html">loess</a></span><span class="op">(</span><span class="va">NOx</span> <span class="op">~</span> <span class="va">E</span>, data <span class="op">=</span> <span class="va">ethanol</span>, span <span class="op">=</span> <span class="va">span</span>, degree <span class="op">=</span> <span class="fl">1</span>, family <span class="op">=</span> <span class="st">"gaussian"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">NOx</span> <span class="op">~</span> <span class="va">E</span>, data <span class="op">=</span> <span class="va">ethanol</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="st">"gray"</span>, cex <span class="op">=</span> <span class="fl">1.3</span>, xlab <span class="op">=</span> <span class="st">"E"</span>, ylab <span class="op">=</span> <span class="st">"NOx"</span><span class="op">)</span>
<span class="va">tmp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="va">b1</span><span class="op">$</span><span class="va">x</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">b1</span><span class="op">$</span><span class="va">x</span><span class="op">[</span><span class="va">tmp</span><span class="op">]</span>, <span class="va">b1</span><span class="op">$</span><span class="va">fitted</span><span class="op">[</span><span class="va">tmp</span><span class="op">]</span>, lwd <span class="op">=</span> <span class="fl">4</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">b0</span><span class="op">$</span><span class="va">x</span><span class="op">[</span><span class="va">tmp</span><span class="op">]</span>, <span class="va">b0</span><span class="op">$</span><span class="va">fitted</span><span class="op">[</span><span class="va">tmp</span><span class="op">]</span>, lwd <span class="op">=</span> <span class="fl">4</span>, col <span class="op">=</span> <span class="st">"blue"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="23-kernel-regression_files/figure-html/kernel0.4-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>This fit is an improvement from the previous one, but it does not
capture well the <em>peak</em> of the data (around <code>E</code> = 0.90). It is easy to see that
a quadratic local fit might be able to do this, without affecting the
quality of the fit elsewhere. Below we compare the locally linear (red) and
locally quadratic (dark green) fits:</p>
<div class="sourceCode" id="cb102"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># local quad</span>
<span class="va">span</span> <span class="op">&lt;-</span> <span class="fl">.4</span>
<span class="va">b2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/loess.html">loess</a></span><span class="op">(</span><span class="va">NOx</span> <span class="op">~</span> <span class="va">E</span>, data <span class="op">=</span> <span class="va">ethanol</span>, span <span class="op">=</span> <span class="va">span</span>, degree <span class="op">=</span> <span class="fl">2</span>, family <span class="op">=</span> <span class="st">"gaussian"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">NOx</span> <span class="op">~</span> <span class="va">E</span>, data <span class="op">=</span> <span class="va">ethanol</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="st">"gray"</span>, cex <span class="op">=</span> <span class="fl">1.3</span>, xlab <span class="op">=</span> <span class="st">"E"</span>, ylab <span class="op">=</span> <span class="st">"NOx"</span><span class="op">)</span>
<span class="va">tmp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/order.html">order</a></span><span class="op">(</span><span class="va">b2</span><span class="op">$</span><span class="va">x</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">b1</span><span class="op">$</span><span class="va">x</span><span class="op">[</span><span class="va">tmp</span><span class="op">]</span>, <span class="va">b1</span><span class="op">$</span><span class="va">fitted</span><span class="op">[</span><span class="va">tmp</span><span class="op">]</span>, lwd <span class="op">=</span> <span class="fl">4</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="va">b2</span><span class="op">$</span><span class="va">x</span><span class="op">[</span><span class="va">tmp</span><span class="op">]</span>, <span class="va">b2</span><span class="op">$</span><span class="va">fitted</span><span class="op">[</span><span class="va">tmp</span><span class="op">]</span>, lwd <span class="op">=</span> <span class="fl">4</span>, col <span class="op">=</span> <span class="st">"darkgreen"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="23-kernel-regression_files/figure-html/kernel0.5-1.png" width="90%" style="display: block; margin: auto;"></div>
</div>
<div id="choosing-the-bandwidth" class="section level3">
<h3>
<span class="header-section-number">8.0.3</span> Choosing the bandwidth<a class="anchor" aria-label="anchor" href="#choosing-the-bandwidth"><i class="fas fa-link"></i></a>
</h3>
<p>It is easy to see that the bandwidths plays a similar role to
the one played by the penalty term in smoothers based on
splines or other bases. A very small bandwidth results in
an estimator that is too adaptive to local quirks of the
training set. Similarly, a bandwidth that is too large
will result in an estimator that essentially fit a single
global model to the whole data set.</p>
<p>We illustrate the effect of different choices of
bandwidths below. We take a local quadratic (2nd degree
polynomial) fit, with a very small span (0.05):</p>
<div class="sourceCode" id="cb103"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tmp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/loess.html">loess</a></span><span class="op">(</span><span class="va">NOx</span> <span class="op">~</span> <span class="va">E</span>, data <span class="op">=</span> <span class="va">ethanol</span>, span <span class="op">=</span> <span class="fl">.05</span>, degree <span class="op">=</span> <span class="fl">2</span>, family <span class="op">=</span> <span class="st">"gaussian"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">NOx</span> <span class="op">~</span> <span class="va">E</span>, data <span class="op">=</span> <span class="va">ethanol</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="st">"gray"</span>, cex <span class="op">=</span> <span class="fl">1.5</span><span class="op">)</span>
<span class="co"># artificial grid of values to show predictions for the plot</span>
<span class="va">prs</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">ethanol</span>, <span class="fu"><a href="https://rdrr.io/r/base/seq.html">seq</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">min</a></span><span class="op">(</span><span class="va">E</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html">max</a></span><span class="op">(</span><span class="va">E</span><span class="op">)</span>, length <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">tmp</span>, newdata <span class="op">=</span> <span class="va">prs</span><span class="op">)</span> <span class="op">~</span> <span class="va">prs</span>, data <span class="op">=</span> <span class="va">ethanol</span>, lwd <span class="op">=</span> <span class="fl">4</span>, col <span class="op">=</span> <span class="st">"steelblue"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="23-kernel-regression_files/figure-html/kernel1-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Larger spans result in “better” fits, at least in the sense of
being more pleasant to the eye:</p>
<div class="sourceCode" id="cb104"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">tmp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/loess.html">loess</a></span><span class="op">(</span><span class="va">NOx</span> <span class="op">~</span> <span class="va">E</span>, data <span class="op">=</span> <span class="va">ethanol</span>, span <span class="op">=</span> <span class="fl">.25</span>, degree <span class="op">=</span> <span class="fl">2</span>, family <span class="op">=</span> <span class="st">"gaussian"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">NOx</span> <span class="op">~</span> <span class="va">E</span>, data <span class="op">=</span> <span class="va">ethanol</span>, pch <span class="op">=</span> <span class="fl">19</span>, col <span class="op">=</span> <span class="st">"gray"</span>, cex <span class="op">=</span> <span class="fl">1.5</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">tmp</span>, newdata <span class="op">=</span> <span class="va">prs</span><span class="op">)</span> <span class="op">~</span> <span class="va">prs</span>, data <span class="op">=</span> <span class="va">ethanol</span>, lwd <span class="op">=</span> <span class="fl">4</span>, col <span class="op">=</span> <span class="st">"hotpink"</span><span class="op">)</span>
<span class="va">tmp</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/loess.html">loess</a></span><span class="op">(</span><span class="va">NOx</span> <span class="op">~</span> <span class="va">E</span>, data <span class="op">=</span> <span class="va">ethanol</span>, span <span class="op">=</span> <span class="fl">.5</span>, degree <span class="op">=</span> <span class="fl">2</span>, family <span class="op">=</span> <span class="st">"gaussian"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/lines.html">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">tmp</span>, newdata <span class="op">=</span> <span class="va">prs</span><span class="op">)</span> <span class="op">~</span> <span class="va">prs</span>, data <span class="op">=</span> <span class="va">ethanol</span>, lwd <span class="op">=</span> <span class="fl">4</span>, col <span class="op">=</span> <span class="st">"darkgreen"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/legend.html">legend</a></span><span class="op">(</span><span class="st">"topleft"</span>, legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"span: 0.25"</span>, <span class="st">"span: 0.50"</span><span class="op">)</span>, col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"hotpink"</span>, <span class="st">"darkgreen"</span><span class="op">)</span>, lwd <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="23-kernel-regression_files/figure-html/kernel2-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>The range of sensible or acceptable values of the argument <code>span</code> in <code>loess</code>
is determined, of course, by the exact definition of this parameter. Information
can be found in the corresponding help page, as usual. As you probably know,
an “optimal” value of span could be chosen using cross-validation. A
couple of <strong>very good</strong> questions for you are the following:</p>
<ul>
<li>are kernel regression estimators <em>linear</em> in the sense of there being a
matrix <strong>S</strong> such that the fitted values equal <strong>S y</strong>, where <strong>y</strong> is the
vector of responses in the training set, and <strong>S</strong> does not depend on <strong>y</strong>?</li>
<li>use K-fold cross-validation to choose an “optimal” value of <code>span</code>.</li>
</ul>
</div>
<div id="the-problem-of-outliers-and-other-model-departures-1" class="section level2">
<h2>
<span class="header-section-number">8.1</span> The problem of outliers and other model departures<a class="anchor" aria-label="anchor" href="#the-problem-of-outliers-and-other-model-departures-1"><i class="fas fa-link"></i></a>
</h2>
<p>When the data may contain outliers and/or other atypical observations,
the estimation methods discussed above may be seriously affected, even
if there are only a few such aberrant data points in the training set
(possible outliers in the test / validation set are also a concern, but
we don’t have time to discuss it here). Some robust estimation
methods based on kernel smoothers exist. See for example <span class="citation">(Boente, Martínez, and Salibián-Barrera <a href="references.html#ref-BoenteMartinez2017" role="doc-biblioref">2017</a>)</span>
and references therein. This paper deals with a slightly more
complex model (additive model), but when only component exists, it
is the same model discussed in class. The <a href="https://cran.r-project.org/package=RBF">RBF</a> package
implementing this method is available from <a href="https://cran.r-project.org/package=RBF">CRAN</a>
and also <a href="https://github.com/msalibian/RBF">here</a>.</p>
<!-- Effect of the degree, now quadratic: -->
<!-- ```{r kernel3, fig.width=5, fig.height=5, message=FALSE, warning=FALSE} -->
<!-- tmp <- loess(NOx ~ E, data=ethanol, span = .5, degree=2, family='gaussian') -->
<!-- plot(NOx ~ E, data=ethanol, pch=19, col='gray', cex=1.5) -->
<!-- lines(predict(tmp, newdata=prs) ~ prs, data=ethanol, lwd=4, col='blue') -->
<!-- ``` -->
<!-- Now quadratic, span = 0.20 -->
<!-- ```{r kernel4, fig.width=5, fig.height=5, message=FALSE, warning=FALSE} -->
<!-- tmp <- loess(NOx ~ E, data=ethanol, span = .2, degree=2, family='gaussian') -->
<!-- plot(NOx ~ E, data=ethanol, pch=19, col='gray', cex=1.5) -->
<!-- lines(predict(tmp)[order(E)] ~ sort(E), data=ethanol, lwd=4, col='steelblue') -->
<!-- lines(predict(tmp, newdata=prs) ~ prs, data=ethanol, lwd=2, col='red2') -->
<!-- ``` -->
<!-- Kinks are artifact of sparsity of data -->

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="non-parametric-regression.html"><span class="header-section-number">7</span> Non-parametric regression</a></div>
<div class="next"><a href="regression-trees.html"><span class="header-section-number">9</span> Regression trees</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li>
<a class="nav-link" href="#kernel-regression-local-regression"><span class="header-section-number">8</span> Kernel regression / local regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#fixed-versus-variable-bandwidths"><span class="header-section-number">8.0.1</span> Fixed versus variable bandwidths</a></li>
<li><a class="nav-link" href="#local-regression-versus-local-means"><span class="header-section-number">8.0.2</span> Local regression versus local means</a></li>
<li><a class="nav-link" href="#choosing-the-bandwidth"><span class="header-section-number">8.0.3</span> Choosing the bandwidth</a></li>
</ul>
</li>
<li><a class="nav-link" href="#the-problem-of-outliers-and-other-model-departures-1"><span class="header-section-number">8.1</span> The problem of outliers and other model departures</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>UBC Stat 406 Worksheets</strong>" was written by Daniel J. McDonald and Matías Salibán-Barrera. It was last built on 2021-08-17.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
