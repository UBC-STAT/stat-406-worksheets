<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>18 What is Adaboost doing, really? | UBC Stat 406 Worksheets</title>
<meta name="author" content="Daniel J. McDonald and Matías Salibán-Barrera">
<meta name="description" content="Following the work of (Friedman, Hastie, and Tibshirani 2000) (see also Chapter 10 of [ESL]), we saw in class that Adaboost can be interpreted as fitting an additive model in a stepwise (greedy)...">
<meta name="generator" content="bookdown 0.23 with bs4_book()">
<meta property="og:title" content="18 What is Adaboost doing, really? | UBC Stat 406 Worksheets">
<meta property="og:type" content="book">
<meta property="og:url" content="https://ubc-stat.github.io/stat-406-worksheets/what-is-adaboost-doing-really.html">
<meta property="og:description" content="Following the work of (Friedman, Hastie, and Tibshirani 2000) (see also Chapter 10 of [ESL]), we saw in class that Adaboost can be interpreted as fitting an additive model in a stepwise (greedy)...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="18 What is Adaboost doing, really? | UBC Stat 406 Worksheets">
<meta name="twitter:description" content="Following the work of (Friedman, Hastie, and Tibshirani 2000) (see also Chapter 10 of [ESL]), we saw in class that Adaboost can be interpreted as fitting an additive model in a stepwise (greedy)...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.5.1/tabs.js"></script><script src="libs/bs3compat-0.2.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">UBC Stat 406 Worksheets</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Stat 406 Worksheets</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Module 0 – Review</li>
<li><a class="" href="predictions-using-a-linear-model.html"><span class="header-section-number">1</span> Predictions using a linear model</a></li>
<li class="book-part">Module 1 – Model Selection</li>
<li><a class="" href="predictions-using-a-linear-model-continued.html"><span class="header-section-number">2</span> Predictions using a linear model (continued)</a></li>
<li><a class="" href="cross-validation-concerns.html"><span class="header-section-number">3</span> Cross-validation concerns</a></li>
<li><a class="" href="comparing-models.html"><span class="header-section-number">4</span> Comparing models</a></li>
<li class="book-part">Module 2 – Regression</li>
<li><a class="" href="ridge-regression.html"><span class="header-section-number">5</span> Ridge regression</a></li>
<li><a class="" href="lasso.html"><span class="header-section-number">6</span> LASSO</a></li>
<li><a class="" href="non-parametric-regression.html"><span class="header-section-number">7</span> Non-parametric regression</a></li>
<li><a class="" href="kernel-regression-local-regression.html"><span class="header-section-number">8</span> Kernel regression / local regression</a></li>
<li><a class="" href="regression-trees.html"><span class="header-section-number">9</span> Regression trees</a></li>
<li><a class="" href="pruning-regression-trees-with-rpart.html"><span class="header-section-number">10</span> Pruning regression trees with rpart</a></li>
<li class="book-part">Module 3 – Classification</li>
<li><a class="" href="parametric-classifiers.html"><span class="header-section-number">11</span> Parametric classifiers</a></li>
<li><a class="" href="qda.html"><span class="header-section-number">12</span> QDA</a></li>
<li><a class="" href="classification-trees.html"><span class="header-section-number">13</span> Classification Trees</a></li>
<li><a class="" href="bagging-for-regression.html"><span class="header-section-number">14</span> Bagging for regression</a></li>
<li><a class="" href="bagging-for-classification.html"><span class="header-section-number">15</span> Bagging for classification</a></li>
<li><a class="" href="random-forests.html"><span class="header-section-number">16</span> Random Forests</a></li>
<li><a class="" href="boosting-a-statistical-learning-perspective.html"><span class="header-section-number">17</span> Boosting (a Statistical Learning perspective)</a></li>
<li><a class="active" href="what-is-adaboost-doing-really.html"><span class="header-section-number">18</span> What is Adaboost doing, really?</a></li>
<li><a class="" href="single-layer-neural-network.html"><span class="header-section-number">19</span> Single layer neural network</a></li>
<li class="book-part">Module 5 – Unsupervised learning</li>
<li><a class="" href="introduction.html"><span class="header-section-number">20</span> Introduction</a></li>
<li><a class="" href="clustering.html"><span class="header-section-number">21</span> Clustering</a></li>
<li><a class="" href="model-based-clustering.html"><span class="header-section-number">22</span> Model based clustering</a></li>
<li><a class="" href="hierarchical-clustering.html"><span class="header-section-number">23</span> Hierarchical clustering</a></li>
<li><a class="" href="references.html">References</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="alt-pca.html"><span class="header-section-number">A</span> PCA and alternating regression</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/ubc-stat/stat-406-worksheets">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="what-is-adaboost-doing-really" class="section level1">
<h1>
<span class="header-section-number">18</span> What is Adaboost doing, <em>really</em>?<a class="anchor" aria-label="anchor" href="#what-is-adaboost-doing-really"><i class="fas fa-link"></i></a>
</h1>
<p>Following the work of <span class="citation">(Friedman, Hastie, and Tibshirani <a href="references.html#ref-FriedmanHastie2000" role="doc-biblioref">2000</a>)</span> (see also
Chapter 10 of [ESL]), we saw in class that Adaboost can be
interpreted as fitting an <em>additive model</em> in a stepwise (greedy) way,
using an exponential loss.
It is then easy to prove that Adaboost.M1
is computing an approximation to the <em>optimal classifier</em>
G( x ) = log[ P( Y = 1 | X = x ) / P( Y = -1 | X = x ) ] / 2,
where <em>optimal</em> here is taken with respect to the <strong>exponential loss</strong>
function. More specifically, Adaboost.M1 is using an
additive model to approximate that function. In other words, Boosting is
attempting to find functions <span class="math inline">\(f_1\)</span>, <span class="math inline">\(f_2\)</span>, …, <span class="math inline">\(f_N\)</span> such that
<span class="math inline">\(G(x) = \sum_i f_i( x^{(i)} )\)</span>, where <span class="math inline">\(x^{(i)}\)</span> is a sub-vector
of <span class="math inline">\(x\)</span> (i.e. the function <span class="math inline">\(f_i\)</span> only depends on <em>some</em> of the
available features, typically a few of them: 1 or 2, say). Note
that each <span class="math inline">\(f_i\)</span> generally depends on a different subset of
features than the other <span class="math inline">\(f_j\)</span>’s.</p>
<p>Knowing the function the boosting algorithm is approximating (even
if it does it in a greedy and suboptimal way), allows us to
understand when the algorithm is expected to work well,
and also when it may not work well.
In particular, it provides one way to choose the complexity of the
<em>weak lerners</em> used to construct the ensemble. For an example
you can refer to the corresponding lab activity.</p>
<div id="a-more-challenging-example-the-email-spam-data" class="section level3">
<h3>
<span class="header-section-number">18.0.1</span> A more challenging example, the <code>email spam</code> data<a class="anchor" aria-label="anchor" href="#a-more-challenging-example-the-email-spam-data"><i class="fas fa-link"></i></a>
</h3>
<p>The email spam data set is a relatively classic data set
containing 57 features (potentially explanatory variables)
measured on 4601 email messages. The goal is to predict
whether an email is <em>spam</em> or not. The 57 features are
a mix of continuous and discrete variables. More information
can be found at
<a href="https://archive.ics.uci.edu/ml/datasets/spambase">https://archive.ics.uci.edu/ml/datasets/spambase</a>.</p>
<p>We first load the data and randomly separate it into a training and
a test set. A more thorough analysis would be to use
<em>full</em> K-fold cross-validation, but given the computational
complexity, I decided to leave the rest of this
3-fold CV exercise to the reader.</p>
<div class="sourceCode" id="cb246"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">spam</span>, package <span class="op">=</span> <span class="st">"ElemStatLearn"</span><span class="op">)</span>
<span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">spam</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">987</span><span class="op">)</span>
<span class="va">ii</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="va">n</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">floor</a></span><span class="op">(</span><span class="va">n</span> <span class="op">/</span> <span class="fl">3</span><span class="op">)</span><span class="op">)</span>
<span class="va">spam.te</span> <span class="op">&lt;-</span> <span class="va">spam</span><span class="op">[</span><span class="va">ii</span>, <span class="op">]</span>
<span class="va">spam.tr</span> <span class="op">&lt;-</span> <span class="va">spam</span><span class="op">[</span><span class="op">-</span><span class="va">ii</span>, <span class="op">]</span></code></pre></div>
<p>We now use Adaboost with 500 iterations, using <em>stumps</em> (1-split
trees) as our
weak learners / classifiers, and check the performance on
the test set:</p>
<div class="sourceCode" id="cb247"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">adabag</span><span class="op">)</span>
<span class="va">onesplit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.control.html">rpart.control</a></span><span class="op">(</span>cp <span class="op">=</span> <span class="op">-</span><span class="fl">1</span>, maxdepth <span class="op">=</span> <span class="fl">1</span>, minsplit <span class="op">=</span> <span class="fl">0</span>, xval <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>
<span class="va">bo1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/adabag/man/boosting.html">boosting</a></span><span class="op">(</span><span class="va">spam</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">spam.tr</span>, boos <span class="op">=</span> <span class="cn">FALSE</span>, mfinal <span class="op">=</span> <span class="fl">500</span>, control <span class="op">=</span> <span class="va">onesplit</span><span class="op">)</span>
<span class="va">pr1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">bo1</span>, newdata <span class="op">=</span> <span class="va">spam.te</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">spam.te</span><span class="op">$</span><span class="va">spam</span>, <span class="va">pr1</span><span class="op">$</span><span class="va">class</span><span class="op">)</span> <span class="co"># (pr1$confusion)</span>
<span class="co">#&gt;        </span>
<span class="co">#&gt;         email spam</span>
<span class="co">#&gt;   email   883   39</span>
<span class="co">#&gt;   spam     45  566</span></code></pre></div>
<p>The classification error rate on the test set is 0.055. We now
compare it with that of a Random Forest and look at the fit:</p>
<div class="sourceCode" id="cb248"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">randomForest</a></span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>
<span class="op">(</span><span class="va">a</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/randomForest/man/randomForest.html">randomForest</a></span><span class="op">(</span><span class="va">spam</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">spam.tr</span>, ntree <span class="op">=</span> <span class="fl">500</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Call:</span>
<span class="co">#&gt;  randomForest(formula = spam ~ ., data = spam.tr, ntree = 500) </span>
<span class="co">#&gt;                Type of random forest: classification</span>
<span class="co">#&gt;                      Number of trees: 500</span>
<span class="co">#&gt; No. of variables tried at each split: 7</span>
<span class="co">#&gt; </span>
<span class="co">#&gt;         OOB estimate of  error rate: 5.05%</span>
<span class="co">#&gt; Confusion matrix:</span>
<span class="co">#&gt;       email spam class.error</span>
<span class="co">#&gt; email  1813   53  0.02840300</span>
<span class="co">#&gt; spam    102 1100  0.08485857</span></code></pre></div>
<p>Note that the OOB estimate of the classification error rate
is 0.051.
The number of trees used seems to be appropriate in terms
of the stability of the OOB error rate estimate:</p>
<div class="sourceCode" id="cb249"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">a</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="44-adaboost_files/figure-html/spam.plot.rf-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Now use the test set to estimate the error rate of the Random Forest
(for a fair comparison with the one computed with boosting) and obtain</p>
<div class="sourceCode" id="cb250"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">pr.rf</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">a</span>, newdata <span class="op">=</span> <span class="va">spam.te</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">spam.te</span><span class="op">$</span><span class="va">spam</span>, <span class="va">pr.rf</span><span class="op">)</span>
<span class="co">#&gt;        pr.rf</span>
<span class="co">#&gt;         email spam</span>
<span class="co">#&gt;   email   886   36</span>
<span class="co">#&gt;   spam     36  575</span></code></pre></div>
<p>The performance of Random Forests on this test set is better than that of
boosting (recall that the estimated classification error rate
for 1-split trees-based Adaboost was
0.055, while for the Random Forest is 0.047 on the test set and 0.051 using OOB).</p>
<p>Is there <em>any room for improvement</em> for Adaboost?
As we discussed in class, depending on the interactions that may be
present in the <em>true classification function</em>, we might be able to
improve our boosting classifier by slightly increasing the complexity
of our base ensemble members. Here we try to use 3-split classification
trees, instead of the 1-split ones used above:</p>
<div class="sourceCode" id="cb251"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">threesplits</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.control.html">rpart.control</a></span><span class="op">(</span>cp <span class="op">=</span> <span class="op">-</span><span class="fl">1</span>, maxdepth <span class="op">=</span> <span class="fl">3</span>, minsplit <span class="op">=</span> <span class="fl">0</span>, xval <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>
<span class="va">bo3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/adabag/man/boosting.html">boosting</a></span><span class="op">(</span><span class="va">spam</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">spam.tr</span>, boos <span class="op">=</span> <span class="cn">FALSE</span>, mfinal <span class="op">=</span> <span class="fl">500</span>, control <span class="op">=</span> <span class="va">threesplits</span><span class="op">)</span>
<span class="va">pr3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">bo3</span>, newdata <span class="op">=</span> <span class="va">spam.te</span><span class="op">)</span>
<span class="op">(</span><span class="va">pr3</span><span class="op">$</span><span class="va">confusion</span><span class="op">)</span>
<span class="co">#&gt;                Observed Class</span>
<span class="co">#&gt; Predicted Class email spam</span>
<span class="co">#&gt;           email   881   34</span>
<span class="co">#&gt;           spam     41  577</span></code></pre></div>
<p>The number of elements on the boosting ensemble (500) appears to be
appropriate when we look at the error rate on the test set as
a function of the number of boosting iterations:</p>
<div class="sourceCode" id="cb252"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/adabag/man/errorevol.html">errorevol</a></span><span class="op">(</span><span class="va">bo3</span>, newdata <span class="op">=</span> <span class="va">spam.te</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="44-adaboost_files/figure-html/spam.5-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>There is, in fact, a noticeable improvement in performance on this
test set compared to the AdaBoost using <em>stumps</em>.
The estimated classification error rate of AdaBoost using 3-split trees on this test set is
0.049. Recall that the estimated classification error rate
for the Random Forest was 0.047
(or 0.051 using OOB).</p>
<p>As mentioned above you are strongly encouraged to finish this analysis
by doing a complete K-fold CV analysis in order to compare boosting with random
forests on these data.</p>
</div>
<div id="an-example-on-improving-adaboosts-performance-including-interactions" class="section level3">
<h3>
<span class="header-section-number">18.0.2</span> An example on improving Adaboost’s performance including interactions<a class="anchor" aria-label="anchor" href="#an-example-on-improving-adaboosts-performance-including-interactions"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Some error I can’t track happens below</strong></p>
<p>Consider the data set in the file <code>boost.sim.csv</code>. This
is a synthetic data inspired by the
well-known Boston Housing data. The response variable is <code>class</code>
and the two predictors are <code>lon</code> and <code>lat</code>. We read the data set</p>
<div class="sourceCode" id="cb253"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">sim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"data/boost.sim.csv"</span>, header <span class="op">=</span> <span class="cn">TRUE</span>, sep <span class="op">=</span> <span class="st">","</span>, row.names <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></code></pre></div>
<p>We split the data randomly into a training and a test set:</p>
<div class="sourceCode" id="cb254"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>
<span class="va">ii</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">sim</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">sim</span><span class="op">)</span> <span class="op">/</span> <span class="fl">3</span><span class="op">)</span>
<span class="va">sim.tr</span> <span class="op">&lt;-</span> <span class="va">sim</span><span class="op">[</span><span class="op">-</span><span class="va">ii</span>, <span class="op">]</span>
<span class="va">sim.te</span> <span class="op">&lt;-</span> <span class="va">sim</span><span class="op">[</span><span class="va">ii</span>, <span class="op">]</span></code></pre></div>
<p>As before, we use <em>stumps</em> as our base classifiers</p>
<div class="sourceCode" id="cb255"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/bethatkinson/rpart">rpart</a></span><span class="op">)</span>
<span class="va">stump</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.control.html">rpart.control</a></span><span class="op">(</span>cp <span class="op">=</span> <span class="op">-</span><span class="fl">1</span>, maxdepth <span class="op">=</span> <span class="fl">1</span>, minsplit <span class="op">=</span> <span class="fl">0</span>, xval <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></code></pre></div>
<p>and run 300 iterations of the boosting algorithm:</p>
<div class="sourceCode" id="cb256"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">17</span><span class="op">)</span>
<span class="va">sim1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/adabag/man/boosting.html">boosting</a></span><span class="op">(</span><span class="va">class</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">sim.tr</span>, boos <span class="op">=</span> <span class="cn">FALSE</span>, mfinal <span class="op">=</span> <span class="fl">300</span>, control <span class="op">=</span> <span class="va">stump</span><span class="op">)</span></code></pre></div>
<p>We examine the evolution of our ensemble on the test set:</p>
<div class="sourceCode" id="cb257"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/adabag/man/errorevol.html">errorevol</a></span><span class="op">(</span><span class="va">sim1</span>, newdata <span class="op">=</span> <span class="va">sim.te</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>and note that the peformance is both disappointing and does not improve with
the number of iterations. The error rate on the test set is
.
Based on the discussion in class about the effect of the
complexity of the base classifiers,
we now increase slightly their complexity: from
stumps to trees with up to 2 splits:</p>
<div class="sourceCode" id="cb258"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">twosplit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.control.html">rpart.control</a></span><span class="op">(</span>cp <span class="op">=</span> <span class="op">-</span><span class="fl">1</span>, maxdepth <span class="op">=</span> <span class="fl">2</span>, minsplit <span class="op">=</span> <span class="fl">0</span>, xval <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">17</span><span class="op">)</span>
<span class="va">sim2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/adabag/man/boosting.html">boosting</a></span><span class="op">(</span><span class="va">class</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">sim.tr</span>, boos <span class="op">=</span> <span class="cn">FALSE</span>, mfinal <span class="op">=</span> <span class="fl">300</span>, control <span class="op">=</span> <span class="va">twosplit</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/adabag/man/errorevol.html">errorevol</a></span><span class="op">(</span><span class="va">sim2</span>, newdata <span class="op">=</span> <span class="va">sim.te</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>Note that the error rate improves noticeably to
.
Interestingly, note as well that increasing the number
of splits of the base classifiers does not seem to
help much. With 3-split trees:</p>
<div class="sourceCode" id="cb259"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">threesplit</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.control.html">rpart.control</a></span><span class="op">(</span>cp <span class="op">=</span> <span class="op">-</span><span class="fl">1</span>, maxdepth <span class="op">=</span> <span class="fl">3</span>, minsplit <span class="op">=</span> <span class="fl">0</span>, xval <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">17</span><span class="op">)</span>
<span class="va">sim3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/adabag/man/boosting.html">boosting</a></span><span class="op">(</span><span class="va">class</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">sim.tr</span>, boos <span class="op">=</span> <span class="cn">FALSE</span>, mfinal <span class="op">=</span> <span class="fl">300</span>, control <span class="op">=</span> <span class="va">threesplit</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/pkg/adabag/man/errorevol.html">errorevol</a></span><span class="op">(</span><span class="va">sim3</span>, newdata <span class="op">=</span> <span class="va">sim.te</span><span class="op">)</span><span class="op">)</span></code></pre></div>
<p>the error rate on the test set is</p>
<div class="sourceCode" id="cb260"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">sim3</span>, newdata <span class="op">=</span> <span class="va">sim.te</span><span class="op">)</span><span class="op">$</span><span class="va">error</span>, <span class="fl">4</span><span class="op">)</span></code></pre></div>
<p>while with 4-split trees the error rate is</p>
<div class="sourceCode" id="cb261"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">sim4</span>, newdata <span class="op">=</span> <span class="va">sim.te</span><span class="op">)</span><span class="op">$</span><span class="va">error</span>, <span class="fl">4</span><span class="op">)</span></code></pre></div>
<p>The explanation for this is that the response variables
in the data set  were in fact generated
through the following relationship:</p>
<pre><code>log [ P ( Y = 1 | X = x ) / P ( Y = -1 | X = x ) ] / 2
 = [ max( x2 - 2, 0) - max( x1 + 1, 0) ] ( 1- x1 + x2 )</code></pre>
<p>where <span class="math inline">\(x = (x_1, x_2)^\top\)</span>. Since <em>stumps</em> (1-split trees)
are by definition functions of a single
variable, boosting will not be able to approximate the above function using
a linear combination of them, regardless of how many terms you use. Two-split
trees, on the other hand, are able to model interactions between the two
explanatory variables <span class="math inline">\(X_1\)</span> (<code>lon</code>) and
<span class="math inline">\(X_2\)</span> (<code>lat</code>), and thus, with sufficient terms in the sum, we are able to
approximate the above function relatively well.</p>
<p>As before, note that the analysis above may depend on the specific
training / test split we used, so it is strongly suggested that you
re-do it using a proper cross-validation setup.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="boosting-a-statistical-learning-perspective.html"><span class="header-section-number">17</span> Boosting (a Statistical Learning perspective)</a></div>
<div class="next"><a href="single-layer-neural-network.html"><span class="header-section-number">19</span> Single layer neural network</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav"><li>
<a class="nav-link" href="#what-is-adaboost-doing-really"><span class="header-section-number">18</span> What is Adaboost doing, really?</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#a-more-challenging-example-the-email-spam-data"><span class="header-section-number">18.0.1</span> A more challenging example, the email spam data</a></li>
<li><a class="nav-link" href="#an-example-on-improving-adaboosts-performance-including-interactions"><span class="header-section-number">18.0.2</span> An example on improving Adaboost’s performance including interactions</a></li>
</ul>
</li></ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/ubc-stat/stat-406-worksheets/blob/main/44-adaboost.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/ubc-stat/stat-406-worksheets/edit/main/44-adaboost.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>UBC Stat 406 Worksheets</strong>" was written by Daniel J. McDonald and Matías Salibán-Barrera. It was last built on 2021-08-18.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
