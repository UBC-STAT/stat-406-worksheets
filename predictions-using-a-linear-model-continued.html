<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>2 Predictions using a linear model (continued) | UBC Stat 406 Worksheets</title>
<meta name="author" content="Daniel J. McDonald and Matías Salibán-Barrera">
<meta name="description" content="In these notes we continue looking at the problem of comparing different models based on their prediction properties. As in the previous lecture, we consider a full and a reduced model, and in all...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="2 Predictions using a linear model (continued) | UBC Stat 406 Worksheets">
<meta property="og:type" content="book">
<meta property="og:url" content="https://ubc-stat.github.io/stat-406-worksheets/predictions-using-a-linear-model-continued.html">
<meta property="og:description" content="In these notes we continue looking at the problem of comparing different models based on their prediction properties. As in the previous lecture, we consider a full and a reduced model, and in all...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="2 Predictions using a linear model (continued) | UBC Stat 406 Worksheets">
<meta name="twitter:description" content="In these notes we continue looking at the problem of comparing different models based on their prediction properties. As in the previous lecture, we consider a full and a reduced model, and in all...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.0/transition.js"></script><script src="libs/bs3compat-0.3.0/tabs.js"></script><script src="libs/bs3compat-0.3.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">UBC Stat 406 Worksheets</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Stat 406 Worksheets</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Module 0 – Review</li>
<li><a class="" href="predictions-using-a-linear-model.html"><span class="header-section-number">1</span> Predictions using a linear model</a></li>
<li class="book-part">Module 1 – Model Selection</li>
<li><a class="active" href="predictions-using-a-linear-model-continued.html"><span class="header-section-number">2</span> Predictions using a linear model (continued)</a></li>
<li><a class="" href="cross-validation-concerns.html"><span class="header-section-number">3</span> Cross-validation concerns</a></li>
<li><a class="" href="comparing-models.html"><span class="header-section-number">4</span> Comparing models</a></li>
<li class="book-part">Module 2 – Regression</li>
<li><a class="" href="ridge-regression.html"><span class="header-section-number">5</span> Ridge regression</a></li>
<li><a class="" href="lasso.html"><span class="header-section-number">6</span> LASSO</a></li>
<li><a class="" href="non-parametric-regression.html"><span class="header-section-number">7</span> Non-parametric regression</a></li>
<li><a class="" href="kernel-regression-local-regression.html"><span class="header-section-number">8</span> Kernel regression / local regression</a></li>
<li><a class="" href="regression-trees.html"><span class="header-section-number">9</span> Regression trees</a></li>
<li><a class="" href="pruning-regression-trees-with-rpart.html"><span class="header-section-number">10</span> Pruning regression trees with rpart</a></li>
<li class="book-part">Module 3 – Classification</li>
<li><a class="" href="parametric-classifiers.html"><span class="header-section-number">11</span> Parametric classifiers</a></li>
<li><a class="" href="qda.html"><span class="header-section-number">12</span> QDA</a></li>
<li><a class="" href="classification-trees.html"><span class="header-section-number">13</span> Classification Trees</a></li>
<li class="book-part">Module 4 – Modern techniques</li>
<li><a class="" href="bagging-for-regression.html"><span class="header-section-number">14</span> Bagging for regression</a></li>
<li><a class="" href="bagging-for-classification.html"><span class="header-section-number">15</span> Bagging for classification</a></li>
<li><a class="" href="random-forests.html"><span class="header-section-number">16</span> Random Forests</a></li>
<li><a class="" href="boosting-a-statistical-learning-perspective.html"><span class="header-section-number">17</span> Boosting (a Statistical Learning perspective)</a></li>
<li><a class="" href="what-is-adaboost-doing-really.html"><span class="header-section-number">18</span> What is Adaboost doing, really?</a></li>
<li><a class="" href="single-layer-neural-network.html"><span class="header-section-number">19</span> Single layer neural network</a></li>
<li class="book-part">Module 5 – Unsupervised learning</li>
<li><a class="" href="introduction.html"><span class="header-section-number">20</span> Introduction</a></li>
<li><a class="" href="clustering.html"><span class="header-section-number">21</span> Clustering</a></li>
<li><a class="" href="model-based-clustering.html"><span class="header-section-number">22</span> Model based clustering</a></li>
<li><a class="" href="hierarchical-clustering.html"><span class="header-section-number">23</span> Hierarchical clustering</a></li>
<li><a class="" href="references.html">References</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="alt-pca.html"><span class="header-section-number">A</span> PCA and alternating regression</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/ubc-stat/stat-406-worksheets">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="predictions-using-a-linear-model-continued" class="section level1">
<h1>
<span class="header-section-number">2</span> Predictions using a linear model (continued)<a class="anchor" aria-label="anchor" href="#predictions-using-a-linear-model-continued"><i class="fas fa-link"></i></a>
</h1>
<p>In these notes we continue looking at the problem of
comparing different models based on their
prediction properties. As in the previous lecture, we consider
a <strong>full</strong> and a <strong>reduced</strong> model, and in all that follows we assume that the
variables included in the <strong>reduced</strong> model were not selected using the training data.
<strong>This seemingly innocent assumption is in fact critical, and we will later come back to it.</strong></p>
<div id="estimating-the-mspe-with-a-test-set" class="section level2">
<h2>
<span class="header-section-number">2.1</span> Estimating the MSPE with a test set<a class="anchor" aria-label="anchor" href="#estimating-the-mspe-with-a-test-set"><i class="fas fa-link"></i></a>
</h2>
<p>One way to estimate the mean squared prediction error of a
model or predictor is to use it on a test set (where the
responses are known, but that was not used when training the
predcitor or estimating the model), and the comparing the
predictions with the actual responses.</p>
<p>First, we load the training set and fit both models:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x.tr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"data/pollution-train.dat"</span>, header <span class="op">=</span> <span class="cn">TRUE</span>, sep <span class="op">=</span> <span class="st">","</span><span class="op">)</span>
<span class="va">full</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MORT</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">x.tr</span><span class="op">)</span>
<span class="va">reduced</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MORT</span> <span class="op">~</span> <span class="va">POOR</span> <span class="op">+</span> <span class="va">HC</span> <span class="op">+</span> <span class="va">NOX</span> <span class="op">+</span> <span class="va">HOUS</span> <span class="op">+</span> <span class="va">NONW</span>, data <span class="op">=</span> <span class="va">x.tr</span><span class="op">)</span></code></pre></div>
<p>Although the <strong>full</strong> model fits the data better than the
reduced one (see Lecture 1), its predictions on the test set are better.
First, compute the two vectors of test set predictions:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x.te</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.table</a></span><span class="op">(</span><span class="st">"data/pollution-test.dat"</span>, header <span class="op">=</span> <span class="cn">TRUE</span>, sep <span class="op">=</span> <span class="st">","</span><span class="op">)</span>
<span class="va">pr.full</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">full</span>, newdata <span class="op">=</span> <span class="va">x.te</span><span class="op">)</span>
<span class="va">pr.reduced</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">reduced</span>, newdata <span class="op">=</span> <span class="va">x.te</span><span class="op">)</span></code></pre></div>
<p>And now, use them to estimate the mean squared prediction error of
each model:</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">x.te</span>, <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">MORT</span> <span class="op">-</span> <span class="va">pr.full</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; [1] 2859.367</span>
<span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">x.te</span>, <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">MORT</span> <span class="op">-</span> <span class="va">pr.reduced</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span>
<span class="co">#&gt; [1] 1861.884</span></code></pre></div>
<p>Previously, we also saw that
this is not just an artifact of the specific
training / test split of the data. The <strong>reduced</strong>
model generally produces better predictions,
regardless of the specific training / test
split we use. We can verify this repeating
the procedure many times (100, say) and looking
at the estimated mean squared prediction errors
obtained each time for each model.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="st">"data/rutgers-lib-30861_CSV-1.csv"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>
<span class="va">Nsplits</span> <span class="op">&lt;-</span> <span class="fl">100</span>
<span class="va">mspe.full</span> <span class="op">&lt;-</span> <span class="va">mspe.red</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html">vector</a></span><span class="op">(</span><span class="st">"numeric"</span>, <span class="va">Nsplits</span><span class="op">)</span>
<span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">Nsplits</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">g</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span>, each <span class="op">=</span> <span class="fl">15</span><span class="op">)</span><span class="op">)</span>
  <span class="va">a.tr</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="va">g</span> <span class="op">!=</span> <span class="fl">2</span>, <span class="op">]</span>
  <span class="va">a.te</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="va">g</span> <span class="op">==</span> <span class="fl">2</span>, <span class="op">]</span>
  <span class="va">full</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MORT</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">a.tr</span><span class="op">)</span>
  <span class="va">reduced</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MORT</span> <span class="op">~</span> <span class="va">POOR</span> <span class="op">+</span> <span class="va">HC</span> <span class="op">+</span> <span class="va">NOX</span> <span class="op">+</span> <span class="va">HOUS</span> <span class="op">+</span> <span class="va">NONW</span>, data <span class="op">=</span> <span class="va">a.tr</span><span class="op">)</span>
  <span class="va">a.te</span><span class="op">$</span><span class="va">pr.full</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">full</span>, newdata <span class="op">=</span> <span class="va">a.te</span><span class="op">)</span>
  <span class="va">a.te</span><span class="op">$</span><span class="va">pr.reduced</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">reduced</span>, newdata <span class="op">=</span> <span class="va">a.te</span><span class="op">)</span>
  <span class="va">mspe.full</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">a.te</span>, <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">MORT</span> <span class="op">-</span> <span class="va">pr.full</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span>
  <span class="va">mspe.red</span><span class="op">[</span><span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">a.te</span>, <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">MORT</span> <span class="op">-</span> <span class="va">pr.reduced</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span>
<span class="op">}</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/boxplot.html">boxplot</a></span><span class="op">(</span><span class="va">mspe.full</span>, <span class="va">mspe.red</span>,
  names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Full"</span>, <span class="st">"Reduced"</span><span class="op">)</span>,
  col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"gray80"</span>, <span class="st">"tomato3"</span><span class="op">)</span>,
  main <span class="op">=</span> <span class="st">"Air Pollution - 100 training/test splits"</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">5000</span><span class="op">)</span>
<span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/mtext.html">mtext</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/grDevices/plotmath.html">hat</a></span><span class="op">(</span><span class="va">MSPE</span><span class="op">)</span><span class="op">)</span>, side <span class="op">=</span> <span class="fl">2</span>, line <span class="op">=</span> <span class="fl">2.5</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-test-set-and-cv_files/figure-html/testrain-1.png" width="90%" style="display: block; margin: auto;"></div>
</div>
<div id="leave-one-out-cross-validation" class="section level2">
<h2>
<span class="header-section-number">2.2</span> Leave-one-out cross-validation<a class="anchor" aria-label="anchor" href="#leave-one-out-cross-validation"><i class="fas fa-link"></i></a>
</h2>
<p>A different procedure to estimate the prediction power
of a model or method is called <strong>leave-one-out CV</strong>.
One advantage of using this method is that
the model we fit can use a larger training set.
We discussed the procedure in class. Here
we apply it to estimate the mean squared
prediction error of the <strong>full</strong> and <strong>reduced</strong>
models. Again, we assume that the reduced model
was chosen independently from the training set.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/utils/read.table.html">read.csv</a></span><span class="op">(</span><span class="st">"data/rutgers-lib-30861_CSV-1.csv"</span><span class="op">)</span>
<span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="va">pr.full</span> <span class="op">&lt;-</span> <span class="va">pr.reduced</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">n</span><span class="op">)</span>
<span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">full</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MORT</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="op">-</span><span class="va">i</span>, <span class="op">]</span><span class="op">)</span>
  <span class="va">reduced</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MORT</span> <span class="op">~</span> <span class="va">POOR</span> <span class="op">+</span> <span class="va">HC</span> <span class="op">+</span> <span class="va">NOX</span> <span class="op">+</span> <span class="va">HOUS</span> <span class="op">+</span> <span class="va">NONW</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="op">-</span><span class="va">i</span>, <span class="op">]</span><span class="op">)</span>
  <span class="va">pr.full</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">full</span>, newdata <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">i</span>, <span class="op">]</span><span class="op">)</span>
  <span class="va">pr.reduced</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">reduced</span>, newdata <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">i</span>, <span class="op">]</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>Now we have the leave-one-out predictions for each model
and can compute the corresponding estimated mean squared
prediction errors:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">MORT</span> <span class="op">-</span> <span class="va">pr.full</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>
<span class="co">#&gt; [1] 2136.785</span>
<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">x</span><span class="op">$</span><span class="va">MORT</span> <span class="op">-</span> <span class="va">pr.reduced</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>
<span class="co">#&gt; [1] 1848.375</span></code></pre></div>
<p>Note that here again the reduced model seems to yield better
prediction errors.</p>
</div>
<div id="k-fold-cross-validation" class="section level2">
<h2>
<span class="header-section-number">2.3</span> K-fold cross-validation<a class="anchor" aria-label="anchor" href="#k-fold-cross-validation"><i class="fas fa-link"></i></a>
</h2>
<p>Leave-one-out cross-validation can be computationally
very demanding (or even unfeasible) when the sample size
is large and training the predictor is relatively costly.
One solution is called <strong>K-fold CV</strong>. We split the data
into <strong>K</strong> folds, train the predictor on the data without
a fold, and use it to predict the responses
in the removed fold. We cycle through the folds, and
use the average of the squared prediction errors as
an estimate of the mean squared prediction error.
The following script does <strong>5-fold CV</strong> for the
<code>full</code> and <code>reduced</code> linear models on the
pollution dataset, once again assuming that the reduced model
was originally chosen without using the data.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>
<span class="va">k</span> <span class="op">&lt;-</span> <span class="fl">5</span>
<span class="va">pr.full</span> <span class="op">&lt;-</span> <span class="va">pr.reduced</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="va">n</span><span class="op">)</span>
<span class="co"># Create labels for the "folds"</span>
<span class="va">inds</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="va">n</span><span class="op">)</span> <span class="op">%%</span> <span class="va">k</span> <span class="op">+</span> <span class="fl">1</span>
<span class="co"># shuffle the rows of x, this is bad coding!</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span>
<span class="va">xs</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="va">n</span>, replace <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span>, <span class="op">]</span>
<span class="co"># loop through the folds</span>
<span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">k</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">x.tr</span> <span class="op">&lt;-</span> <span class="va">xs</span><span class="op">[</span><span class="va">inds</span> <span class="op">!=</span> <span class="va">j</span>, <span class="op">]</span>
  <span class="va">x.te</span> <span class="op">&lt;-</span> <span class="va">xs</span><span class="op">[</span><span class="va">inds</span> <span class="op">==</span> <span class="va">j</span>, <span class="op">]</span>
  <span class="va">full</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MORT</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">x.tr</span><span class="op">)</span>
  <span class="va">reduced</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MORT</span> <span class="op">~</span> <span class="va">POOR</span> <span class="op">+</span> <span class="va">HC</span> <span class="op">+</span> <span class="va">NOX</span> <span class="op">+</span> <span class="va">HOUS</span> <span class="op">+</span> <span class="va">NONW</span>, data <span class="op">=</span> <span class="va">x.tr</span><span class="op">)</span>
  <span class="va">pr.full</span><span class="op">[</span><span class="va">inds</span> <span class="op">==</span> <span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">full</span>, newdata <span class="op">=</span> <span class="va">x.te</span><span class="op">)</span>
  <span class="va">pr.reduced</span><span class="op">[</span><span class="va">inds</span> <span class="op">==</span> <span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">reduced</span>, newdata <span class="op">=</span> <span class="va">x.te</span><span class="op">)</span>
<span class="op">}</span></code></pre></div>
<p>We now compute the estimated mean squared prediction
error of each model:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">xs</span><span class="op">$</span><span class="va">MORT</span> <span class="op">-</span> <span class="va">pr.full</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>
<span class="co">#&gt; [1] 2227.21</span>
<span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">xs</span><span class="op">$</span><span class="va">MORT</span> <span class="op">-</span> <span class="va">pr.reduced</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>
<span class="co">#&gt; [1] 2003.857</span></code></pre></div>
<p>This method is clearly faster than leave-one-out CV, but
the results may depend on the specific fold partition,
and on the number <strong>K</strong> of folds used.</p>
<ul>
<li>One way to obtain more stable mean squared prediction errors
using K-fold CV is to repeat the above procedure
many times, and compare the distribution of the
mean squared prediction errors for each estimator.
First, fit the <strong>full</strong> and <strong>reduced</strong> models using the
whole data set as training:</li>
</ul>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">m.f</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MORT</span> <span class="op">~</span> <span class="va">.</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">)</span>
<span class="va">m.r</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">MORT</span> <span class="op">~</span> <span class="va">POOR</span> <span class="op">+</span> <span class="va">HC</span> <span class="op">+</span> <span class="va">NOX</span> <span class="op">+</span> <span class="va">HOUS</span> <span class="op">+</span> <span class="va">NONW</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">)</span></code></pre></div>
<p>We will use 50 runs of 5-fold CV comparing the <strong>full</strong> and <strong>reduced</strong> models.
Again, here we assume that the reduced model was not obtained
using the training data.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="va">N</span> <span class="op">&lt;-</span> <span class="fl">50</span>
<span class="va">mspe1</span> <span class="op">&lt;-</span> <span class="va">mspe2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html">vector</a></span><span class="op">(</span><span class="st">"double"</span>, <span class="va">N</span><span class="op">)</span>
<span class="va">ii</span> <span class="op">&lt;-</span> <span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="op">(</span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%%</span> <span class="fl">5</span> <span class="op">+</span> <span class="fl">1</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">327</span><span class="op">)</span>
<span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="va">N</span><span class="op">)</span> <span class="op">{</span>
  <span class="va">ii</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="va">ii</span><span class="op">)</span>
  <span class="va">pr.f</span> <span class="op">&lt;-</span> <span class="va">pr.r</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/vector.html">vector</a></span><span class="op">(</span><span class="st">"double"</span>, <span class="va">n</span><span class="op">)</span>
  <span class="kw">for</span> <span class="op">(</span><span class="va">j</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">)</span> <span class="op">{</span>
    <span class="va">pr.f</span><span class="op">[</span><span class="va">ii</span> <span class="op">==</span> <span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span>
      <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span><span class="op">(</span><span class="va">m.f</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">ii</span> <span class="op">!=</span> <span class="va">j</span>, <span class="op">]</span><span class="op">)</span>,
      newdata <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">ii</span> <span class="op">==</span> <span class="va">j</span>, <span class="op">]</span>
    <span class="op">)</span>
    <span class="va">pr.r</span><span class="op">[</span><span class="va">ii</span> <span class="op">==</span> <span class="va">j</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span>
      <span class="fu"><a href="https://rdrr.io/r/stats/update.html">update</a></span><span class="op">(</span><span class="va">m.r</span>, data <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">ii</span> <span class="op">!=</span> <span class="va">j</span>, <span class="op">]</span><span class="op">)</span>,
      newdata <span class="op">=</span> <span class="va">x</span><span class="op">[</span><span class="va">ii</span> <span class="op">==</span> <span class="va">j</span>, <span class="op">]</span>
    <span class="op">)</span>
  <span class="op">}</span>
  <span class="va">mspe1</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">MORT</span> <span class="op">-</span> <span class="va">pr.f</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span>
  <span class="va">mspe2</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html">with</a></span><span class="op">(</span><span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">MORT</span> <span class="op">-</span> <span class="va">pr.r</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span>
<span class="op">}</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/boxplot.html">boxplot</a></span><span class="op">(</span><span class="va">mspe1</span>, <span class="va">mspe2</span>,
  names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Full"</span>, <span class="st">"Reduced"</span><span class="op">)</span>,
  col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"gray80"</span>, <span class="st">"tomato3"</span><span class="op">)</span>,
  main <span class="op">=</span> <span class="st">"Air Pollution - 50 runs 5-fold CV"</span>, ylim <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">5000</span><span class="op">)</span>
<span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/mtext.html">mtext</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/grDevices/plotmath.html">hat</a></span><span class="op">(</span><span class="va">MSPE</span><span class="op">)</span><span class="op">)</span>, side <span class="op">=</span> <span class="fl">2</span>, line <span class="op">=</span> <span class="fl">2.5</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="10-test-set-and-cv_files/figure-html/cv10runs-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>Note that the estimated mean squared prediction
error of the <strong>reduced</strong> model has a smaller mean / median
than that of the <strong>full</strong> one. This tells us that
the conclusion we reached favouring the reduced model
(in terms of its prediction mean squared error) does
not depend on a particular choice of folds. In other
words, this provides more evidence to conclude that
the reduced model will produce better predictions
than the full one.</p>
<ul>
<li><p>A computationally simpler (albeit possibly less precise) way
to account for the K-fold variability is to run
K-fold CV once and
use the sample standard error of the
<strong>K</strong> <em>smaller</em> mean squared prediction errors to
construct a rough <em>confidence interval</em> around
the overall mean squared prediction error estimate (that is
the average of the mean squared prediction errors
over the K folds).</p></li>
<li><p>The dependency of this MSPE on <strong>K</strong> is more involved.
We will discuss it later.</p></li>
</ul>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="predictions-using-a-linear-model.html"><span class="header-section-number">1</span> Predictions using a linear model</a></div>
<div class="next"><a href="cross-validation-concerns.html"><span class="header-section-number">3</span> Cross-validation concerns</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#predictions-using-a-linear-model-continued"><span class="header-section-number">2</span> Predictions using a linear model (continued)</a></li>
<li><a class="nav-link" href="#estimating-the-mspe-with-a-test-set"><span class="header-section-number">2.1</span> Estimating the MSPE with a test set</a></li>
<li><a class="nav-link" href="#leave-one-out-cross-validation"><span class="header-section-number">2.2</span> Leave-one-out cross-validation</a></li>
<li><a class="nav-link" href="#k-fold-cross-validation"><span class="header-section-number">2.3</span> K-fold cross-validation</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/ubc-stat/stat-406-worksheets/blob/main/10-test-set-and-cv.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/ubc-stat/stat-406-worksheets/edit/main/10-test-set-and-cv.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>UBC Stat 406 Worksheets</strong>" was written by Daniel J. McDonald and Matías Salibán-Barrera. It was last built on 2021-09-17.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
