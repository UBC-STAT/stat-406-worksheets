[{"path":"index.html","id":"stat-406-worksheets","chapter":"Stat 406 Worksheets","heading":"Stat 406 Worksheets","text":"Authors:Daniel J. McDonaldMatías Salibián-BarreraLast updated: 16 August 2023","code":""},{"path":"preface.html","id":"preface","chapter":"Preface","heading":"Preface","text":"worksheet collection originally created Prof. Salibián-Barrera taught course 2019 previous iterations. 2021 version, Prof. McDonald made revisions turned book use labs supplement course readings.expectation run worksheets lab sections. material fair game final exam.run , need number packages. attempt install , try:notes licensed Creative Commons Attribution-ShareAlike 4.0 International License.\nSee human-readable version \nreal thing .","code":"\nif (!suppressWarnings(require(remotes, quietly = TRUE)))\n  install.packages(\"remotes\")\ntmp <- tempdir()\ndp <- file.path(tmp, \"DESCRIPTION\")\ndownload.file(\n  \"https://raw.githubusercontent.com/UBC-STAT/stat-406-worksheets/main/DESCRIPTION\",\n  dp\n)\nremotes::install_deps(tmp)\nunlink(tmp)\nrm(tmp, dp)"},{"path":"predictions-using-a-linear-model.html","id":"predictions-using-a-linear-model","chapter":"1 Predictions using a linear model","heading":"1 Predictions using a linear model","text":"document explore (rather superficially)\nchallenges found trying estimate forecasting\nproperties (e.g. mean squared prediction error) (linear) predictor. \nuse air-pollution data set, split training set test set.\ntest set ignored training model (case linear\nmodel, “training” simply means “estimating vector linear regression\nparameters”).interested sets (training test) constructed:\nran following script (\nneed , providing data sets ,\ncan re-create want ):now read training data set file pollution-train.dat,\navailable , check read properly:response variable MORT.\nfirst step fit \nlinear regression model available\npredictors look diagnostic plots\neverything looks fine:also take look estimated coeficients:fit appears routine, reasonable (? check come conclusion?).","code":"\nx <- read.csv(\"data/rutgers-lib-30861_CSV-1.csv\")\nset.seed(123)\nii <- sample(rep(1:4, each = 15))\n# this is the training set `pollution-train.dat`\nx.tr <- x[ii != 2, ]\n# this is the test set `pollution-test.dat`\nx.te <- x[ii == 2, ]\n# then I saved them to disk:\n# write.csv(x.tr, file='pollution-train.dat', row.names=FALSE, quote=FALSE)\n# write.csv(x.te, file='pollution-test.dat', row.names=FALSE, quote=FALSE)\nx.tr <- read.table(\"data/pollution-train.dat\", header = TRUE, sep = \",\")\n# sanity check\nhead(x.tr)\n#>   PREC JANT JULT OVR65 POPN EDUC HOUS DENS NONW WWDRK POOR HC NOX SO. HUMID\n#> 1   36   27   71   8.1 3.34 11.4 81.5 3243  8.8  42.6 11.7 21  15  59    59\n#> 2   35   23   72  11.1 3.14 11.0 78.8 4281  3.5  50.7 14.4  8  10  39    57\n#> 3   44   29   74  10.4 3.21  9.8 81.6 4260  0.8  39.4 12.4  6   6  33    54\n#> 4   47   45   79   6.5 3.41 11.1 77.5 3125 27.1  50.2 20.6 18   8  24    56\n#> 5   43   35   77   7.6 3.44  9.6 84.6 6441 24.4  43.7 14.3 43  38 206    55\n#> 6   53   45   80   7.7 3.45 10.2 66.8 3325 38.5  43.1 25.5 30  32  72    54\n#>       MORT\n#> 1  921.870\n#> 2  997.875\n#> 3  962.354\n#> 4  982.291\n#> 5 1071.289\n#> 6 1030.380\nfull <- lm(MORT ~ ., data = x.tr)\nplot(full, which = 1)\nplot(full, which = 2)\nsummary(full)\n#> \n#> Call:\n#> lm(formula = MORT ~ ., data = x.tr)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -66.06 -14.11  -0.78  17.13  66.09 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  2.210e+03  5.091e+02   4.341 0.000157 ***\n#> PREC         1.786e+00  1.306e+00   1.367 0.181994    \n#> JANT        -1.794e+00  1.205e+00  -1.489 0.147375    \n#> JULT        -4.767e+00  2.913e+00  -1.636 0.112591    \n#> OVR65       -1.150e+01  9.335e+00  -1.232 0.227734    \n#> POPN        -1.586e+02  7.373e+01  -2.151 0.039980 *  \n#> EDUC        -1.278e+01  1.421e+01  -0.899 0.376043    \n#> HOUS        -8.500e-01  2.013e+00  -0.422 0.676023    \n#> DENS         8.253e-03  5.274e-03   1.565 0.128473    \n#> NONW         4.844e+00  1.566e+00   3.093 0.004357 ** \n#> WWDRK       -1.666e-01  1.947e+00  -0.086 0.932408    \n#> POOR        -1.755e+00  3.530e+00  -0.497 0.622938    \n#> HC          -4.090e-01  5.452e-01  -0.750 0.459193    \n#> NOX          5.607e-01  1.109e+00   0.506 0.616884    \n#> SO.          1.762e-01  1.848e-01   0.954 0.348033    \n#> HUMID       -2.647e+00  2.160e+00  -1.225 0.230307    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 32.55 on 29 degrees of freedom\n#> Multiple R-squared:  0.7978, Adjusted R-squared:  0.6931 \n#> F-statistic: 7.626 on 15 and 29 DF,  p-value: 1.805e-06"},{"path":"predictions-using-a-linear-model.html","id":"a-new-focus-prediction","chapter":"1 Predictions using a linear model","heading":"1.0.1 A new focus: prediction","text":"course primarily concerned making (good) predictions cases\n(data points) may observed yet (future predictions). bit\ndifferent focus Statistics courses may taken. \nsee later course learned Statistics courses\n(e.g. trade-offs flexibility stability different models, uncertainty\nstandard techniques reduce , etc.) prove\ncritical building good predictions.simple example, rest note compare quality model’s predictions simpler (smaller) linear model 5 predictors. illustrative example, worry 5 explanatory variables selected, however, play critical role later course).now fit reduced model look estimated parameters diagnostic plotsAlthough reduced linear model (5 predictors) seem provide fit\ngood one get full model, still acceptable.observation obvious , since,\nalready now, model always yield\nbetter fit data terms \nresidual sum squares submodels\n(.e. model using subset explanatory\nvariables). expect able formally\nprove last satement.question interest :\n“model produces better predictions?” many cases one \ninterested predicting future observations, .e. \npredicting response variable data\navailable model / predictor \nfit trained. discussed class, reasonably\nfair comparison can obtined \ncomparing mean squared predictions\ntwo linear models test set, \nread R follows:Now compute predicted values test set\nfull reduced models:compute corresponding mean squared prediction errors:Note reduced model (fit data\nwell full model) nevertheless produced\nbetter predictions (smaller mean squared prediction\nerrors) test set.point put critical / skeptical\nhat wonder happen chance, .e.\nmay just\nartifact specific training/test partition\nused. following simple experiment shows \ncase. good exercise\nrepeat many times (100, say) verify\nclaim.First, read whole data create new\ntraining / test random split.code chunk, used x.tr denote \ntraining set x.te test set.\nNow, fit full reduced models\nnew training set:Finally, estimate mean squared prediction\nerror models squared prediction\nerror test set:Note estimated mean squared prediction error\nreduced model considerably smaller\nfull model (even though latter always fits \ntraining set better reduced one).","code":"\nreduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\nsummary(reduced)\n#> \n#> Call:\n#> lm(formula = MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -95.654 -21.848  -1.995  21.555  81.039 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 1117.2254   164.3972   6.796 4.09e-08 ***\n#> POOR          -4.7667     2.5516  -1.868 0.069268 .  \n#> HC            -1.4237     0.3705  -3.843 0.000437 ***\n#> NOX            2.6880     0.7262   3.702 0.000660 ***\n#> HOUS          -2.0595     1.7940  -1.148 0.257957    \n#> NONW           4.3004     1.0140   4.241 0.000132 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 39.44 on 39 degrees of freedom\n#> Multiple R-squared:  0.6007, Adjusted R-squared:  0.5495 \n#> F-statistic: 11.73 on 5 and 39 DF,  p-value: 5.844e-07\nplot(reduced, which = 1)\nplot(reduced, which = 2)\nsum(resid(reduced)^2)\n#> [1] 60652.22\nsum(resid(full)^2)\n#> [1] 30718.19\nx.te <- read.table(\"data/pollution-test.dat\", header = TRUE, sep = \",\")\nhead(x.te)\n#>   PREC JANT JULT OVR65 POPN EDUC HOUS DENS NONW WWDRK POOR HC NOX SO. HUMID\n#> 1   52   42   79   7.7 3.39  9.6 69.2 2302 22.2  41.3 24.2 18   8  27    56\n#> 2   33   26   76   8.6 3.20 10.9 83.4 6122 16.3  44.9 10.7 88  63 278    58\n#> 3   40   34   77   9.2 3.21 10.2 77.0 4101 13.0  45.7 15.1 26  26 146    57\n#> 4   35   46   85   7.1 3.22 11.8 79.9 1441 14.8  51.2 16.1  1   1   1    54\n#> 5   15   30   73   8.2 3.15 12.2 84.2 4824  4.7  53.1 12.7 17   8  28    38\n#> 6   43   27   72   9.0 3.25 11.5 87.1 2909  7.2  51.6  9.5  7   3  10    56\n#>       MORT\n#> 1 1017.613\n#> 2 1024.885\n#> 3  970.467\n#> 4  860.101\n#> 5  871.766\n#> 6  887.466\nx.te$pr.full <- predict(full, newdata = x.te)\nx.te$pr.reduced <- predict(reduced, newdata = x.te)\nwith(x.te, mean((MORT - pr.full)^2))\n#> [1] 2859.367\nwith(x.te, mean((MORT - pr.reduced)^2))\n#> [1] 1861.884\n# repeat with different partitions\nx <- read.csv(\"data/rutgers-lib-30861_CSV-1.csv\")\nset.seed(456)\nii <- sample(rep(1:4, each = 15))\nx.tr <- x[ii != 2, ]\nx.te <- x[ii == 2, ]\nfull <- lm(MORT ~ ., data = x.tr)\nreduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\nx.te$pr.full <- predict(full, newdata = x.te)\nx.te$pr.reduced <- predict(reduced, newdata = x.te)\nwith(x.te, mean((MORT - pr.full)^2))\n#> [1] 2194.79\nwith(x.te, mean((MORT - pr.reduced)^2))\n#> [1] 1393.885"},{"path":"predictions-using-a-linear-model-continued.html","id":"predictions-using-a-linear-model-continued","chapter":"2 Predictions using a linear model (continued)","heading":"2 Predictions using a linear model (continued)","text":"notes continue looking problem \ncomparing different models based \nprediction properties. previous lecture, consider\nfull reduced model, follows assume \nvariables included reduced model selected using training data.\nseemingly innocent assumption fact critical, later come back .","code":""},{"path":"predictions-using-a-linear-model-continued.html","id":"estimating-the-mspe-with-a-test-set","chapter":"2 Predictions using a linear model (continued)","heading":"2.1 Estimating the MSPE with a test set","text":"One way estimate mean squared prediction error \nmodel predictor use test set (\nresponses known, used training \npredcitor estimating model), comparing \npredictions actual responses.First, load training set fit models:Although full model fits data better \nreduced one (see Lecture 1), predictions test set better.\nFirst, compute two vectors test set predictions:now, use estimate mean squared prediction error \nmodel:Previously, also saw \njust artifact specific\ntraining / test split data. reduced\nmodel generally produces better predictions,\nregardless specific training / test\nsplit use. can verify repeating\nprocedure many times (100, say) looking\nestimated mean squared prediction errors\nobtained time model.","code":"\nx.tr <- read.table(\"data/pollution-train.dat\", header = TRUE, sep = \",\")\nfull <- lm(MORT ~ ., data = x.tr)\nreduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\nx.te <- read.table(\"data/pollution-test.dat\", header = TRUE, sep = \",\")\npr.full <- predict(full, newdata = x.te)\npr.reduced <- predict(reduced, newdata = x.te)\nwith(x.te, mean((MORT - pr.full)^2))\n#> [1] 2859.367\nwith(x.te, mean((MORT - pr.reduced)^2))\n#> [1] 1861.884\nx <- read.csv(\"data/rutgers-lib-30861_CSV-1.csv\")\nset.seed(123)\nNsplits <- 100\nmspe.full <- mspe.red <- vector(\"numeric\", Nsplits)\nfor (j in 1:Nsplits) {\n  g <- sample(rep(1:4, each = 15))\n  a.tr <- x[g != 2, ]\n  a.te <- x[g == 2, ]\n  full <- lm(MORT ~ ., data = a.tr)\n  reduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = a.tr)\n  a.te$pr.full <- predict(full, newdata = a.te)\n  a.te$pr.reduced <- predict(reduced, newdata = a.te)\n  mspe.full[j] <- with(a.te, mean((MORT - pr.full)^2))\n  mspe.red[j] <- with(a.te, mean((MORT - pr.reduced)^2))\n}\nboxplot(mspe.full, mspe.red,\n  names = c(\"Full\", \"Reduced\"),\n  col = c(\"gray80\", \"tomato3\"),\n  main = \"Air Pollution - 100 training/test splits\", ylim = c(0, 5000)\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"},{"path":"predictions-using-a-linear-model-continued.html","id":"leave-one-out-cross-validation","chapter":"2 Predictions using a linear model (continued)","heading":"2.2 Leave-one-out cross-validation","text":"different procedure estimate prediction power\nmodel method called leave-one-CV.\nOne advantage using method \nmodel fit can use larger training set.\ndiscussed procedure class. \napply estimate mean squared\nprediction error full reduced\nmodels. , assume reduced model\nchosen independently training set.Now leave-one-predictions model\ncan compute corresponding estimated mean squared\nprediction errors:Note reduced model seems yield better\nprediction errors.","code":"\nx <- read.csv(\"data/rutgers-lib-30861_CSV-1.csv\")\nn <- nrow(x)\npr.full <- pr.reduced <- rep(0, n)\nfor (i in 1:n) {\n  full <- lm(MORT ~ ., data = x[-i, ])\n  reduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x[-i, ])\n  pr.full[i] <- predict(full, newdata = x[i, ])\n  pr.reduced[i] <- predict(reduced, newdata = x[i, ])\n}\nmean((x$MORT - pr.full)^2)\n#> [1] 2136.785\nmean((x$MORT - pr.reduced)^2)\n#> [1] 1848.375"},{"path":"predictions-using-a-linear-model-continued.html","id":"k-fold-cross-validation","chapter":"2 Predictions using a linear model (continued)","heading":"2.3 K-fold cross-validation","text":"Leave-one-cross-validation can computationally\ndemanding (even unfeasible) sample size\nlarge training predictor relatively costly.\nOne solution called K-fold CV. split data\nK folds, train predictor data without\nfold, use predict responses\nremoved fold. cycle folds, \nuse average squared prediction errors \nestimate mean squared prediction error.\nfollowing script 5-fold CV \nfull reduced linear models \npollution dataset, assuming reduced model\noriginally chosen without using data.now compute estimated mean squared prediction\nerror model:method clearly faster leave-one-CV, \nresults may depend specific fold partition,\nnumber K folds used.One way obtain stable mean squared prediction errors\nusing K-fold CV repeat procedure\nmany times, compare distribution \nmean squared prediction errors estimator.\nFirst, fit full reduced models using \nwhole data set training:use 50 runs 5-fold CV comparing full reduced models.\n, assume reduced model obtained\nusing training data.Note estimated mean squared prediction\nerror reduced model smaller mean / median\nfull one. tells us \nconclusion reached favouring reduced model\n(terms prediction mean squared error) \ndepend particular choice folds. \nwords, provides evidence conclude \nreduced model produce better predictions\nfull one.computationally simpler (albeit possibly less precise) way\naccount K-fold variability run\nK-fold CV \nuse sample standard error \nK smaller mean squared prediction errors \nconstruct rough confidence interval around\noverall mean squared prediction error estimate (\naverage mean squared prediction errors\nK folds).computationally simpler (albeit possibly less precise) way\naccount K-fold variability run\nK-fold CV \nuse sample standard error \nK smaller mean squared prediction errors \nconstruct rough confidence interval around\noverall mean squared prediction error estimate (\naverage mean squared prediction errors\nK folds).dependency MSPE K involved.\ndiscuss later.dependency MSPE K involved.\ndiscuss later.","code":"\nn <- nrow(x)\nk <- 5\npr.full <- pr.reduced <- rep(0, n)\n# Create labels for the \"folds\"\ninds <- (1:n) %% k + 1\n# shuffle the rows of x, this is bad coding!\nset.seed(123)\nxs <- x[sample(n, replace = FALSE), ]\n# loop through the folds\nfor (j in 1:k) {\n  x.tr <- xs[inds != j, ]\n  x.te <- xs[inds == j, ]\n  full <- lm(MORT ~ ., data = x.tr)\n  reduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\n  pr.full[inds == j] <- predict(full, newdata = x.te)\n  pr.reduced[inds == j] <- predict(reduced, newdata = x.te)\n}\nmean((xs$MORT - pr.full)^2)\n#> [1] 2227.21\nmean((xs$MORT - pr.reduced)^2)\n#> [1] 2003.857\nm.f <- lm(MORT ~ ., data = x)\nm.r <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x)\nN <- 50\nmspe1 <- mspe2 <- vector(\"double\", N)\nii <- (1:(n <- nrow(x))) %% 5 + 1\nset.seed(327)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.f <- pr.r <- vector(\"double\", n)\n  for (j in 1:5) {\n    pr.f[ii == j] <- predict(\n      update(m.f, data = x[ii != j, ]),\n      newdata = x[ii == j, ]\n    )\n    pr.r[ii == j] <- predict(\n      update(m.r, data = x[ii != j, ]),\n      newdata = x[ii == j, ]\n    )\n  }\n  mspe1[i] <- with(x, mean((MORT - pr.f)^2))\n  mspe2[i] <- with(x, mean((MORT - pr.r)^2))\n}\nboxplot(mspe1, mspe2,\n  names = c(\"Full\", \"Reduced\"),\n  col = c(\"gray80\", \"tomato3\"),\n  main = \"Air Pollution - 50 runs 5-fold CV\", ylim = c(0, 5000)\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"},{"path":"cross-validation-concerns.html","id":"cross-validation-concerns","chapter":"3 Cross-validation concerns","heading":"3 Cross-validation concerns","text":"document study perform cross-validation\nmodel selected determined using \ntraining data. Consider following synthetic data\nsetThis data used class. example\nknow “true” model , thus also know\n“optimal” predictor .\nHowever, let us ignore knowledge, build \nlinear model instead.\nGiven many variables available, use\nforward stepwise (AIC-based) select good subset \ninclude linear model:Without thinking much, use 50 runs 5-fold CV (ten runs)\ncompare MSPE \nnull model (know “true”) \none obtained using forward stepwise:Something wrong! ? ?change obtain reliable estimates MSPE \nmodel selected stepwise approach?","code":"\ndat <- read.table(\"data/fallacy.dat\", header = TRUE, sep = \",\")\nlibrary(MASS)\np <- ncol(dat)\nnull <- lm(Y ~ 1, data = dat)\nfull <- lm(Y ~ ., data = dat) # needed for stepwise\nstep.lm <- stepAIC(null, scope = list(lower = null, upper = full), trace = FALSE)\nn <- nrow(dat)\nii <- (1:n) %% 5 + 1\nset.seed(17)\nN <- 50\nmspe.n <- mspe.st <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.n <- pr.st <- rep(0, n)\n  for (j in 1:5) {\n    tmp.st <- update(step.lm, data = dat[ii != j, ])\n    pr.st[ii == j] <- predict(tmp.st, newdata = dat[ii == j, ])\n    pr.n[ii == j] <- with(dat[ii != j, ], mean(Y))\n  }\n  mspe.st[i] <- with(dat, mean((Y - pr.st)^2))\n  mspe.n[i] <- with(dat, mean((Y - pr.n)^2))\n}\nboxplot(mspe.st, mspe.n, names = c(\"Stepwise\", \"NULL\"), col = c(\"gray60\", \"hotpink\"), main = \"Wrong\")\nsummary(mspe.st)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.5931  0.6392  0.6658  0.6663  0.6945  0.7517\nsummary(mspe.n)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.044   1.050   1.057   1.057   1.062   1.084"},{"path":"cross-validation-concerns.html","id":"estimating-mspe-with-cv-when-the-model-was-built-using-the-data","chapter":"3 Cross-validation concerns","heading":"3.1 Estimating MSPE with CV when the model was built using the data","text":"Misuse cross-validation , unfortunately,\nunusual. one example see (Ambroise McLachlan 2002).particular, every fold one needs repeat everything done training set (selecting variables, looking pairwise correlations, AIC values, etc.)","code":""},{"path":"cross-validation-concerns.html","id":"correlated-covariates","chapter":"3 Cross-validation concerns","heading":"3.2 Correlated covariates","text":"Technological advances recent decades resulted data\ncollected fundamentally different manner way\ndone “classical” statistical methods developed\n(early mid 1900’s).\nSpecifically, now uncommon data sets \nabundance potentially useful explanatory variables\n(example variables observations).\nSometimes investigators sure collected variables\ncan \nexpected useful meaningful.consequence “wide net” data collection strategy \nmany explanatory variables may correlated \n. follows illustrate \nproblems can cause training interpreting\nmodels, also resulting predictions.","code":""},{"path":"cross-validation-concerns.html","id":"variables-that-were-important-may-suddenly-dissappear","chapter":"3 Cross-validation concerns","heading":"3.2.1 Variables that were important may suddenly “dissappear”","text":"Consider air pollution data set used\nearlier, \nreduced linear regression model discussed class:Note coefficients seem significant based \nindividual tests hypothesis (POOR \nHOUS maybe marginally ). sense 5\nexplanatory varibles model appear relevant.Now, fit full model, , include\navailable explanatory variables data set:full model \nmany parameters need estimated, two \nappear significantly different zero (NONW\nPREC), others appear redundant.\nparticular, note p-values individual\ntest hypotheses 4 5\nregression coefficients variables reduced\nmodel now become significant.words, coeffficients \nexplanatory variables appeared \nrelevant one model may turn\n“significant” variables\nincluded. pose challenges\ninterpreting estimated parameters \nmodels.","code":"\nx <- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\nreduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x)\nround(summary(reduced)$coef, 3)\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) 1172.831    143.241   8.188    0.000\n#> POOR          -4.065      2.238  -1.817    0.075\n#> HC            -1.480      0.333  -4.447    0.000\n#> NOX            2.846      0.652   4.369    0.000\n#> HOUS          -2.911      1.533  -1.899    0.063\n#> NONW           4.470      0.846   5.283    0.000\nfull <- lm(MORT ~ ., data = x)\nround(summary(full)$coef, 3)\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) 1763.981    437.327   4.034    0.000\n#> PREC           1.905      0.924   2.063    0.045\n#> JANT          -1.938      1.108  -1.748    0.087\n#> JULT          -3.100      1.902  -1.630    0.110\n#> OVR65         -9.065      8.486  -1.068    0.291\n#> POPN        -106.826     69.780  -1.531    0.133\n#> EDUC         -17.157     11.860  -1.447    0.155\n#> HOUS          -0.651      1.768  -0.368    0.714\n#> DENS           0.004      0.004   0.894    0.376\n#> NONW           4.460      1.327   3.360    0.002\n#> WWDRK         -0.187      1.662  -0.113    0.911\n#> POOR          -0.168      3.227  -0.052    0.959\n#> HC            -0.672      0.491  -1.369    0.178\n#> NOX            1.340      1.006   1.333    0.190\n#> SO.            0.086      0.148   0.585    0.562\n#> HUMID          0.107      1.169   0.091    0.928\nround(summary(full)$coef[names(coef(reduced)), ], 3)\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) 1763.981    437.327   4.034    0.000\n#> POOR          -0.168      3.227  -0.052    0.959\n#> HC            -0.672      0.491  -1.369    0.178\n#> NOX            1.340      1.006   1.333    0.190\n#> HOUS          -0.651      1.768  -0.368    0.714\n#> NONW           4.460      1.327   3.360    0.002"},{"path":"cross-validation-concerns.html","id":"why-does-this-happen","chapter":"3 Cross-validation concerns","heading":"3.2.2 Why does this happen?","text":"Recall covariance matrix least squares estimator involves \ninverse (X’X), X’ denotes transpose n x p matrix X (\ncontains vector explanatory variables row). easy see\ntwo columns X linearly dependent, X’X rank deficient.\ntwo columns X “close” linearly dependent (e.g. \nlinear corrleation high), matrix X’X ill-conditioned, \ninverse large entries. means estimated\nstandard errors least squares estimator unduly large, resulting\nnon-significant test hypotheses parameter separately, even \nglobal test simultaneously highly significant.","code":""},{"path":"cross-validation-concerns.html","id":"why-is-this-a-problem-if-we-are-interested-in-prediction","chapter":"3 Cross-validation concerns","heading":"3.2.3 Why is this a problem if we are interested in prediction?","text":"Although many applications one interested interpreting parameters\nmodel, even one trying fit / train model \npredictions, highly variable parameter estimators typically result \nnoticeable loss prediction accuracy. can easily seen \nbias / variance factorization mean squared prediction error (MSPE)\nmentioned class. Hence, better predictions can obtained one\nuses less-variable parameter (regression function) estimators.","code":""},{"path":"cross-validation-concerns.html","id":"what-can-we-do","chapter":"3 Cross-validation concerns","heading":"3.2.4 What can we do?","text":"commonly used strategy remove explanatory variables \nmodel, leaving non-redundant covariates. However, easier said \ndone. seen strategies previous Statistics\ncourses (e.g. stepwise variable selection).\ncoming weeks investigate methods deal problem.","code":""},{"path":"comparing-models.html","id":"comparing-models","chapter":"4 Comparing models","heading":"4 Comparing models","text":"","code":""},{"path":"comparing-models.html","id":"general-strategy","chapter":"4 Comparing models","heading":"4.1 General strategy","text":"Suppose set competing models want choose \n“best” one. order properly define problem need following:list models considered;numerical measure compare two models list;strategy (algorithm, criterion) navigate set models; anda criterion stop search.example, stepwise methods models consideration \nstep differ current model one\ncoefficient (variable). numerical measure used compare models\nAIC, Mallow’s Cp, etc. strategy consider\nsubmodels one fewer variable current one, stop\neither none “p-1” submodels better current one, \nreach empty model.","code":""},{"path":"comparing-models.html","id":"what-is-aic","chapter":"4 Comparing models","heading":"4.2 What is AIC?","text":"One intuitively sensible quantity can used compare models \ndistance measuring “close” distributions implied models actual stochastic process generating data (“stochastic process” refers random mechanism generated observations). order need:distance / metric (least “quasimetric”) models; anda way estimating distance “true” model unknown.AIC provides unbiased estimator Kullback-Leibler divergence\nestimated model “true” one. See lecture slides\ndetails.","code":""},{"path":"comparing-models.html","id":"using-stepwise-aic-to-select-a-model","chapter":"4 Comparing models","heading":"4.3 Using stepwise + AIC to select a model","text":"apply stepwise regression based AIC select linear\nregression model airpollution data. R can use\nfunction stepAIC package MASS perform stepwise\nsearch, synthetic data set discussed class:want see progression search step--step, set \nargument trace=TRUE call stepAIC .\nselected model automatically fit returned, \ncode st object class lm containing \n“best” linear regression fit.now compare mean squared prediction errors \nfull model selected stepwise.\nuse 50 runs 5-fold CV, obtain\nfollowing:Note since synthetic data set, can also\nestimate MSPE true model (compute analytically instead?)\ncompare full stepwise models.\nobtain:","code":"\nset.seed(123)\nx1 <- rnorm(506)\nx2 <- rnorm(506, mean = 2, sd = 1)\nx3 <- rexp(506, rate = 1)\nx4 <- x2 + rnorm(506, sd = .1)\nx5 <- x1 + rnorm(506, sd = .1)\nx6 <- x1 - x2 + rnorm(506, sd = .1)\nx7 <- x1 + x3 + rnorm(506, sd = .1)\ny <- x1 * 3 + x2 / 3 + rnorm(506, sd = 2.2)\n\nx <- data.frame(\n  y = y, x1 = x1, x2 = x2,\n  x3 = x3, x4 = x4, x5 = x5, x6 = x6, x7 = x7\n)\n\nlibrary(MASS)\nnull <- lm(y ~ 1, data = x)\nfull <- lm(y ~ ., data = x)\nst <- stepAIC(null, scope = list(lower = null, upper = full), trace = FALSE)\nst\n#> \n#> Call:\n#> lm(formula = y ~ x1 + x6, data = x)\n#> \n#> Coefficients:\n#> (Intercept)           x1           x6  \n#>   -0.000706     3.175239    -0.282906\nk <- 5\nn <- nrow(x)\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 50\nmspe.t <- mspe.f <- mspe.st <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.t <- pr.f <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    x0 <- x[ii != j, ]\n    null0 <- lm(y ~ 1, data = x0)\n    full0 <- lm(y ~ ., data = x0) # needed for stepwise\n    true0 <- lm(y ~ x1 + x2, data = x0)\n    step.lm0 <- stepAIC(null0, scope = list(lower = null0, upper = full0), trace = FALSE)\n    pr.st[ii == j] <- predict(step.lm0, newdata = x[ii == j, ])\n    pr.f[ii == j] <- predict(full0, newdata = x[ii == j, ])\n    pr.t[ii == j] <- predict(true0, newdata = x[ii == j, ])\n  }\n  mspe.st[i] <- mean((x$y - pr.st)^2)\n  mspe.f[i] <- mean((x$y - pr.f)^2)\n  mspe.t[i] <- mean((x$y - pr.t)^2)\n}\nboxplot(mspe.st, mspe.f,\n  names = c(\"Stepwise\", \"Full\"),\n  col = c(\"gray60\", \"hotpink\"), ylab = \"MSPE\"\n)\nboxplot(mspe.t, mspe.st, mspe.f,\n  names = c(\"True\", \"Stepwise\", \"Full\"),\n  col = c(\"tomato\", \"gray60\", \"hotpink\"), ylab = \"MSPE\"\n)"},{"path":"comparing-models.html","id":"stepwise-applied-to-the-air-pollution-data","chapter":"4 Comparing models","heading":"4.3.1 Stepwise applied to the “air pollution” data","text":"now use stepwise air pollution data select model, \nestimate MSPE using 5-fold CV. compare predictions \nmodel full model.can also use package leaps run thorough search\namong possible subsets. air pollution data:call asked leaps compute 10 best models\nsize, according Mallow’s Cp criterion. can look \nreturned objectWe now find best model (based Mallow’s Cp), \nfit corresponding model:compare variables used model \nused model found stepwise:reasonable ask whether model found leaps \nmuch better one returned stepAIC:Finally, MSPE model found leaps?Note “suboptimal” model (stepwise) seems better \none found “proper” (exhaustive) search, returned \nleaps. intriguing, see phenomenon\noccur different contexts later course.","code":"\nlibrary(MASS)\nairp <- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\nnull <- lm(MORT ~ 1, data = airp)\nfull <- lm(MORT ~ ., data = airp)\n(tmp.st <- stepAIC(full, scope = list(lower = null), trace = FALSE))\n#> \n#> Call:\n#> lm(formula = MORT ~ PREC + JANT + JULT + OVR65 + POPN + EDUC + \n#>     NONW + HC + NOX, data = airp)\n#> \n#> Coefficients:\n#> (Intercept)         PREC         JANT         JULT        OVR65         POPN  \n#>   1934.0539       1.8565      -2.2620      -3.3200     -10.9205    -137.3831  \n#>        EDUC         NONW           HC          NOX  \n#>    -23.4211       4.6623      -0.9221       1.8710\nk <- 5\nn <- nrow(airp)\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 50\nmspe.f <- mspe.st <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.f <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    x0 <- airp[ii != j, ]\n    null0 <- lm(MORT ~ 1, data = x0)\n    full0 <- lm(MORT ~ ., data = x0) # needed for stepwise\n    step.lm0 <- stepAIC(null0, scope = list(lower = null0, upper = full0), trace = FALSE)\n    pr.st[ii == j] <- predict(step.lm0, newdata = airp[ii == j, ])\n    pr.f[ii == j] <- predict(full0, newdata = airp[ii == j, ])\n  }\n  mspe.st[i] <- mean((airp$MORT - pr.st)^2)\n  mspe.f[i] <- mean((airp$MORT - pr.f)^2)\n}\nboxplot(mspe.st, mspe.f,\n  names = c(\"Stepwise\", \"Full\"),\n  col = c(\"gray60\", \"hotpink\"), ylab = \"MSPE\"\n)\nlibrary(leaps)\na <- leaps(x = as.matrix(airp[, -16]), y = airp$MORT, int = TRUE, method = \"Cp\", nbest = 10)\nstr(a)\n#> List of 4\n#>  $ which: logi [1:141, 1:15] FALSE FALSE TRUE FALSE FALSE FALSE ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:141] \"1\" \"1\" \"1\" \"1\" ...\n#>   .. ..$ : chr [1:15] \"1\" \"2\" \"3\" \"4\" ...\n#>  $ label: chr [1:16] \"(Intercept)\" \"1\" \"2\" \"3\" ...\n#>  $ size : num [1:141] 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ Cp   : num [1:141] 53.6 82.3 82.6 97 97.2 ...\nj0 <- which.min(a$Cp)\n(m1 <- lm(MORT ~ ., data = airp[, c(a$which[j0, ], TRUE)]))\n#> \n#> Call:\n#> lm(formula = MORT ~ ., data = airp[, c(a$which[j0, ], TRUE)])\n#> \n#> Coefficients:\n#> (Intercept)         PREC         JANT         JULT         EDUC         NONW  \n#>   1180.3565       1.7970      -1.4836      -2.3553     -13.6190       4.5853  \n#>         SO.  \n#>      0.2596\nformula(m1)[[3]]\n#> PREC + JANT + JULT + EDUC + NONW + SO.\nformula(tmp.st)[[3]]\n#> PREC + JANT + JULT + OVR65 + POPN + EDUC + NONW + HC + NOX\nextractAIC(m1)\n#> [1]   7.0000 429.0017\nextractAIC(tmp.st)\n#> [1]  10.000 429.634\n# proper way\nk <- 5\nn <- nrow(airp)\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 50\nmspe.l <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.l <- rep(0, n)\n  for (j in 1:k) {\n    x0 <- airp[ii != j, ]\n    tmp.leaps <- leaps(x = as.matrix(x0[, -16]), y = as.vector(x0[, 16]), int = TRUE, method = \"Cp\", nbest = 10)\n    j0 <- which.min(tmp.leaps$Cp)\n    step.leaps <- lm(MORT ~ ., data = x0[, c(tmp.leaps$which[j0, ], TRUE)])\n    pr.l[ii == j] <- predict(step.leaps, newdata = airp[ii == j, ])\n  }\n  mspe.l[i] <- mean((airp$MORT - pr.l)^2)\n}\nboxplot(mspe.st, mspe.f, mspe.l,\n  names = c(\"Stepwise\", \"Full\", \"Leaps\"),\n  col = c(\"gray60\", \"hotpink\", \"steelblue\"), ylab = \"MSPE\"\n)"},{"path":"comparing-models.html","id":"an-example-where-one-may-not-need-to-select-variables","chapter":"4 Comparing models","heading":"4.4 An example where one may not need to select variables","text":"cases one may need select subset explanatory\nvariables, fact, may affect negatively accuracy \nresulting predictions. follows discuss example.\nConsider credit card data set contains information\ncredit card users. interest predicting \nbalance carried client. first load data, \nsimplify presentation consider numerical\nexplanatory variables:6 available covariates, stepwise search selects\nmodel 5 (discarding Education):easy exercise check MSPE \nsmaller model fact worse one full one:","code":"\nx <- read.table(\"data/Credit.csv\", sep = \",\", header = TRUE, row.names = 1)\nx <- x[, c(1:6, 11)]\nlibrary(MASS)\nnull <- lm(Balance ~ 1, data = x)\nfull <- lm(Balance ~ ., data = x)\n(tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0))\n#> \n#> Call:\n#> lm(formula = Balance ~ Rating + Income + Limit + Age + Cards, \n#>     data = x)\n#> \n#> Coefficients:\n#> (Intercept)       Rating       Income        Limit          Age        Cards  \n#>   -449.3610       2.0224      -7.5621       0.1286      -0.8883      11.5527\nn <- nrow(x)\nk <- 5\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 100\nmspe.st <- mspe.f <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.f <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    null <- lm(Balance ~ 1, data = x[ii != j, ])\n    full <- lm(Balance ~ ., data = x[ii != j, ])\n    tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n    pr.st[ii == j] <- predict(tmp.st, newdata = x[ii == j, ])\n    pr.f[ii == j] <- predict(full, newdata = x[ii == j, ])\n  }\n  mspe.st[i] <- mean((x$Balance - pr.st)^2)\n  mspe.f[i] <- mean((x$Balance - pr.f)^2)\n}\nboxplot(mspe.st, mspe.f,\n  names = c(\"Stepwise\", \"Full\"),\n  col = c(\"tomato\", \"springgreen\"), cex.axis = 1.5,\n  cex.lab = 1.5, cex.main = 2\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"},{"path":"ridge-regression.html","id":"ridge-regression","chapter":"5 Ridge regression","heading":"5 Ridge regression","text":"Variable selection methods like stepwise can highly variable. illustrate \nissue consider following simple experiment. ,\napply stepwise 5 randomly selected folds data, look \nmodels selected .Although many variables appear one model, NONW .\n, JANT PREC 4 5.\nalso several appear one model (HOUS, WWDRK POPN).\nvariability may turn impact (negatively) accuracy \nresulting predictions.different approach dealing potentially correlated explanatory\nvariables (goal obtaining less variable / accurate\npredictions) “regularize” parameter estimates. words\nmodify optimization problem defines parameter\nestimators (case linear regression fits tweak\nleast squares problem) limit size (fact restricting\nbounded possibly small subset parameter\nspace).first proposal regularized / penalized estimator \nlinear regression models Ridge Regression.\nuse function glmnet package glmnet \ncompute Ridge Regression estimator. Note \nfunction implements larger family regularized estimators,\norder obtain Ridge Regression estimator\nneed set argument alpha = 0 glmnet().\n\n\nalso specify range possible values penalty\ncoefficient (use grid 50 values \nexp(-3) exp(10)).returned object contains estimated regression coefficients \npossible value regularization parameter. can look \nusing plot method objects class glmnet \nfollows:","code":"\nairp <- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\nlibrary(MASS)\nk <- 5\nn <- nrow(airp)\nset.seed(123456)\nii <- sample((1:n) %% k + 1)\nfor (j in 1:k) {\n  x0 <- airp[ii != j, ]\n  null0 <- lm(MORT ~ 1, data = x0)\n  full0 <- lm(MORT ~ ., data = x0) # needed for stepwise\n  step.lm0 <- stepAIC(null0,\n    scope = list(lower = null0, upper = full0),\n    trace = FALSE\n  )\n  print(formula(step.lm0)[[3]])\n}\n#> NONW + EDUC + JANT + OVR65 + SO.\n#> NONW + EDUC + PREC + SO. + JULT\n#> NONW + EDUC + JANT + SO. + PREC\n#> NONW + SO. + JANT + PREC + DENS\n#> NONW + JANT + EDUC + DENS + POPN + JULT + PREC + OVR65\nlibrary(glmnet)\n# alpha = 0 - Ridge\n# alpha = 1 - LASSO\ny <- as.vector(airp$MORT)\nxm <- as.matrix(airp[, -16])\nlambdas <- exp(seq(-3, 10, length = 50))\na <- glmnet(\n  x = xm, y = y, lambda = rev(lambdas),\n  family = \"gaussian\", alpha = 0\n)\nplot(a, xvar = \"lambda\", label = TRUE, lwd = 6, cex.axis = 1.5, cex.lab = 1.2, ylim = c(-20, 20))"},{"path":"ridge-regression.html","id":"selecting-the-level-of-regularization","chapter":"5 Ridge regression","heading":"5.1 Selecting the level of regularization","text":"Different values penalization parameter typically yield estimators \nvarying predictive accuracies. select good level regularization estimate\nMSPE estimator resulting value penalization parameter.\nOne way run K-fold cross validation value \npenalty. glmnet package provides built-function ,\nplot method display results:plot red dots estimated MSPE’s value \npenalty, vertical lines mark plus/minus one (estimated) standard deviations (\nestimated MSPE’s). plot method also mark optimal value \nregularization parameter, also largest one estimated MSPE\nwithin 1-SD optimal. latter meant provide regularized\nestimator estimated MSPE within error-margin estimated minimum.Note, however, “analysis” random (intrinsic randomness \nK-fold CV). run , likely get different results. many cases,\nhowever, results qualitatively similar. run 5-fold CV \ndata get following plot:Note plots similar, equal. good idea repeat \ntimes explore much variability involved. one interested\nselecting one value penalization parameter stable \nobtained single 5-fold CV run, one run several times \ntake average estimated optimal values. example:value reasonably close ones saw plots .","code":"\n# run 5-fold CV\nset.seed(123)\ntmp <- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\nplot(tmp, lwd = 6, cex.axis = 1.5, cex.lab = 1.2)\nset.seed(23)\ntmp <- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\nplot(tmp, lwd = 6, cex.axis = 1.5, cex.lab = 1.2)\nset.seed(123)\nop.la <- 0\nfor (j in 1:20) {\n  tmp <- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\n  op.la <- op.la + tmp$lambda.min # tmp$lambda.1se\n}\n(op.la <- op.la / 20)\n#> [1] 11.44547\nlog(op.la)\n#> [1] 2.437594"},{"path":"ridge-regression.html","id":"comparing-predictions","chapter":"5 Ridge regression","heading":"5.2 Comparing predictions","text":"now run cross-validation experiment compare \nMSPE 3 models: full model, one\nselected stepwise ridge regression\none.","code":"\nn <- nrow(xm)\nk <- 5\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 100\nmspe.st <- mspe.ri <- mspe.f <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.f <- pr.ri <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    tmp.ri <- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 0, family = \"gaussian\"\n    )\n    null <- lm(MORT ~ 1, data = airp[ii != j, ])\n    full <- lm(MORT ~ ., data = airp[ii != j, ])\n    tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n    pr.ri[ii == j] <- predict(tmp.ri, s = \"lambda.min\", newx = xm[ii == j, ])\n    pr.st[ii == j] <- predict(tmp.st, newdata = airp[ii == j, ])\n    pr.f[ii == j] <- predict(full, newdata = airp[ii == j, ])\n  }\n  mspe.ri[i] <- mean((airp$MORT - pr.ri)^2)\n  mspe.st[i] <- mean((airp$MORT - pr.st)^2)\n  mspe.f[i] <- mean((airp$MORT - pr.f)^2)\n}\nboxplot(mspe.ri, mspe.st, mspe.f,\n  names = c(\"Ridge\", \"Stepwise\", \"Full\"),\n  col = c(\"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1.5, cex.lab = 1.5,\n  cex.main = 2, ylim = c(1300, 3000)\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"},{"path":"ridge-regression.html","id":"a-more-stable-ridge-regression","chapter":"5 Ridge regression","heading":"5.3 A more stable Ridge Regression?","text":"try obtain ridge regression estimator\nstable predictions using \naverage optimal penalty value using 20 runs.\nimprovement appear \nsubstantial.","code":"\nn <- nrow(xm)\nk <- 5\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 100\nmspe.ri2 <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.ri2 <- rep(0, n)\n  for (j in 1:k) {\n    op.la <- 0\n    for (h in 1:20) {\n      tmp <- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\n      op.la <- op.la + tmp$lambda.min # tmp$lambda.1se\n    }\n    op.la <- op.la / 20\n    tmp.ri <- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas, nfolds = 5,\n      alpha = 0, family = \"gaussian\"\n    )\n    pr.ri2[ii == j] <- predict(tmp.ri, s = op.la, newx = xm[ii == j, ])\n  }\n  mspe.ri2[i] <- mean((airp$MORT - pr.ri2)^2)\n}\nboxplot(mspe.ri2, mspe.ri, mspe.st, mspe.f,\n  names = c(\"Stable R\", \"Ridge\", \"Stepwise\", \"Full\"),\n  col = c(\"steelblue\", \"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1.5, cex.lab = 1.5,\n  cex.main = 2, ylim = c(1300, 3000)\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"},{"path":"ridge-regression.html","id":"an-example-where-one-may-not-need-to-select-variables-1","chapter":"5 Ridge regression","heading":"5.4 An example where one may not need to select variables","text":"cases one may need select subset explanatory\nvariables, fact, may affect negatively accuracy \nresulting predictions. follows discuss example.\nConsider credit card data set contains information\ncredit card users. interest predicting \nbalance carried client. first load data, \nsimplify presentation consider numerical\nexplanatory variables:6 available covariates, stepwise search selects\nmodel 5 (discarding Education):easy exercise check MSPE \nsmaller model fact worse one full one:Using ridge regression instead stepwise prevent\nnegative effect possible correlations among \ncovariates yields slight improvement (full model),\nclear gain worth effort.","code":"\nx <- read.table(\"data/Credit.csv\", sep = \",\", header = TRUE, row.names = 1)\nx <- x[, c(1:6, 11)]\nlibrary(MASS)\nnull <- lm(Balance ~ 1, data = x)\nfull <- lm(Balance ~ ., data = x)\n(tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0))\n#> \n#> Call:\n#> lm(formula = Balance ~ Rating + Income + Limit + Age + Cards, \n#>     data = x)\n#> \n#> Coefficients:\n#> (Intercept)       Rating       Income        Limit          Age        Cards  \n#>   -449.3610       2.0224      -7.5621       0.1286      -0.8883      11.5527\nn <- nrow(x)\nk <- 5\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 100\nmspe.st <- mspe.f <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.f <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    null <- lm(Balance ~ 1, data = x[ii != j, ])\n    full <- lm(Balance ~ ., data = x[ii != j, ])\n    tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n    pr.st[ii == j] <- predict(tmp.st, newdata = x[ii == j, ])\n    pr.f[ii == j] <- predict(full, newdata = x[ii == j, ])\n  }\n  mspe.st[i] <- mean((x$Balance - pr.st)^2)\n  mspe.f[i] <- mean((x$Balance - pr.f)^2)\n}\nboxplot(mspe.st, mspe.f,\n  names = c(\"Stepwise\", \"Full\"),\n  col = c(\"tomato\", \"springgreen\"), cex.axis = 1.5,\n  cex.lab = 1.5, cex.main = 2\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\ny <- as.vector(x$Balance)\nxm <- as.matrix(x[, -7])\nlambdas <- exp(seq(-3, 10, length = 50))\nn <- nrow(xm)\nk <- 5\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 100\nmspe.st <- mspe.ri <- mspe.f <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.f <- pr.ri <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    tmp.ri <- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 0, family = \"gaussian\"\n    )\n    null <- lm(Balance ~ 1, data = x[ii != j, ])\n    full <- lm(Balance ~ ., data = x[ii != j, ])\n    tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n    pr.ri[ii == j] <- predict(tmp.ri, s = \"lambda.min\", newx = xm[ii == j, ])\n    pr.st[ii == j] <- predict(tmp.st, newdata = x[ii == j, ])\n    pr.f[ii == j] <- predict(full, newdata = x[ii == j, ])\n  }\n  mspe.ri[i] <- mean((x$Balance - pr.ri)^2)\n  mspe.st[i] <- mean((x$Balance - pr.st)^2)\n  mspe.f[i] <- mean((x$Balance - pr.f)^2)\n}\nboxplot(mspe.ri, mspe.st, mspe.f,\n  names = c(\n    \"Ridge\", \"Stepwise\",\n    \"Full\"\n  ), col = c(\"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1.5,\n  cex.lab = 1.5, cex.main = 2\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"},{"path":"ridge-regression.html","id":"an-important-limitation-of-ridge-regression","chapter":"5 Ridge regression","heading":"5.5 An important limitation of Ridge Regression","text":"Ridge Regression typically yields estimators accurate (less variable)\npredictions, specially noticeable correlation among covariates.\nHowever, important note Ridge Regression select\nvariables, sense “replace” methods like stepwise \ninterest using smaller number explanatory variables. Furthermore,\ninterpretation Ridge Regression coefficient estimates \ngenerally difficult. LASSO regression estimates proposed \naddress two issues (stable predictions correlated\ncovariates present variable selection) simultaneously.","code":""},{"path":"ridge-regression.html","id":"effective-degrees-of-freedom","chapter":"5 Ridge regression","heading":"5.6 Effective degrees of freedom","text":"Intuitively, interpret “degrees freedom” number \n“free” parameters available us tuning \nfit / train\nmodel predictor, expect Ridge Regression estimator\nless\n“degrees freedom” regular least squares regression\nestimator, given solution constrained\noptimization problem. , course, informal\nargument, particularly since proper definition\n“degrees freedom”.general definition discussed class, called “effective\ndegrees freedom” (EDF), reduces trace “hat” matrix \nlinear predictor (including, limited , linear\nregression models), due Efron (Efron 1986).\nmay also want look recent papers \ncite one .easy (worth time ) see Ridge\nRegression estimator computed penalty / regularization\nparameter equal b, corresponding EDF sum \nratio eigenvalue X’X respect plus b\n(see formula lecture slides). compute EDF\nRidge Regression fit air pollution data \npenalty parameter considered fixed average optimal value\n20 runs 5-fold CV:","code":"\nairp <- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\ny <- as.vector(airp$MORT)\nxm <- as.matrix(airp[, -16])\nlibrary(glmnet)\nlambdas <- exp(seq(-3, 10, length = 50))\nset.seed(123)\nop.la <- 0\nfor (j in 1:20) {\n  tmp <- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\n  op.la <- op.la + tmp$lambda.min # tmp$lambda.1se\n}\nop.la <- op.la / 20\nxm <- scale(as.matrix(airp[, -16]), scale = FALSE)\nxm.svd <- svd(xm)\n(est.edf <- sum(xm.svd$d^2 / (xm.svd$d^2 + op.la)))\n#> [1] 12.99595"},{"path":"ridge-regression.html","id":"important-caveat","chapter":"5 Ridge regression","heading":"5.7 Important caveat!","text":"Note discussion EDF assumed \nmatrix defining linear predictor depend \nvalues response variable (depends matrix X),\ncase linear regression. fine \nRidge Regression estimators long penalty parameter\nchosen using data. typically case\npractice. Although general definition EDF still holds,\nlonger true Ridge Regression yields linear\npredictor, thus corresponding EDF may \nequal trace corresponding\nmatrix.","code":""},{"path":"lasso.html","id":"lasso","chapter":"6 LASSO","heading":"6 LASSO","text":"different approach perform kind variable selection may \nstable stepwise methods use L1 regularization term\n(instead L2 one used ridge regression). Notwidthstanding \ngeometric “interpretation” effect using L1 penalty,\ncan also argued L1 norm , cases, convex relaxation\n(envelope) “L0” norm (number non-zero elements). result,\nestimators based LASSO (L1-regularized regression) typically \nentries equal zero.Just case Ridge Regression,\nvalue penalty parameter increases, solutions\nL1 regularized problem change\nusual least squares estimator (regularization parameter equals\nzero) vector zeroes (penalty constant sufficiently\nlarge). One difference using L1 L2 penalty \nL1-regularized problem, usually finite value penalty term\nproduces solution zeroes, whereas L2 penalizations\ngenerally true.sequence solutions changing value penalty parameter\noften used way rank (“sequence”) explanatory variables, listing \norder “enter” (estimated coefficient changes \nzero non-zero value). can\nalso estimate MSPE solution (finite\ngrid values penalty parameter) select one \ngood prediction properties. \nestimated regression coefficients selected solution exactly zero \ncommonly said explanatory variables included\nchosen model.two main implementation LASSO R, one \nvia glmnet function (package glmnet), \nfunction lars package lars. , course,\ncompute estimators, different ways.first compute path LASSO solutions credit data\nused previous lectures:plot method can used show path solutions, just \nridge regression:Using lars::lars() obtain:lars returned object matrix regression estimators, one\nvalue penalty constant new coefficient “enters” \nmodel:presentation exploits fact LASSO regression estimators\npiecewise linear values regularization parameter \nvariable enters drops model.order select one LASSO estimator (among infinitely many \npossible) can use K-fold CV estimate MSPE \n(grid values penalty parameter, example), \nchoose one smallest estimated MSPE:Given random nature, always good idea run K-fold CV experiments\n:now repeat steps using implementation\nglmnet:ran CV :Zoom CV plot check 1-SE rule:returned object includes “optimal” value \npenalization parameter, can used \nfind corresponding estimates regression\ncoefficients, using method coef:can also use coef compute coefficients \nvalue penalty parameter. example \nshow coefficients corresponding\npenalty values exp(4) exp(4.5):","code":"\nx <- read.table(\"data/Credit.csv\", sep = \",\", header = TRUE, row.names = 1)\n# use non-factor variables\nx <- x[, c(1:6, 11)]\ny <- as.vector(x$Balance)\nxm <- as.matrix(x[, -7])\nlibrary(glmnet)\n# alpha = 1 - LASSO\nlambdas <- exp(seq(-3, 10, length = 50))\na <- glmnet(\n  x = xm, y = y, lambda = rev(lambdas),\n  family = \"gaussian\", alpha = 1, intercept = TRUE\n)\nplot(a, xvar = \"lambda\", label = TRUE, lwd = 6, cex.axis = 1.5, cex.lab = 1.2)\nlibrary(lars)\nb <- lars(x = xm, y = y, type = \"lasso\", intercept = TRUE)\nplot(b, lwd = 4)\n# see the variables\ncoef(b)\n#>         Income      Limit   Rating     Cards        Age Education\n#> [1,]  0.000000 0.00000000 0.000000  0.000000  0.0000000  0.000000\n#> [2,]  0.000000 0.00000000 1.835963  0.000000  0.0000000  0.000000\n#> [3,]  0.000000 0.01226464 2.018929  0.000000  0.0000000  0.000000\n#> [4,] -4.703898 0.05638653 2.433088  0.000000  0.0000000  0.000000\n#> [5,] -5.802948 0.06600083 2.545810  0.000000 -0.3234748  0.000000\n#> [6,] -6.772905 0.10049065 2.257218  6.369873 -0.6349138  0.000000\n#> [7,] -7.558037 0.12585115 2.063101 11.591558 -0.8923978  1.998283\nb\n#> \n#> Call:\n#> lars(x = xm, y = y, type = \"lasso\", intercept = TRUE)\n#> R-squared: 0.878 \n#> Sequence of LASSO moves:\n#>      Rating Limit Income Age Cards Education\n#> Var       3     2      1   5     4         6\n#> Step      1     2      3   4     5         6\n# select one solution\nset.seed(123)\ntmp.la <- cv.lars(\n  x = xm, y = y, intercept = TRUE, type = \"lasso\", K = 5,\n  index = seq(0, 1, length = 20)\n)\nset.seed(23)\ntmp.la <- cv.lars(\n  x = xm, y = y, intercept = TRUE, type = \"lasso\", K = 5,\n  index = seq(0, 1, length = 20)\n)\n# run 5-fold CV with glmnet()\nset.seed(123)\ntmp <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 1,\n  family = \"gaussian\", intercept = TRUE\n)\nplot(tmp, lwd = 6, cex.axis = 1.5, cex.lab = 1.2)\nset.seed(23)\ntmp <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 1,\n  family = \"gaussian\", intercept = TRUE\n)\nplot(tmp, lwd = 6, cex.axis = 1.5, cex.lab = 1.2)\nplot(tmp, lwd = 6, cex.axis = 1.5, cex.lab = 1.2, ylim = c(22000, 33000))\n# optimal lambda\ntmp$lambda.min\n#> [1] 0.04978707\n# coefficients for the optimal lambda\ncoef(tmp, s = tmp$lambda.min)\n#> 7 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                       s1\n#> (Intercept) -481.9460966\n#> Income        -7.5489897\n#> Limit          0.1141714\n#> Rating         2.2352534\n#> Cards         10.7283522\n#> Age           -0.8914429\n#> Education      2.0194979\n# coefficients for other values of lambda\ncoef(tmp, s = exp(4))\n#> 7 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                        s1\n#> (Intercept) -262.35053476\n#> Income        -0.63094341\n#> Limit          0.02749778\n#> Rating         1.91772580\n#> Cards          .         \n#> Age            .         \n#> Education      .\ncoef(tmp, s = exp(4.5)) # note no. of zeroes...\n#> 7 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                        s1\n#> (Intercept) -175.98151842\n#> Income         .         \n#> Limit          0.01492881\n#> Rating         1.76170516\n#> Cards          .         \n#> Age            .         \n#> Education      ."},{"path":"lasso.html","id":"compare-mspes-of-ridge-lasso-on-the-credit-data","chapter":"6 LASSO","heading":"6.1 Compare MSPEs of Ridge & LASSO on the credit data","text":"now use 50 runs 5-fold cross-validation \nestimate (compare) MSPEs different\nestimators / predictors:see example LASSO seem provide better\npredictions Ridge Regression. However, yield \nsequence explanatory variables can interpreted \nbased “importance” linear regression model (see\n).","code":"\nlibrary(MASS)\nn <- nrow(xm)\nk <- 5\nii <- (1:n)%%k + 1\nset.seed(123)\nN <- 50\nmspe.la <- mspe.st <- mspe.ri <- mspe.f <- rep(0, N)\nfor (i in 1:N) {\n    ii <- sample(ii)\n    pr.la <- pr.f <- pr.ri <- pr.st <- rep(0, n)\n    for (j in 1:k) {\n        tmp.ri <- cv.glmnet(x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n            nfolds = 5, alpha = 0, family = \"gaussian\")\n        tmp.la <- cv.glmnet(x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n            nfolds = 5, alpha = 1, family = \"gaussian\")\n        null <- lm(Balance ~ 1, data = x[ii != j, ])\n        full <- lm(Balance ~ ., data = x[ii != j, ])\n        tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n        pr.ri[ii == j] <- predict(tmp.ri, s = \"lambda.min\", newx = xm[ii == j, ])\n        pr.la[ii == j] <- predict(tmp.la, s = \"lambda.min\", newx = xm[ii == j, ])\n        pr.st[ii == j] <- predict(tmp.st, newdata = x[ii == j, ])\n        pr.f[ii == j] <- predict(full, newdata = x[ii == j, ])\n    }\n    mspe.ri[i] <- mean((x$Balance - pr.ri)^2)\n    mspe.la[i] <- mean((x$Balance - pr.la)^2)\n    mspe.st[i] <- mean((x$Balance - pr.st)^2)\n    mspe.f[i] <- mean((x$Balance - pr.f)^2)\n}\nboxplot(mspe.la, mspe.ri, mspe.st, mspe.f, names = c(\"LASSO\", \"Ridge\", \"Stepwise\",\n    \"Full\"), col = c(\"steelblue\", \"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1,\n    cex.lab = 1, cex.main = 2)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"},{"path":"lasso.html","id":"comparing-lasso-with-ridge-regression-on-the-air-pollution-data","chapter":"6 LASSO","heading":"6.2 Comparing LASSO with Ridge Regression on the air pollution data","text":"Let us compare Ridge Regression LASSO fits \nair pollution data. course, Ridge Regression fit\nLASSO fit mean fit obtained \noptimal value penalty constant chosen terms\ncorresponding estimated MSPE (\ngeneral estimated using K-fold cross validation).first load data use cv.glmnet() \nalpha = 0 select approximately optimal\nRidge Regression fit (makes calculation\napproximately optimal?).plot included illustration purposes .\nSimilarly, now compute approximately optimal LASSO fit,\nlook curve estimated MSPEs:interesting compare corresponding estimated regression coefficients,\nput side side two columns:Note several relatively similar, LASSO includes fewer .\npossible explanation particular correlation structure among \nexplanatory variables. specifically, groups \ncorrelated covariates present,\nLASSO tends choose one , whereas Ridge Regression tend\nkeep . formal statement see (Zou Hastie 2005, Lemma 2).important note observations regarding Ridge Regression\nLASSO fits trained air pollution data made \nreliable (stable, less variable) choice penalty parameter. example,\nmay want run 5-fold CV experiments several times take \naverage estimated optimal penalty parameters. simplify presentation\npurse , may good exercise reader \n.following heatmap pairwise correlations among explanatory variables\nreveals certain patterns may used explain difference\nmentioned . Note visualization method variables \ngrouped (“clustered”) according pairwise correlations order \nimprove interpretability plot. see later course\nparticular clustering method used (hierarchical clustering).","code":"\nairp <- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\ny <- as.vector(airp$MORT)\nxm <- as.matrix(airp[, names(airp) != \"MORT\"])\nlambdas <- exp(seq(-3, 10, length = 50))\n# Ridge Regression\nset.seed(23)\nair.l2 <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0,\n  family = \"gaussian\", intercept = TRUE\n)\nplot(air.l2)\n# LASSO\nset.seed(23)\nair.l1 <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 1,\n  family = \"gaussian\", intercept = TRUE\n)\nplot(air.l1)\ncbind(\n  round(coef(air.l2, s = \"lambda.min\"), 3),\n  round(coef(air.l1, s = \"lambda.min\"), 3)\n)\n#> 16 x 2 sparse Matrix of class \"dgCMatrix\"\n#>                   s1       s1\n#> (Intercept) 1129.267 1070.341\n#> PREC           1.493    1.420\n#> JANT          -0.999   -1.124\n#> JULT          -1.054   -0.877\n#> OVR65         -2.260    .    \n#> POPN          -1.621    .    \n#> EDUC          -8.280  -10.800\n#> HOUS          -1.164   -0.380\n#> DENS           0.005    0.003\n#> NONW           2.895    3.825\n#> WWDRK         -0.464    .    \n#> POOR           0.653    .    \n#> HC            -0.030    .    \n#> NOX            0.056    .    \n#> SO.            0.237    0.226\n#> HUMID          0.388    .\nlibrary(ggcorrplot)\nggcorrplot(cor(xm), hc.order = TRUE, outline.col = \"white\")"},{"path":"lasso.html","id":"compare-mspe-of-ridge-and-lasso-on-air-pollution-data","chapter":"6 LASSO","heading":"6.3 Compare MSPE of Ridge and LASSO on air pollution data","text":"Since focus properties resulting predictions, may \ninteresting compare estimated MSPE different models / predictors\nconsidered far: full linear model, model selected via stepwise + AIC,\nridge regression LASSO. usual, use 50 runs 5-fold CV, obtain\nfollowing boxplots:see marginal advantage LASSO, rather minor, \nthree methods seen far improve similar margins\npredictions obtained using full linear regression model.","code":"\nlibrary(MASS)\nn <- nrow(xm)\nk <- 5\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 50\nmspe.la <- mspe.st <- mspe.ri <- mspe.f <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.la <- pr.f <- pr.ri <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    tmp.ri <- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 0, family = \"gaussian\"\n    )\n    tmp.la <- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 1, family = \"gaussian\"\n    )\n    null <- lm(MORT ~ 1, data = airp[ii != j, ])\n    full <- lm(MORT ~ ., data = airp[ii != j, ])\n    tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = FALSE)\n    pr.ri[ii == j] <- predict(tmp.ri, s = \"lambda.min\", newx = xm[ii == j, ])\n    pr.la[ii == j] <- predict(tmp.la, s = \"lambda.min\", newx = xm[ii == j, ])\n    pr.st[ii == j] <- predict(tmp.st, newdata = airp[ii == j, ])\n    pr.f[ii == j] <- predict(full, newdata = airp[ii == j, ])\n  }\n  mspe.ri[i] <- mean((airp$MORT - pr.ri)^2)\n  mspe.la[i] <- mean((airp$MORT - pr.la)^2)\n  mspe.st[i] <- mean((airp$MORT - pr.st)^2)\n  mspe.f[i] <- mean((airp$MORT - pr.f)^2)\n}\nboxplot(mspe.la, mspe.ri, mspe.st, mspe.f, names = c(\"LASSO\", \"Ridge\", \"Stepwise\", \"Full\"), col = c(\"steelblue\", \"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1, cex.lab = 1, cex.main = 2)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"},{"path":"lasso.html","id":"less-desirable-properties-of-lasso","chapter":"6 LASSO","heading":"6.4 Less desirable properties of LASSO","text":"important LASSO estimator , properties may sometimes\nfully satisfactory. particular:LASSO selects right variables restrictive conditions (words, generally “variable selection”-consistent).LASSO sampling distribution one obtain standard least squares estimator knew features include ones exclude model (orther words, LASSO “oracle” property).groups correlated explanatory variables present LASSO tends include one variable (randomly) group, relegate others end sequence.precise statements theoretical results regarding three points , see (Zou Hastie 2005; Zou 2006).","code":""},{"path":"lasso.html","id":"elastic-net","chapter":"6 LASSO","heading":"6.5 Elastic net","text":"Elastic Net estimators introduced find \ninformative compromise LASSO Ridge Regression.Note cv.glmnet considers fits variying\nvalues one penalty constants, \none (alpha) kept fixed. compare different\nElastic Net fits run cv.glmnet 4 values \nalpha: 0.05, 0.1, 0.5 0.75.","code":"\n# EN\nset.seed(23)\nair.en.75 <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0.75,\n  family = \"gaussian\", intercept = TRUE\n)\nset.seed(23)\nair.en.05 <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0.05,\n  family = \"gaussian\", intercept = TRUE\n)\nset.seed(23)\nair.en.1 <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0.1,\n  family = \"gaussian\", intercept = TRUE\n)\nset.seed(23)\nair.en.5 <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0.5,\n  family = \"gaussian\", intercept = TRUE\n)\nplot(air.en.05)\nplot(air.en.5)\nplot(air.en.75)"},{"path":"lasso.html","id":"run-en-on-airpollution-data-compare-fits","chapter":"6 LASSO","heading":"6.5.1 Run EN on airpollution data, compare fits","text":"now compare estimates regression coefficients\nobtained different methods discussed far \nalleviate potential problems caused correlated covariates.comment made regarding need \nstable choice “optimal” fits (\nmethods) applies . , limit one\nrun 5-fold CV purely based simplifying \npresentation.","code":"\na <- cbind(\n  round(coef(air.l2, s = \"lambda.min\"), 3),\n  round(coef(air.l1, s = \"lambda.min\"), 3),\n  round(coef(air.en.05, s = \"lambda.min\"), 3),\n  round(coef(air.en.1, s = \"lambda.min\"), 3),\n  round(coef(air.en.5, s = \"lambda.min\"), 3),\n  round(coef(air.en.75, s = \"lambda.min\"), 3)\n)\ncolnames(a) <- c(\"Ridge\", \"LASSO\", \"EN-05\", \"EN-10\", \"EN-50\", \"EN-75\")\na\n#> 16 x 6 sparse Matrix of class \"dgCMatrix\"\n#>                Ridge    LASSO    EN-05    EN-10    EN-50    EN-75\n#> (Intercept) 1129.267 1070.341 1116.791 1112.228 1101.074 1099.067\n#> PREC           1.493    1.420    1.479    1.481    1.498    1.495\n#> JANT          -0.999   -1.124   -0.968   -0.990   -1.124   -1.153\n#> JULT          -1.054   -0.877   -1.036   -1.041   -1.156   -1.182\n#> OVR65         -2.260    .       -1.099   -0.265    .        .    \n#> POPN          -1.621    .        .        .        .        .    \n#> EDUC          -8.280  -10.800   -8.277   -8.413   -9.585  -10.147\n#> HOUS          -1.164   -0.380   -1.136   -1.102   -0.705   -0.575\n#> DENS           0.005    0.003    0.005    0.005    0.004    0.004\n#> NONW           2.895    3.825    3.187    3.454    3.816    3.895\n#> WWDRK         -0.464    .       -0.422   -0.391   -0.141   -0.052\n#> POOR           0.653    .        0.268    .        .        .    \n#> HC            -0.030    .       -0.006   -0.003    .        .    \n#> NOX            0.056    .        0.000    .        .        .    \n#> SO.            0.237    0.226    0.242    0.241    0.233    0.230\n#> HUMID          0.388    .        0.290    0.241    0.061    0.005"},{"path":"lasso.html","id":"compare-mspes-of-full-lasso-ridge-en-and-stepwise","chapter":"6 LASSO","heading":"6.5.2 Compare MSPE’s of Full, LASSO, Ridge, EN and stepwise","text":"see example Elastic Net alpha = 0.75 (far \nLASSO) provides slightly better estimated MSPEs.","code":"\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 50\nmspe.en <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.en <- rep(0, n)\n  for (j in 1:k) {\n    tmp.en <- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 0.75, family = \"gaussian\"\n    )\n    pr.en[ii == j] <- predict(tmp.en, s = \"lambda.min\", newx = xm[ii == j, ])\n  }\n  mspe.en[i] <- mean((airp$MORT - pr.en)^2)\n}\nboxplot(mspe.en, mspe.la, mspe.ri, mspe.st, mspe.f,\n  names = c(\"EN\", \"LASSO\", \"Ridge\", \"Stepwise\", \"Full\"),\n  col = c(\"hotpink\", \"steelblue\", \"gray80\", \"tomato\", \"springgreen\"),\n  cex.axis = 1, cex.lab = 1, cex.main = 2\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"},{"path":"non-parametric-regression.html","id":"non-parametric-regression","chapter":"7 Non-parametric regression","heading":"7 Non-parametric regression","text":"now turn attention situation \nregression function E(Y|X) necessarily linear.\nFurthermore, assume form \nunknown. knew regression\nfunction linear combination\nsine cosine function, “E(Y|X=x) = + b sin(x)\n+ c cos(x)”, , b c uknown,\nexample, problem fact linear\nregression problem. general,\ntrue regression function known (\nassumed) belong family functions can parametrize, \nestimation can done via standard least squares.\nInstead focus case \nregression function completely unknown.note next one discuss two ways estimating\n\\(E(Y|X)\\):one using bases (e.g. polynomial basis, spline basis); andone using kernels (aka local regression).simplify presentation (also \nintrinsic limitation methods, \ndiscussed detail later course), initially consider\ncase single explanatory variable\n(.e. X scalar, vector).","code":""},{"path":"non-parametric-regression.html","id":"polynomial-regression","chapter":"7 Non-parametric regression","heading":"7.1 Polynomial regression","text":"illustrate basis methods, consider \nlidar data, available package SemiPar. \ninformation available \ncorresponding help page: help(lidar, package='SemiPar').\nnow load data plot , response\nvariable logratio explanatory one \nrange:class discussed formal motivation look \npolynomial approximation regression function.\nargument, however, specify degree\napproximating polynomial use. \nfirst try 4th degree polynomial problem reduces linear\nregression one (see lecture slides). can use \ncommand like lm(logratio ~ range + range^2 + range^3 + range^4).\nHowever, call lm work intend \n(recommend check find reason ).\nInstead, need use something like\nlm(logratio ~ range + (range^2) + (range^3) + (range^4)).\navoid type long formula, can instead use\nfunction poly() R generate design matrix\ncontaining desired powers\nrange, plug call lm().\ncode fits two approximations (3rd degree \n4th degree polynomial), plots data\noverlays estimated regression function:Note fit reasonable, although probably\nroom improvement. Based \nformal motivation discussed class use polynomials first\nplace, may seem natural increase order\napproximating polynomial order improve quality\napproximation. However, \neasily seen good idea. compare \n4th degree approximation used (blue) \n10th degree one (red):Note 10th order fit follows data much closely, \nstarts become “adaptive” departing quite often \nmain (larger scale) trend associate regression (conditional\nmean) function.(Can explain discrepancy observe \nmotivation used class, suggests higher order\npolynomials provide better approximations?)","code":"\ndata(lidar, package = \"SemiPar\")\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\n# Degree 4 polynomials\npm <- lm(logratio ~ poly(range, 4), data = lidar)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(pm)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"blue\")\npm3 <- lm(logratio ~ poly(range, 3), data = lidar)\nlines(predict(pm3)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"hotpink\")\nlegend(\"topright\", legend = c(\"3rd degree\", \"4th degree\"), lwd = 6, col = c(\"hotpink\", \"blue\"))\n# Degree 10 polynomials\npm2 <- lm(logratio ~ poly(range, 10), data = lidar)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(pm)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"blue\")\nlines(predict(pm2)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"red\")"},{"path":"non-parametric-regression.html","id":"a-more-stable-basis-splines","chapter":"7 Non-parametric regression","heading":"7.2 A more stable basis: splines","text":"Part problem global polynomial bases ones used\nnecessarily become wiggly within range data, also quickly\nincrease decrease near edge observations. stable\nalso remarkably flexible basis given spline functions,\ndiscussed class.first show build naive linear spline basis \n5 knots (placed (1:5)/6 quantiles\n(.e. 0.17, 0.33, 0.5, 0.67, 0.83 percentiles) \nobserved values explanatory variable), use\nestimate regression function.\nRemember linear spline function knot w \ngiven f_w(x) = max( x - w, 0 ). Given fixed\nset pre-selected knots w_1, w_2, …, w_k,\nconsider regression functions linear combinations\ncorresponding k linear spline functions.Note higher-order splines (e.g. cubic splines\ndiscussed ), naive spline basis used numerically\nunstable, usually works poorly practice.\ninclude simply illustration \nmethodology stress point type approach\n(estimates regression function linear combination\nexplicit basis) fact nothing\nslightly complex linear models.First find 5 knots mentioned used \nconstruct spline basis:Now compute matrix “explanatory variables”, \n: matrix columns 5 basis\nfunctions f_1, f_2, …, f_5 evaluated\nn observed values (single) explanatory variable x_1, …, x_n. words, \nmatrix X (, j) cell value f_j(x_i),\nj=1, …, k, =1, …, n. code use (abuse?) R’s recycling rules\noperating vectors arrays (can spot ?)Now matrix “explanatory variables”, can simply use lm estimate\ncoefficients linear combination functions spline basis\nprovide regression function estimator. plot data overlay \nfitted / estimated regression function:better (numerically stable) bases linear\nspace spanned spline functions. bases different\nnumerical properties can become cumbersome describe. \nuse function bs (package splines) build B-spline basis.\naccessible discussion, see example (Wood 2017, sec. 4.1).Given chosen knots degree splines\n(linear, quadratic, cubic, etc.) set (linear space)\nfunctions using construct regression estimate\ndepend specific basis use\n(words: different bases span\nlinear space functions). consequence,\nestimated regression function regardless basis use\n(provided run serious numerical issues\nnaive basis). illustrate fact,\nuse B-spline basis 5 knots ,\ncompare estimated regression\nfunction one obtained \nusing poor people naive basis. plot overlays \nfits (naive one thick pink line , one\nusing b-splines thinner blue line):expected, fits provide estimated regression function, although\ncoefficients naturally different (lengths ,\ncoincidence, always happen?)Note , \nusing set linear splines, estimated regression functions always piecewise\nlinear (.e. linear functions pair knots). obtain smoother (e.g. differentiable,\ncontinuously differentiable, even twice continously differentiable)\nregression estimators use higher-order splines.","code":"\n# select the knots at 5 specific quantiles\n(kn <- as.numeric(quantile(lidar$range, (1:5) / 6)))\n#> [1] 444.6667 499.6667 555.0000 609.6667 664.6667\n# prepare the matrix of covariates / explanatory variables\nx <- matrix(0, dim(lidar)[1], length(kn) + 1)\nfor (j in 1:length(kn)) {\n    x[, j] <- pmax(lidar$range - kn[j], 0)\n}\nx[, length(kn) + 1] <- lidar$range\n# Fit the regression model\nppm <- lm(lidar$logratio ~ x)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(ppm)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"hotpink\")\nlibrary(splines)\nppm2 <- lm(logratio ~ bs(range, degree = 1, knots = kn), data = lidar)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(ppm)[order(range)] ~ sort(range), data = lidar, lwd = 8, col = \"hotpink\")\nlines(predict(ppm2)[order(range)] ~ sort(range), data = lidar, lwd = 3, col = \"darkblue\")\nas.vector(coef(ppm))\n#> [1]  0.0269095640  0.0002488169 -0.0003235802 -0.0082773735  0.0063779378\n#> [6]  0.0007385513 -0.0001847752\nas.vector(coef(ppm2))\n#> [1] -0.04515276 -0.01010104 -0.00657875 -0.02093988 -0.48762440 -0.60636798\n#> [7] -0.68496471"},{"path":"non-parametric-regression.html","id":"higher-order-splines-quadratic-cubic-etc.","chapter":"7 Non-parametric regression","heading":"7.3 Higher order splines (quadratic, cubic, etc.)","text":"follows () use function bs evaluate desired\nspline basis observed values explanatory variable (case range).use arguments degree = 2 knots = kn indicate want \nquadratic spline basis knots located elements vector kn.\n, simply use lm\nestimate coefficients, overlay estimated regression\nfunction data:useful consequence fact regression estimators fact\njust linear regression estimators\n(using richer / flexible basis just straight\npredictors) can easily compute (pointwise) standard errors\nfitted regression curve, follows. first fit \nplot quadratic\nspline using 5 knots :compute estimated standard error predicted regression curve grid\nvalues explanatory variable range, first build grid 200 equally spaced points\nwithin observed scope variable range:predict method objects class lm returns estimated standard\nerrors fitted value set argument\nse.fit = TRUE:now compute upper lower confidence bands (used 2 standard errors) around fitted regression line:Finally, display confidence bands just constructed (using base R graphics,\nalso consider using ggplot2):important note confidence bands constructed assuming knots fixed (random), similarly degree spline basis.Increasing degree cubic basis yields smoother fits (higher order continuous derivatives). example, using cubic splines yields even smoother fit:Note estimated regression function seems started “twitch”\nwiggle, particularly upper end observations.","code":"\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nppmq <- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nlines(predict(ppmq)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"steelblue\")\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nppmc <- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"gray30\")\nxx <- seq(min(lidar$range), max(lidar$range), length = 200)\nppmc <- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nps <- predict(ppmc, newdata = list(range = xx), se.fit = TRUE)\nup <- (ps$fit + 2 * ps$se.fit)\nlo <- (ps$fit - 2 * ps$se.fit)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 4, col = \"gray30\")\nmyrgb <- col2rgb(\"red\") / 256 # , alpha=TRUE)\nmyrgb <- rgb(red = myrgb[1], green = myrgb[2], blue = myrgb[3], alpha = .3)\npolygon(c(xx, rev(xx)), c(up, rev(lo)), density = NA, col = myrgb) #' lightblue')\nlines(ps$fit ~ xx, data = lidar, lwd = 4, col = \"blue\")\n# cubic splines\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nppmc <- lm(logratio ~ bs(range, degree = 3, knots = kn), data = lidar)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"tomato3\")"},{"path":"non-parametric-regression.html","id":"how-many-knots-should-we-use","chapter":"7 Non-parametric regression","heading":"7.4 How many knots should we use?","text":"far used 5 knots, used number knots. consider quadratic\nspline basis 10 knots, fit appears bit better (least aesthetically):using knots? following plot used 50 knots:Clearly good idea!","code":"\nk <- 10\nkn <- as.numeric(quantile(lidar$range, (1:k)/(k + 1)))\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nppmc <- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"tomato3\")\nk <- 50\nkn <- as.numeric(quantile(lidar$range, (1:k) / (k + 1)))\nppmc <- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"hotpink\")"},{"path":"non-parametric-regression.html","id":"smoothing-splines","chapter":"7 Non-parametric regression","heading":"7.5 Smoothing splines","text":"follow approach discussed far need \nfind “optimal” selecting number knots \npositions, plus order spline basis. Although one\nconsider using cross-validation , note \nrequire considerable computational effort (need\nperform exhaustive search 3-dimensional grid).saw class natural cubic splines provide natural\noptimal\nspace look good regression estimator. formal\nsurprisingly simple proof optimality result,\nsee Section 4.1 ofWood, S. (2006). Generalized additive models : introduction R.\nChapman & Hall/CRC,\nBoca Raton, FL. Library link.result justifies using natural cubic splines,\nalso eliminates many unknown “tuning parameters” (degree\nspline basis, number knots, locations).\nfact, need select one tuning parameter–penalty term, \ncan done using cross-validation “flavour” (although\nsetting leave-one-CV particularly appealing, \ndiscussed class).function smooth.spline R computes cubic smoothing spline\n(natural cubic spline). Details arguments different options\navailable help page.applied lidar data penalization\nparameter equal 0.2 (setting argument spar = 0.2)\nobtain following estimated regression function:fit clearly wiggly unsatisfactory. obtain smoother fit increase penalty term 0.5:larger penalty parameter, smoother fit. Setting 0.8 yields:easy see larger penalty coefficient closer\nresulting natural cubic spline becomes linear function (?).\nexample, use smooth.spline(spar=2):","code":"\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\ntmp <- smooth.spline(x = lidar$range, y = lidar$logratio, spar = 0.2, cv = FALSE,\n    all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 6, col = \"magenta\")\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\ntmp <- smooth.spline(x = lidar$range, y = lidar$logratio, spar = 0.5, cv = FALSE,\n    all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 6, col = \"red\")\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\ntmp <- smooth.spline(x = lidar$range, y = lidar$logratio, spar = 0.8, cv = FALSE,\n    all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 6, col = \"blue\")\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\ntmp <- smooth.spline(x = lidar$range, y = lidar$logratio, spar = 2, cv = FALSE, all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 6, col = \"tomato3\")"},{"path":"non-parametric-regression.html","id":"selecting-an-optimal-penalty-parameter","chapter":"7 Non-parametric regression","heading":"7.6 Selecting an “optimal” penalty parameter","text":"discussed class, “optimal” natural cubic spline can found using\ncross-validation, linear predictors, leave-one-cross-validation\nparticularly attractive (terms computational cost).\nfunction smooth.spline R compute (use) optimal value penalty term using\nleave-one-cross-validation set argument cv = TRUE:","code":"\ntmp.cv <- smooth.spline(x = lidar$range, y = lidar$logratio, cv = TRUE, all.knots = TRUE)\n# tmp.cv$spar = 0.974\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(tmp.cv$y ~ tmp.cv$x, lwd = 6, col = \"blue\")"},{"path":"non-parametric-regression.html","id":"sanity-check-always-a-good-idea","chapter":"7 Non-parametric regression","heading":"7.6.1 Sanity check (always a good idea)","text":"Note optimal value found regularization parameter (spar) \nalso returned element\n$spar object returned smooth.spline. Just sanity check\ncan now call smooth.spline cv = FALSE manually set\nspar optimal value, verify obtain fit:","code":"\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(tmp.cv$y ~ tmp.cv$x, lwd = 8, col = \"blue\")\ntmp <- smooth.spline(x = lidar$range, y = lidar$logratio, spar = tmp.cv$spar, cv = FALSE,\n    all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 3, col = \"red\")"},{"path":"non-parametric-regression.html","id":"the-problem-of-outliers-and-other-model-departures","chapter":"7 Non-parametric regression","heading":"7.7 The problem of outliers and other model departures","text":"data may contain outliers /atypical observations,\nestimation methods discussed may seriously affected, even\naberrant data points training set\n(possible outliers test / validation set also concern, \ndon’t time discuss ). robust estimation\nmethods based splines exist. See example (Tharmaratnam et al. 2010)\nreferences therein. Software R implementing method available\n.","code":""},{"path":"kernel-regression-local-regression.html","id":"kernel-regression-local-regression","chapter":"8 Kernel regression / local regression","heading":"8 Kernel regression / local regression","text":"different approach estimating regression function \nbased recalling true regression function\nf() = E(Y | X = ), mean response variable Y conditional\nevent explanatory variable(s) X equal(s) . \nlots data, , principle, think following\nintuitively simple regression estimator: given c, consider \nobservations (Y, X) training set X = c, take\nestimated f(c) \naverage corresponding observed values response variable\nY. resonable estimator E(Y | X = c ) (\nsufficient cases training data pairs X = c).Although simple approach usually work practice (\nenough training points X = c many values c),\nidea can still used construct regression estimator works\nlocally, .e. given c uses points training set\nX close c (can think working neighbourhood \nc). family regression estimators called\nlocal regression, kernel regression. latter name based\nfact use specific\nfamily functions (called kernels) define points\nneighbours \nused estimate regression function.\nNote kernel functions different used \nSupport Vector Machines reproducible kernel Hilbert spaces methods.Probably simplest kernel regression estimator simply take\naverage responses training points explanatory\nvariables within h point interest. “window width” h\ncalled bandwidth. can use function\nksmooth package KernSmooth R (\ngreat exercise write R function ). code \nconsiders one specific explanatory variable air pollution data\n(just illustration purposes) fits local averages regression\nestimator, bandwidth equal 50:Note gap estimated regression function. think\nhappened?increase bandwidth 50 60 obtain following estimated\nregression function:fit still rather unsatisfactory. example, note\nlooks like staircase. estimated regression curve\nfairly jagged, usually expect true regression\nfunction . Can explain regression estimator looks like ?discussed class, using smoother kernel function\nresults smoother estimated regression function. plot\nuses bandwidth , kernel function \nstandard gaussian density:Better properties estimated regression function obtained\none uses smooth kernel compact support (support \ngaussian density function whole real line thus \ncompact). reasons (better kernel regression estimators\nkernel compact support) rather technical \ndiscussed . good technical reference topics \nfollowing, available -line via Library:Nonparametric Semiparametric Models. (2004).\nHardle, W., Werwatz, ., Muller, M. Sperlich, S.\nSpringer-Verlag Berlin Heidelberg.\nDOI: 10.1007/978-3-642-17146-8In follows use R function loess\nimplements approach tri-cubic\nkernel given k() = ( 1 - (||)^3 )^3 || < 1, 0 otherwise.\nfollowing plot compares kernel gaussian one.\nSince important characteristics kernel shape support set,\nstandardized reach maximum value (1):","code":"\ndat <- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\nlibrary(KernSmooth)\nx <- dat$SO.\ny <- dat$MORT\nh <- 50\na <- ksmooth(x = x, y = y, kernel = \"box\", bandwidth = h, n.points = 1000)\nplot(y ~ x, pch = 19, col = \"gray\", cex = 1.3, xlab = \"SO.\", ylab = \"MORT\")\nlines(a$x, a$y, lwd = 4, col = \"blue\")\nh <- 60\na <- ksmooth(x = x, y = y, kernel = \"box\", bandwidth = h, n.points = 1000)\nplot(y ~ x, pch = 19, col = \"gray\", cex = 1.3, xlab = \"SO.\", ylab = \"MORT\")\nlines(a$x, a$y, lwd = 4, col = \"blue\")\nh <- 60\na <- ksmooth(x = x, y = y, kernel = \"normal\", bandwidth = h, n.points = 1000)\nplot(y ~ x, pch = 19, col = \"gray\", cex = 1.3, xlab = \"SO.\", ylab = \"MORT\")\nlines(a$x, a$y, lwd = 4, col = \"blue\")\ntt <- seq(-2, 2, length = 100)\ntmp <- dnorm(tt)\nplot(tt, tmp / max(tmp), ylab = \"Kernel\", xlab = \"\", lwd = 6, type = \"l\", col = \"gray40\", ylim = c(0, 1))\ntmp <- (1 - abs(tt)^3)^3\ntmp[abs(tt) > 1] <- 0\nlines(tt, tmp / max(tmp), lwd = 6, col = \"red\")"},{"path":"kernel-regression-local-regression.html","id":"fixed-versus-variable-bandwidths","chapter":"8 Kernel regression / local regression","heading":"8.0.1 Fixed versus variable bandwidths","text":"discussed class, fixed bandwidths may present problems practice\ndensity observed explanatory variables uniform\n(.e. dense regions observations \nsparse regions fewer observations). solution\nuse variable bandwidths, point c \ntake bandwidth large enough contain pre-specified proportion\nalpha data. function loess implements approach,\ndesired proportion observations neighbourhood given\nargument span.apply method (span = 0.5, degree = 0 indicate\nusing local averages) example , get following\nfit:Note, particular, upper end estimated regression function\nlooks better approach discussed using fixed bandwidths.Although yet discussed choose bandwidth (either fixed\nvariable) among infinitely many possible ones, expect \nalready know may done.","code":"\na <- loess(MORT ~ SO., data = dat, span = 0.5, degree = 0, family = \"gaussian\")\nplot(MORT ~ SO., data = dat, pch = 19, col = \"gray\", cex = 1.3, xlab = \"SO.\", ylab = \"MORT\")\ntmp <- order(a$x)\nlines(a$x[tmp], a$fitted[tmp], lwd = 4, col = \"steelblue\")"},{"path":"kernel-regression-local-regression.html","id":"local-regression-versus-local-means","chapter":"8 Kernel regression / local regression","heading":"8.0.2 Local regression versus local means","text":"discussed detail class, better way exploit \napproximating properties Taylor expansion, use locally.\nparticular, using kernels , can estimate regression\nfunction using linear function locally (corresponding \nTaylor expansion order 1), quadratic function (expansion \norder 2), etc. illustrate points ethanol\ndata package SemiPar. usual, information data\ncan found help page.load data compute local constant (degree = 0) regression\nestimator, response variable NOx explanatory variable \nE. span arbitrarily set 0.40 (discussed \ndetail ).Note regression estimator tends fit well tails data\n(.e. smallest largest observed values E). better fit\nobtained locally linear estimator (degree = 1), shown red, \nlocally constant one (blue):fit improvement previous one, \ncapture well peak data (around E = 0.90). easy see \nquadratic local fit might able , without affecting \nquality fit elsewhere. compare locally linear (red) \nlocally quadratic (dark green) fits:","code":"\ndata(ethanol, package = \"SemiPar\")\n# local constant\nspan <- .4\nb0 <- loess(NOx ~ E, data = ethanol, span = span, degree = 0, family = \"gaussian\")\nplot(NOx ~ E, data = ethanol, pch = 19, col = \"gray\", cex = 1.3, xlab = \"E\", ylab = \"NOx\")\ntmp <- order(b0$x)\nlines(b0$x[tmp], b0$fitted[tmp], lwd = 4, col = \"blue\")\n# local linear\nspan <- .4\nb1 <- loess(NOx ~ E, data = ethanol, span = span, degree = 1, family = \"gaussian\")\nplot(NOx ~ E, data = ethanol, pch = 19, col = \"gray\", cex = 1.3, xlab = \"E\", ylab = \"NOx\")\ntmp <- order(b1$x)\nlines(b1$x[tmp], b1$fitted[tmp], lwd = 4, col = \"red\")\nlines(b0$x[tmp], b0$fitted[tmp], lwd = 4, col = \"blue\")\n# local quad\nspan <- .4\nb2 <- loess(NOx ~ E, data = ethanol, span = span, degree = 2, family = \"gaussian\")\nplot(NOx ~ E, data = ethanol, pch = 19, col = \"gray\", cex = 1.3, xlab = \"E\", ylab = \"NOx\")\ntmp <- order(b2$x)\nlines(b1$x[tmp], b1$fitted[tmp], lwd = 4, col = \"red\")\nlines(b2$x[tmp], b2$fitted[tmp], lwd = 4, col = \"darkgreen\")"},{"path":"kernel-regression-local-regression.html","id":"choosing-the-bandwidth","chapter":"8 Kernel regression / local regression","heading":"8.0.3 Choosing the bandwidth","text":"easy see bandwidths plays similar role \none played penalty term smoothers based \nsplines bases. small bandwidth results \nestimator adaptive local quirks \ntraining set. Similarly, bandwidth large\nresult estimator essentially fit single\nglobal model whole data set.illustrate effect different choices \nbandwidths . take local quadratic (2nd degree\npolynomial) fit, small span (0.05):Larger spans result “better” fits, least sense \npleasant eye:range sensible acceptable values argument span loess\ndetermined, course, exact definition parameter. Information\ncan found corresponding help page, usual. probably know,\n“optimal” value span chosen using cross-validation. \ncouple good questions following:kernel regression estimators linear sense \nmatrix S fitted values equal S y, y \nvector responses training set, S depend y?use K-fold cross-validation choose “optimal” value span.","code":"\ntmp <- loess(NOx ~ E, data = ethanol, span = .05, degree = 2, family = \"gaussian\")\nplot(NOx ~ E, data = ethanol, pch = 19, col = \"gray\", cex = 1.5)\n# artificial grid of values to show predictions for the plot\nprs <- with(ethanol, seq(min(E), max(E), length = 1000))\nlines(predict(tmp, newdata = prs) ~ prs, data = ethanol, lwd = 4, col = \"steelblue\")\ntmp <- loess(NOx ~ E, data = ethanol, span = .25, degree = 2, family = \"gaussian\")\nplot(NOx ~ E, data = ethanol, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(tmp, newdata = prs) ~ prs, data = ethanol, lwd = 4, col = \"hotpink\")\ntmp <- loess(NOx ~ E, data = ethanol, span = .5, degree = 2, family = \"gaussian\")\nlines(predict(tmp, newdata = prs) ~ prs, data = ethanol, lwd = 4, col = \"darkgreen\")\nlegend(\"topleft\", legend = c(\"span: 0.25\", \"span: 0.50\"), col = c(\"hotpink\", \"darkgreen\"), lwd = 4)"},{"path":"kernel-regression-local-regression.html","id":"the-problem-of-outliers-and-other-model-departures-1","chapter":"8 Kernel regression / local regression","heading":"8.1 The problem of outliers and other model departures","text":"data may contain outliers /atypical observations,\nestimation methods discussed may seriously affected, even\naberrant data points training set\n(possible outliers test / validation set also concern, \ndon’t time discuss ). robust estimation\nmethods based kernel smoothers exist. See example (Boente, Martínez, Salibián-Barrera 2017)\nreferences therein. paper deals slightly \ncomplex model (additive model), component exists, \nmodel discussed class. RBF package\nimplementing method available CRAN\nalso .","code":""},{"path":"regression-trees.html","id":"regression-trees","chapter":"9 Regression trees","heading":"9 Regression trees","text":"Trees provide non-parametric regression estimator \nable overcome serious limitation “classical non-parametric”\nestimators (like based splines, kernels)\nseveral (2 3) explanatory variables \navailable.first describe problem afflicting classical\nnon-parametric methods (also known “curse dimensionality”)\ndescribe compute regression trees R using \nrpart package (although implementations exist).\nDetails discussed class.","code":""},{"path":"regression-trees.html","id":"curse-of-dimensionality","chapter":"9 Regression trees","heading":"9.1 Curse of dimensionality","text":"Suppose random sample n = 100 observations,\nuniformly distributed [0, 1] interval. many \nexpect find within 0.25 middle point \ninterval (.e. many 0.25 0.75)?\ntrivial calculation shows expected number \nobservations falling 0.25 0.75 n/2,\ncase 50. easy verified simple\nnumerical experiment:(wow! chances?)Consider now sample 100 observations, \n5 variables (5-dimensional observations),\nuniformly distributed 5-dimensional unit cube\n([0,1]^5). many expect see \ncentral hypercube sides [0.25, 0.75] x [0.25, 0.75] …\nx [0.25, 0.75] = [0.25, 0.75]^5? simple experiment shows\nnumber probably rather small:fact, expected number observations\ncentral hypercube exactly n / 2^5,\napproximately 3 n = 100.relevant question local regression estimation\nproblem : “large sample want\nstill 50 observations central hypercube?”.\nEasy calculations show number 50 / (1/2)^p,\n, p = 5 1600. , can verify\nsimple experiment:see dimension problem increases\np = 1 p = 5, number observations \nneed maintain expectation 50 points\ncentral hypercube increases factor 16 (5).\nHowever, double dimension problem (p = 10), order expect\n50 observations central [0.25, 0.75] hypercube\nneed sample size n = 51,200. words, \ndoubled dimension, need 32 times data (!)\nfill central hypercube number points.\nMoreover, doubled dimension (p = 20) need \n52 million observations (just!) 50 central hypercube!\nNote now doubled \ndimension need 1024 times data! number \nobservations needed maintain fixed number observations\nregion space grows exponentially \ndimension space.Another way think problem \nask: “given sample size n = 1000, say, wide / large\ncentral hypercube expect\n50 observations ?”. answer \neasily found 1 / (2 (n/50)^(1/p)), \nn = 1000 p = 5 equals 0.27, \np = 10 0.37 p = 20 \n0.43, almost full unit hypercube!sense fair say moderate high dimensions\nlocal neighbourhoods either empty really local.","code":"\n# X ~ U(0,1)\n# how many points do you expect within 0.25 of 1/2?\nset.seed(1234)\nn <- 100\nx <- runif(n)\n(sum(abs(x - 1 / 2) < 0.25)) # half the width of the dist'n\n#> [1] 50\np <- 5\nx <- matrix(runif(n * p), n, p)\n# how many points in the hypercube (0.25, 0.75)^p ?\ntmp <- apply(x, 1, function(a) all(abs(a - 1 / 2) < 0.25))\n(sum(tmp))\n#> [1] 4\n# how many obs do we need to have 50 in the hypercube?\nn <- 50 / (0.5^p)\nx <- matrix(runif(n * p), n, p)\n# how many points in the hypercube (0.25, 0.75)^p ?\ntmp <- apply(x, 1, function(a) all(abs(a - 1 / 2) < 0.25))\n(sum(tmp))\n#> [1] 57"},{"path":"regression-trees.html","id":"regression-trees-as-constrained-non-parametric-regression","chapter":"9 Regression trees","heading":"9.2 Regression trees as constrained non-parametric regression","text":"Regression trees provide alternative non-regression\nestimator works well, even many available features.\ndiscussed class, basic idea approximate \nregression function linear combination “simple”\nfunctions (.e. functions \\(h(x) = ( x \\)\\) equal\n1 argument x belongs set , 0 otherwise.\nfunction support set . Furthermore,\nlinear combination estimated , \niteratively, considering specific class \nsets (ones?) result, regression tree\nglobal optimal approximation simple functions,\ngood suboptimal one, can computed rapidly.\nDetails discussed class, refer notes \ncorresponding slides.several packages R implementing trees,\ncourse use rpart. illustrate \nuse consider Boston data set, contains\ninformation housing US city Boston. \ncorresponding help page contains information., simplify comparison predictions obtained\ntrees regression estimators, instead using\nK-fold CV, start randomly splitting \navailable data training test set:now build regression tree using function rpart leave \narguments \ndefault values. specify response explanatory variables using\nformula, usual, set method='anova' indicate \nwant train regression tree (opposed classification one, example).\nFinally, use corresponding plot method display tree\nstructure:questions :set pseudo-random generation seed (set.seed(123))\ncalling rpart? anything random building trees?uniform argument plot.rpart ? text ?","code":"\nlibrary(rpart)\ndata(Boston, package = \"MASS\")\n# split data into a training and\n# a test set\nset.seed(123456)\nn <- nrow(Boston)\nii <- sample(n, floor(n / 4))\ndat.te <- Boston[ii, ]\ndat.tr <- Boston[-ii, ]\nset.seed(123)\nbos.t <- rpart(medv ~ ., data = dat.tr, method = \"anova\")\nplot(bos.t, uniform = FALSE, margin = 0.05)\ntext(bos.t, pretty = TRUE)"},{"path":"regression-trees.html","id":"compare-predictions","chapter":"9 Regression trees","heading":"9.3 Compare predictions","text":"now compare predictions obtain test \nregression tree, usual linear model using \nexplanatory variables, another one constructed using stepwise\nvariable selections methods, “optimal” LASSO.First, estimate MSPE regression tree using test set:full linear model, estimated MSPE using test set :estimated MSPE linear model constructed via stepwise :Finally, estimated MSPE “optimal” LASSO fit :Note regression tree appears best MSPE, although\nreally assess whether observed differences beyond\nuncertainty associated MSPE estimators. words,\ndifferences still used different\ntraining / test data split? fact, good exercise \nrepeat comparison using many\ndifferent training/test splits, even better: using data \ntraining K-fold CV estimate different MSPEs.","code":"\n# predictions on the test set\npr.t <- predict(bos.t, newdata = dat.te, type = \"vector\")\nwith(dat.te, mean((medv - pr.t)^2))\n#> [1] 16.07227\n# full linear model\nbos.lm <- lm(medv ~ ., data = dat.tr)\npr.lm <- predict(bos.lm, newdata = dat.te)\nwith(dat.te, mean((medv - pr.lm)^2))\n#> [1] 23.25844\nlibrary(MASS)\nnull <- lm(medv ~ 1, data = dat.tr)\nfull <- lm(medv ~ ., data = dat.tr)\nbos.aic <- stepAIC(null, scope = list(lower = null, upper = full), trace = FALSE)\npr.aic <- predict(bos.aic, newdata = dat.te)\nwith(dat.te, mean((medv - pr.aic)^2))\n#> [1] 22.99864\n# LASSO?\nlibrary(glmnet)\nx.tr <- as.matrix(dat.tr[, -14])\ny.tr <- as.vector(dat.tr$medv)\nset.seed(123)\nbos.la <- cv.glmnet(x = x.tr, y = y.tr, alpha = 1)\nx.te <- as.matrix(dat.te[, -14])\npr.la <- predict(bos.la, s = \"lambda.1se\", newx = x.te)\nwith(dat.te, mean((medv - pr.la)^2))\n#> [1] 26.58914"},{"path":"pruning-regression-trees-with-rpart.html","id":"pruning-regression-trees-with-rpart","chapter":"10 Pruning regression trees with rpart","heading":"10 Pruning regression trees with rpart","text":"Important note: discussed class, K-fold CV methodology\nimplemented package rpart seems consider\nsequence trees (, equivalently, complexity parameters)\nbased full training set. details\nrefer corresponding documentation: pages 12 ff \npackage vignette, can accessed R using \ncommand vignette('longintro', package='rpart').\nalternative implementation CV-based pruning,\nplease see also Section “Pruning regression trees tree” .stopping criteria generally used fitting regression trees \ntake account explicitly complexity tree. Hence, \nmay end either overfitting tree, simple one,\ntypically results decline quality corresponding predictions.\ndiscussed class, one solution purposedly grow / train large overfitting\ntree, prune . One can also estimate corresponding MSPE\ntree prunning sequence choose optimal one.\nfunction rpart implements approach, illustrate \n.force rpart build large tree via arguments\nfunction rpart.control. time, obtain good\npicture evolution MSPE different subtrees, set smallest\ncomplexity parameter considered cross-validation\nexperiment low value (use 1e-8).surprisingly, predictions large tree \ngood:prune explore CP table returned \nrpart object find value complexity\nparameter optimal estimated prediction error. estimated\nprediction error subtree (corresponding value CP)\ncontained column xerror, associated\nstandard deviation column xstd. like find\nvalue CP yields corresponding pruned tree smallest\nestimated prediction error. function printcp shows \nCP table corresponding rpart object:probably better easier find \noptimal value programatically follows:can now use function\nprune rpart object setting complexity parameter\nestimated optimal value found :optimally pruned tree looks:Finally, can check predictions pruned\ntree test set:, good exercise \ncompare MSPE pruned tree several\nalternative methods seen class far,\nwithout using training / test split.","code":"\nlibrary(rpart)\ndata(Boston, package = \"MASS\")\n# split data into a training and\n# a test set\nset.seed(123456)\nn <- nrow(Boston)\nii <- sample(n, floor(n / 4))\ndat.te <- Boston[ii, ]\ndat.tr <- Boston[-ii, ]\n\nmyc <- rpart.control(minsplit = 2, cp = 1e-5, xval = 10)\nset.seed(123456)\nbos.to <- rpart(medv ~ .,\n  data = dat.tr, method = \"anova\",\n  control = myc\n)\nplot(bos.to, compress = TRUE) # type='proportional')\n# predictions are poor, unsurprisingly\npr.to <- predict(bos.to, newdata = dat.te, type = \"vector\")\nwith(dat.te, mean((medv - pr.to)^2))\n#> [1] 19.22826\nprintcp(bos.to)\n#> \n#> Regression tree:\n#> rpart(formula = medv ~ ., data = dat.tr, method = \"anova\", control = myc)\n#> \n#> Variables actually used in tree construction:\n#>  [1] age     black   chas    crim    dis     indus   lstat   nox     ptratio\n#> [10] rad     rm      tax     zn     \n#> \n#> Root node error: 32946/380 = 86.7\n#> \n#> n= 380 \n#> \n#>             CP nsplit  rel error  xerror     xstd\n#> 1   4.7150e-01      0 1.00000000 1.00363 0.094075\n#> 2   1.5701e-01      1 0.52850063 0.60388 0.063381\n#> 3   7.9798e-02      2 0.37149536 0.40412 0.050267\n#> 4   5.7540e-02      3 0.29169700 0.34109 0.048146\n#> 5   3.4802e-02      4 0.23415748 0.35907 0.054713\n#> 6   2.0424e-02      5 0.19935554 0.27486 0.041559\n#> 7   1.9408e-02      6 0.17893128 0.26826 0.041522\n#> 8   1.6414e-02      7 0.15952348 0.27163 0.041861\n#> 9   1.1118e-02      8 0.14310945 0.26680 0.041809\n#> 10  9.6449e-03      9 0.13199106 0.26576 0.048628\n#> 11  7.7292e-03     10 0.12234619 0.26166 0.047488\n#> 12  6.5545e-03     11 0.11461702 0.26243 0.047476\n#> 13  5.7344e-03     12 0.10806249 0.24154 0.044548\n#> 14  5.3955e-03     14 0.09659371 0.24499 0.043512\n#> 15  4.6018e-03     15 0.09119826 0.24284 0.043821\n#> 16  3.7390e-03     16 0.08659643 0.24439 0.044009\n#> 17  3.2170e-03     17 0.08285743 0.24188 0.044019\n#> 18  2.5445e-03     18 0.07964044 0.23957 0.043896\n#> 19  2.3205e-03     20 0.07455137 0.24288 0.043992\n#> 20  2.1485e-03     21 0.07223089 0.23905 0.043463\n#> 21  2.1316e-03     22 0.07008242 0.24546 0.044444\n#> 22  2.0477e-03     23 0.06795084 0.24556 0.044447\n#> 23  2.0283e-03     24 0.06590313 0.24493 0.044439\n#> 24  1.9878e-03     25 0.06387482 0.24448 0.044442\n#> 25  1.9781e-03     26 0.06188702 0.24495 0.044438\n#> 26  1.9686e-03     27 0.05990894 0.24495 0.044438\n#> 27  1.6400e-03     28 0.05794032 0.24296 0.044425\n#> 28  1.6357e-03     29 0.05630030 0.24257 0.044401\n#> 29  1.6212e-03     30 0.05466464 0.24257 0.044401\n#> 30  1.5386e-03     31 0.05304346 0.24272 0.044402\n#> 31  1.4205e-03     32 0.05150482 0.24609 0.044437\n#> 32  1.3390e-03     33 0.05008431 0.24671 0.044485\n#> 33  1.2731e-03     34 0.04874534 0.24823 0.044688\n#> 34  1.2294e-03     35 0.04747228 0.24985 0.044671\n#> 35  1.1693e-03     36 0.04624285 0.25214 0.044656\n#> 36  1.1587e-03     37 0.04507358 0.25609 0.044764\n#> 37  1.1306e-03     38 0.04391487 0.25484 0.044738\n#> 38  1.1235e-03     39 0.04278422 0.25484 0.044738\n#> 39  1.1117e-03     40 0.04166071 0.25420 0.044733\n#> 40  1.0183e-03     41 0.04054904 0.25240 0.044720\n#> 41  1.0016e-03     42 0.03953071 0.25440 0.044772\n#> 42  9.8001e-04     43 0.03852907 0.25480 0.044799\n#> 43  9.5959e-04     45 0.03656906 0.25607 0.044791\n#> 44  9.5612e-04     47 0.03464987 0.25828 0.044779\n#> 45  8.9091e-04     48 0.03369375 0.26231 0.045282\n#> 46  8.8600e-04     49 0.03280284 0.25879 0.044930\n#> 47  8.7103e-04     50 0.03191684 0.25836 0.044925\n#> 48  8.4075e-04     51 0.03104580 0.25901 0.044895\n#> 49  8.3105e-04     52 0.03020505 0.25848 0.044897\n#> 50  8.2287e-04     53 0.02937400 0.25882 0.044924\n#> 51  8.2159e-04     54 0.02855113 0.25893 0.044922\n#> 52  7.9802e-04     55 0.02772954 0.25984 0.044889\n#> 53  7.7379e-04     56 0.02693152 0.26006 0.044887\n#> 54  7.6674e-04     57 0.02615772 0.26006 0.044887\n#> 55  7.4051e-04     58 0.02539098 0.26070 0.044591\n#> 56  6.5174e-04     59 0.02465047 0.26100 0.044598\n#> 57  6.4506e-04     60 0.02399873 0.26196 0.044601\n#> 58  6.1748e-04     61 0.02335367 0.26243 0.044616\n#> 59  5.7918e-04     62 0.02273620 0.26455 0.044635\n#> 60  5.6590e-04     63 0.02215702 0.26531 0.044647\n#> 61  5.3958e-04     64 0.02159112 0.26456 0.044653\n#> 62  5.2778e-04     65 0.02105154 0.26771 0.045102\n#> 63  5.2595e-04     66 0.02052376 0.26878 0.045150\n#> 64  4.9608e-04     67 0.01999781 0.26887 0.045148\n#> 65  4.9581e-04     68 0.01950173 0.26876 0.045137\n#> 66  4.6477e-04     69 0.01900592 0.26899 0.045164\n#> 67  4.5562e-04     70 0.01854115 0.26883 0.045164\n#> 68  4.3208e-04     72 0.01762991 0.26818 0.045157\n#> 69  4.2934e-04     74 0.01676575 0.26768 0.045149\n#> 70  4.0512e-04     76 0.01590708 0.26831 0.045173\n#> 71  4.0437e-04     77 0.01550196 0.26865 0.045176\n#> 72  3.8959e-04     78 0.01509758 0.26928 0.045191\n#> 73  3.3745e-04     79 0.01470799 0.27223 0.045179\n#> 74  3.2839e-04     80 0.01437054 0.27232 0.045055\n#> 75  3.2113e-04     81 0.01404215 0.27316 0.045075\n#> 76  3.1358e-04     82 0.01372102 0.27216 0.044977\n#> 77  3.0960e-04     83 0.01340743 0.27273 0.045001\n#> 78  2.8639e-04     84 0.01309783 0.27342 0.045009\n#> 79  2.7607e-04     85 0.01281145 0.27447 0.045074\n#> 80  2.7189e-04     87 0.01225931 0.27379 0.045071\n#> 81  2.6958e-04     88 0.01198742 0.27385 0.045067\n#> 82  2.6552e-04     89 0.01171784 0.27361 0.045096\n#> 83  2.6115e-04     90 0.01145232 0.27350 0.045093\n#> 84  2.5749e-04     91 0.01119117 0.27350 0.045093\n#> 85  2.5578e-04     92 0.01093368 0.27280 0.045100\n#> 86  2.5257e-04     93 0.01067790 0.27277 0.045101\n#> 87  2.2556e-04     94 0.01042532 0.27385 0.045136\n#> 88  2.2386e-04     95 0.01019976 0.27266 0.045131\n#> 89  2.1854e-04     96 0.00997590 0.27266 0.045130\n#> 90  2.1012e-04     97 0.00975736 0.27363 0.045146\n#> 91  2.0946e-04     98 0.00954723 0.27444 0.045161\n#> 92  2.0776e-04     99 0.00933778 0.27444 0.045161\n#> 93  2.0488e-04    100 0.00913001 0.27230 0.044975\n#> 94  2.0296e-04    101 0.00892513 0.27225 0.044975\n#> 95  2.0035e-04    102 0.00872217 0.27223 0.044976\n#> 96  1.9446e-04    103 0.00852182 0.27232 0.044975\n#> 97  1.9166e-04    104 0.00832736 0.27133 0.044929\n#> 98  1.8824e-04    105 0.00813570 0.27103 0.044910\n#> 99  1.8713e-04    106 0.00794747 0.27072 0.044913\n#> 100 1.7808e-04    107 0.00776033 0.26983 0.044895\n#> 101 1.7610e-04    108 0.00758225 0.27000 0.044893\n#> 102 1.7325e-04    109 0.00740615 0.26984 0.044895\n#> 103 1.7018e-04    110 0.00723291 0.26968 0.044896\n#> 104 1.6527e-04    111 0.00706273 0.27027 0.044898\n#> 105 1.5789e-04    112 0.00689746 0.27057 0.044895\n#> 106 1.5735e-04    113 0.00673957 0.27046 0.044898\n#> 107 1.4751e-04    114 0.00658222 0.27066 0.044897\n#> 108 1.4632e-04    115 0.00643470 0.27055 0.044899\n#> 109 1.3986e-04    116 0.00628839 0.27039 0.044902\n#> 110 1.3925e-04    117 0.00614852 0.27111 0.044899\n#> 111 1.3479e-04    120 0.00573078 0.27113 0.044898\n#> 112 1.3357e-04    121 0.00559599 0.27084 0.044895\n#> 113 1.3245e-04    122 0.00546242 0.27097 0.044894\n#> 114 1.3171e-04    123 0.00532997 0.27101 0.044893\n#> 115 1.2728e-04    124 0.00519826 0.27137 0.044889\n#> 116 1.2691e-04    125 0.00507098 0.27085 0.044877\n#> 117 1.2493e-04    126 0.00494407 0.27066 0.044880\n#> 118 1.1699e-04    127 0.00481913 0.27148 0.044907\n#> 119 1.1655e-04    129 0.00458516 0.27168 0.044909\n#> 120 1.1542e-04    130 0.00446861 0.27209 0.044907\n#> 121 1.0244e-04    131 0.00435319 0.27047 0.044874\n#> 122 1.0244e-04    132 0.00425075 0.27085 0.044872\n#> 123 1.0205e-04    133 0.00414831 0.27085 0.044872\n#> 124 9.8401e-05    134 0.00404627 0.27127 0.044871\n#> 125 9.7938e-05    135 0.00394786 0.27064 0.044858\n#> 126 9.7938e-05    136 0.00384993 0.27069 0.044857\n#> 127 9.7128e-05    137 0.00375199 0.27069 0.044857\n#> 128 9.4118e-05    138 0.00365486 0.27006 0.044763\n#> 129 9.3663e-05    139 0.00356074 0.26991 0.044771\n#> 130 9.3243e-05    140 0.00346708 0.26991 0.044771\n#> 131 8.2635e-05    141 0.00337384 0.27063 0.044771\n#> 132 8.2635e-05    142 0.00329120 0.26996 0.044773\n#> 133 7.3547e-05    143 0.00320857 0.27017 0.044774\n#> 134 7.3049e-05    144 0.00313502 0.27061 0.044796\n#> 135 6.8395e-05    145 0.00306197 0.27067 0.044795\n#> 136 6.6928e-05    146 0.00299358 0.27005 0.044771\n#> 137 6.6928e-05    147 0.00292665 0.27005 0.044773\n#> 138 6.5562e-05    148 0.00285972 0.27007 0.044773\n#> 139 5.8916e-05    149 0.00279416 0.27023 0.044772\n#> 140 5.6726e-05    151 0.00267633 0.27085 0.044768\n#> 141 5.6471e-05    152 0.00261960 0.27079 0.044774\n#> 142 5.5090e-05    153 0.00256313 0.27079 0.044774\n#> 143 5.4263e-05    155 0.00245295 0.27081 0.044773\n#> 144 5.1296e-05    156 0.00239869 0.27094 0.044772\n#> 145 5.1296e-05    157 0.00234739 0.27146 0.044770\n#> 146 5.1053e-05    158 0.00229610 0.27146 0.044770\n#> 147 5.1003e-05    159 0.00224504 0.27146 0.044770\n#> 148 4.9576e-05    160 0.00219404 0.27117 0.044769\n#> 149 4.9308e-05    161 0.00214446 0.27118 0.044768\n#> 150 4.8615e-05    162 0.00209516 0.27114 0.044769\n#> 151 4.8615e-05    163 0.00204654 0.27154 0.044767\n#> 152 4.5354e-05    164 0.00199793 0.27154 0.044767\n#> 153 4.2544e-05    165 0.00195257 0.27175 0.044768\n#> 154 4.2519e-05    166 0.00191003 0.27179 0.044772\n#> 155 4.1488e-05    167 0.00186751 0.27179 0.044772\n#> 156 4.0759e-05    169 0.00178453 0.27173 0.044772\n#> 157 4.0675e-05    172 0.00166226 0.27164 0.044773\n#> 158 4.0141e-05    173 0.00162158 0.27112 0.044756\n#> 159 3.9661e-05    174 0.00158144 0.27111 0.044756\n#> 160 3.9133e-05    175 0.00154178 0.27111 0.044756\n#> 161 3.8851e-05    176 0.00150265 0.27118 0.044755\n#> 162 3.6878e-05    177 0.00146380 0.27161 0.044763\n#> 163 3.6524e-05    178 0.00142692 0.27163 0.044763\n#> 164 3.4197e-05    179 0.00139039 0.27160 0.044763\n#> 165 3.2895e-05    180 0.00135620 0.27153 0.044756\n#> 166 3.2781e-05    181 0.00132330 0.27172 0.044754\n#> 167 3.2438e-05    182 0.00129052 0.27183 0.044753\n#> 168 2.9746e-05    184 0.00122564 0.27187 0.044752\n#> 169 2.9503e-05    185 0.00119590 0.27228 0.044763\n#> 170 2.9381e-05    186 0.00116639 0.27228 0.044763\n#> 171 2.9381e-05    187 0.00113701 0.27228 0.044763\n#> 172 2.9139e-05    188 0.00110763 0.27228 0.044763\n#> 173 2.8420e-05    189 0.00107849 0.27255 0.044764\n#> 174 2.6761e-05    190 0.00105007 0.27232 0.044759\n#> 175 2.4484e-05    191 0.00102331 0.27260 0.044758\n#> 176 2.4282e-05    192 0.00099883 0.27181 0.044537\n#> 177 2.3311e-05    193 0.00097455 0.27213 0.044538\n#> 178 2.3083e-05    194 0.00095124 0.27216 0.044537\n#> 179 2.2309e-05    195 0.00092815 0.27216 0.044537\n#> 180 2.1930e-05    196 0.00090584 0.27171 0.044505\n#> 181 2.1854e-05    198 0.00086198 0.27169 0.044508\n#> 182 2.1854e-05    199 0.00084013 0.27169 0.044508\n#> 183 2.1409e-05    201 0.00079642 0.27169 0.044508\n#> 184 2.0325e-05    202 0.00077501 0.27181 0.044510\n#> 185 2.0235e-05    203 0.00075469 0.27120 0.044502\n#> 186 2.0235e-05    204 0.00073445 0.27120 0.044502\n#> 187 2.0235e-05    205 0.00071422 0.27120 0.044502\n#> 188 2.0235e-05    206 0.00069398 0.27120 0.044502\n#> 189 1.8439e-05    207 0.00067375 0.27120 0.044502\n#> 190 1.8363e-05    208 0.00065531 0.27111 0.044501\n#> 191 1.8363e-05    210 0.00061858 0.27113 0.044501\n#> 192 1.8363e-05    211 0.00060022 0.27113 0.044501\n#> 193 1.8262e-05    212 0.00058185 0.27113 0.044501\n#> 194 1.7099e-05    213 0.00056359 0.27096 0.044498\n#> 195 1.7099e-05    214 0.00054649 0.27094 0.044499\n#> 196 1.6390e-05    215 0.00052940 0.27108 0.044499\n#> 197 1.6390e-05    216 0.00051300 0.27106 0.044500\n#> 198 1.4620e-05    217 0.00049661 0.27091 0.044504\n#> 199 1.4620e-05    218 0.00048199 0.27104 0.044503\n#> 200 1.4610e-05    219 0.00046737 0.27104 0.044503\n#> 201 1.3380e-05    220 0.00045276 0.27124 0.044505\n#> 202 1.3380e-05    221 0.00043938 0.27143 0.044517\n#> 203 1.2950e-05    222 0.00042600 0.27144 0.044517\n#> 204 1.2950e-05    223 0.00041305 0.27144 0.044517\n#> 205 1.1382e-05    224 0.00040010 0.27168 0.044520\n#> 206 1.1382e-05    225 0.00038872 0.27179 0.044520\n#> 207 1.0927e-05    226 0.00037734 0.27176 0.044520\n#> 208 1.0118e-05    227 0.00036641 0.27189 0.044526\n#> 209 1.0118e-05    228 0.00035629 0.27189 0.044526\n#> 210 1.0000e-05    229 0.00034618 0.27189 0.044526\n(b <- bos.to$cptable[which.min(bos.to$cptable[, \"xerror\"]), \"CP\"])\n#> [1] 0.00214847\nbos.t3 <- prune(bos.to, cp = b)\nplot(bos.t3, uniform = FALSE, margin = 0.01)\ntext(bos.t3, pretty = FALSE)\n# predictions are better\npr.t3 <- predict(bos.t3, newdata = dat.te, type = \"vector\")\nwith(dat.te, mean((medv - pr.t3)^2))\n#> [1] 16.59113"},{"path":"pruning-regression-trees-with-rpart.html","id":"pruning-regression-trees-with-tree","chapter":"10 Pruning regression trees with rpart","heading":"10.1 Pruning regression trees with tree","text":"implementation trees R package tree follows\noriginal CV-based pruning strategy, discussed \nSection 3.4 bookBreiman, L., Friedman, J.H., Olshen, R.. Stone, C.J. (1984). Classification regression trees. Chapman & Hall.Section 7.2 :Ripley, Brian D. (1996). Pattern recognition neural networks. Cambridge University PressBoth books available electronic form UBC Library:\nBreiman et al. \nRipley, B.D..now use function tree::tree() fit regression\ntree . Note default stopping criteria \nimplementation regression trees different one \nrpart::rpart(), hence obtain results \nneed modify default stopping criteria using argument\ncontrol:plot resulting treeAs discussed , now fit large tree, \npruned later:now use function tree:cv.tree() estimate MSPE \nsubtrees bos.to2, using 5-fold CV, plot estimated\nMSPE (labeled “deviance”) function \ncomplexity parameter (, equivalently, size tree):Finally, use function prune.tree prune larger tree\n“optimal” size, estimated cv.tree :Compare pruned tree one obtained regression trees\nimplementation rpart. particular, can compare \npredictions pruned\ntree test set:Note predictions tree pruned tree\npackage seem better tree pruned \nrpart package. mean rpart gives\ntrees worse predictions tree data coming\nprocess generated training set?\nartifact specific test set used?\nCan think experiment check ?\n, good exercise \ncheck fit (tree rpart) gives pruned\ntrees better prediction properties case.","code":"\nlibrary(tree)\nbos.t2 <- tree(medv ~ ., data = dat.tr, control = tree.control(nobs = nrow(dat.tr), mincut = 6, minsize = 20))\nplot(bos.t2)\ntext(bos.t2)\nset.seed(123)\nbos.to2 <- tree(medv ~ .,\n  data = dat.tr,\n  control = tree.control(nobs = nrow(dat.tr), mincut = 1, minsize = 2, mindev = 1e-5)\n)\nplot(bos.to2)\nset.seed(123)\ntt <- cv.tree(bos.to2, K = 5)\nplot(tt)\nbos.pr2 <- prune.tree(bos.to2, k = tt$k[max(which(tt$dev == min(tt$dev)))])\nplot(bos.pr2)\ntext(bos.pr2)\n# predictions are worse than the rpart-pruned tree\npr.tree <- predict(bos.pr2, newdata = dat.te, type = \"vector\")\nwith(dat.te, mean((medv - pr.tree)^2))\n#> [1] 15.7194"},{"path":"pruning-regression-trees-with-rpart.html","id":"instability-of-regression-trees","chapter":"10 Pruning regression trees with rpart","heading":"10.2 Instability of regression trees","text":"Trees can rather unstable, sense small changes \ntraining data set may result relatively large differences \nfitted trees. simple illustration randomly split \nBoston data used two halves fit regression\ntree portion. display trees.Although expect random halves (moderately large)\ntraining set beat least qualitatively similar,\nNote two trees rather different.\ncompare stable predictor, fit linear\nregression model half, look two sets estimated\ncoefficients side side:Note estimated regression coefficients \nsimilar, least qualitatively comparable.","code":"\n# Instability of trees...\nlibrary(rpart)\ndata(Boston, package = \"MASS\")\nset.seed(123)\nn <- nrow(Boston)\nii <- sample(n, floor(n / 2))\ndat.t1 <- Boston[-ii, ]\nbos.t1 <- rpart(medv ~ ., data = dat.t1, method = \"anova\")\nplot(bos.t1, uniform = FALSE, margin = 0.01)\ntext(bos.t1, pretty = TRUE, cex = .8)\ndat.t2 <- Boston[ii, ]\nbos.t2 <- rpart(medv ~ ., data = dat.t2, method = \"anova\")\nplot(bos.t2, uniform = FALSE, margin = 0.01)\ntext(bos.t2, pretty = TRUE, cex = .8)\n# bos.lmf <- lm(medv ~ ., data=Boston)\nbos.lm1 <- lm(medv ~ ., data = dat.t1)\nbos.lm2 <- lm(medv ~ ., data = dat.t2)\ncbind(\n  round(coef(bos.lm1), 2),\n  round(coef(bos.lm2), 2)\n)\n#>               [,1]   [,2]\n#> (Intercept)  35.47  32.35\n#> crim         -0.12  -0.09\n#> zn            0.04   0.05\n#> indus         0.01   0.03\n#> chas          0.90   3.98\n#> nox         -23.90 -12.33\n#> rm            5.01   3.39\n#> age          -0.01   0.00\n#> dis          -1.59  -1.41\n#> rad           0.33   0.28\n#> tax          -0.01  -0.01\n#> ptratio      -1.12  -0.72\n#> black         0.01   0.01\n#> lstat        -0.31  -0.66"},{"path":"parametric-classifiers.html","id":"parametric-classifiers","chapter":"11 Parametric classifiers","heading":"11 Parametric classifiers","text":"discussed class, commonly referred classification\ncan thought prediction, responses classes \nuse particular loss function (0-1 loss discussed class).\nFurthermore, easy show (class) optimal\nclassifier (terms minimizing \nexpected misclassification error) one assigns observation\nclass highest probability occuring, conditional \nvalue observed explanatory variables.() classification methods cover course can \nsimply thought different approaches estimate conditional probability \nclass, conditional value explanatory variables. \nsymbols: P( G = g | X = x_0).\nobvious parallel done class,\nmany (?) regression methods discussed class \ndifferent ways estimating conditional mean response\nvariable (conditional value explanatory\nvariables). \nfact estimating whole conditional distribution G\ngiven X = x_0;\nsymbols: G | X = x_0.regression case, different ways estimate \noptimal predictor / classifier. model-based, \nnon-parametric nature. can considered “restricted”\nnon-parametric methods (without relying model, imposing \ntype constrain shape classifier). equivalent\nmethods regression continuous responses : linear\nnon-linear regression model-based methods; kernel local regression\nnon-parametric methods; splines regression trees \n“constrained” (regularized?) non-parametric methods.first discuss model-based methods (Linear / Quadratic\nDiscriminant Analysis logistic regression). non-parametric methods (nearest-neighbours \nclassification trees) discussed later.","code":""},{"path":"parametric-classifiers.html","id":"linear-discriminant-analysis","chapter":"11 Parametric classifiers","heading":"11.1 Linear Discriminant Analysis","text":"Probably “second easiest approach”estimate probability\n(easiest one?) model distribution \nexplanatory variables within class (, \nmodel distribution X | G = g\npossible class g).\nconditional distributions uniquely determine \nprobabilities need estimate, discussed class.\nparticular, one simplest models can use\nX | G = g\nNormal (Gaussian) multivariate distribution.\nsaw class, assume distribution features\nclass Gaussian common covariance matrix across clases, \neasy show (strongly suggest ) optimal\nclassifier (using 0-1 loss function mentioned ) linear\nfunction explanatory variables. coefficients linear\nfunction depend parameters assumed Gaussian distributions,\ncan estimated using MLE training set. Plugging \nparameter estimates P( G = g | X)\nprovides natural estimator conditional probabilities,\nthus can compute approximation optimal classifier.function lda \nMASS library implements simple classifier. illustrate \nrather simple well-known vaso constriction data, available \nrobustbase package. details, usual, can found \nhelp page. response variable takes two values (represented \nblue red), two\nexplanatory variables (allows us visualize methods results).train LDA classifier use function lda follows (note \nmodel-like syntax indicate response explanatory variables):Now, given value explanatory variables (Volume, Rate) \ncan use method predict object returned lda() \nestimate conditional probabilities blue red.visualize regions feature space predicted \ncontain blue points (obviously areas \npredicted correspond red responses) \nconstruct relatively fine 2-dimensional grid posible values \nexplanatory variables ((Volume, Rate)):estimate probabilities 2 classes point grid:Finally, plot corresponding “surface” predictions one class\n(.e. conditional probabilites class function \nexplanatory variables):plot higher numbers shown lighther colors\n(dark green corresponds low conditional probabilities).","code":"\ndata(vaso, package = \"robustbase\")\nplot(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\nlibrary(MASS)\na.lda <- lda(Y ~ Volume + Rate, data = vaso)\nxvol <- seq(0, 4, length = 200)\nxrat <- seq(0, 4, length = 200)\nthe.grid <- expand.grid(xvol, xrat)\nnames(the.grid) <- c(\"Volume\", \"Rate\")\npr.lda <- predict(a.lda, newdata = the.grid)$posterior\nimage(xrat, xvol, matrix(pr.lda[, 2], 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", main = \"LDA\"\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)"},{"path":"parametric-classifiers.html","id":"further-considerations","chapter":"11 Parametric classifiers","heading":"11.1.1 Further considerations","text":"model-based approach classification (LDA) optimal\nmodel correct. strongest assumption model\n, course, Gaussian conditional distribution vector \nexplanatory variables: X | G = g N( mu, Sigma) distribution.\nsecond strongest assumption \nequal “shape” (words, covariance matrix\nSigma depend g). latter assumption\ncan relaxed slightly \nassume instead features Gaussian distribution within \nclass, covariance matrix may different across\nclasses.\nsymbols, assume \nX | G = g N( mu, Sigma_g) distribution.\ncorresponding optimal classifier now quadratic\nfunction predictors (prove !). function qda\nMASS library implements classifier, \ncan used just like lda (usual, refer help page details).approach can used number classes. Can think limitations?","code":""},{"path":"parametric-classifiers.html","id":"logistic-regression-review","chapter":"11 Parametric classifiers","heading":"11.2 Logistic regression (Review)","text":"model distribution features within class using \nmultivariate Gaussian distribution, easy see \nboundaries classes linear functions features (verify !)\nFurthermore, log odds ratio classes linear\nfunction. interesting note one can start last\nassumption (instead full Gaussian model) arrive \nfully parametric model conditional distibution classes\ngiven features (see class slides). parameters can \nestimated using maximum likelihood. two classes \nlogistic regression model, may seen previous\ncourses.illustrate vaso data . Since \n2-class problem, just need fit logistic regression model.\nfunction glm R us, specify \nwant fit model using argument family=binomial.\nobtain parameter estimators (glm object ),\nuse predict method obtain predicted conditional\nprobabilities grid used :now plot data surface predicted probabilities \nblue points (higher probabilites displayed lighter colors).","code":"\na <- glm(Y ~ ., data = vaso, family = binomial)\npr <- predict(a, newdata = the.grid, type = \"response\")\nimage(xrat, xvol, matrix(pr, 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", main = \"Logistic\"\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)"},{"path":"qda.html","id":"qda","chapter":"12 QDA","heading":"12 QDA","text":"Similarly way derived LDA classifier class, one relaxes assumption\nconditional distribution vector features X class \ncovariance matrix (shape) (still assumes distributions \nGaussian), () easy find closed form conditional probability\nclass (conditional vector features X). LDA case,\nconditional class probabilities (aka posterior probabilities) depend \nparameters assumed model conditional distributions X \nclass. , , estimate parameters training set (usin observations\ngroup) plug compute conditional class probabilities.Similarly LDA, easy see case class\nboundaries quadratic functions vector features X.illustrate QDA vaso data used . first load \ndata, train QDA classifier using function qda package MASS\n(can also written MASS::qda()).now build relatively fine grid points domain \n2-dimensional vector features use predict method\nassociated qda object predict conditional probability \nclass blue:used function contour draw boundary \nclasses (set points probability blue equal \nprobability red).","code":"\ndata(vaso, package = \"robustbase\")\nlibrary(MASS)\na.qda <- qda(Y ~ ., data = vaso)\nxvol <- seq(0, 4, length = 200)\nxrat <- seq(0, 4, length = 200)\nxx <- expand.grid(xvol, xrat)\nnames(xx) <- c(\"Volume\", \"Rate\")\npr.qda <- predict(a.qda, newdata = xx)$posterior[, 2]\nimage(xrat, xvol, matrix(pr.qda, 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", cex.lab = 1.5, cex.axis = 1.5\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\ncontour(xrat, xvol, matrix(pr.qda, 200, 200),\n  col = \"gray30\", levels = .5,\n  drawlabels = FALSE, lwd = 3, add = TRUE\n)"},{"path":"qda.html","id":"sensitivity-to-the-gaussian-assumption","chapter":"12 QDA","heading":"12.1 Sensitivity to the Gaussian assumption","text":"discussed class (help simple example) sensitivity \nQDA assumed specific conditional distribution \nfeatures within class. easy see LDA may also\naffected similar problems. surprising–many\ncases optimal methods obtained certain conditions \nsensitive vailidity assumptions used derivation.interesting note (discussed class) logistic\nregression affected “good outliers” included \ndata. Considering “good outliers” (terms\ncorresponding likelihood values), probably\nsurprising. Note, furthermore, QDA (LDA) logistic regression\nclassifiers require estimation parameters (maybe \ncan call parametric classifiers?), cases considered far\nparameters estimated using maximum likelihood. However \nsensitivity kind outliers different.","code":""},{"path":"qda.html","id":"more-than-2-classes-the-handwritten-digit-recognition-data","chapter":"12 QDA","heading":"12.2 More than 2 classes – The handwritten digit recognition data","text":"may noted, classification methods seen far can\nused applications arbitrary number classes. now\nillustrate well-known Handwritten Digit Recognition Data\n(usual, see help(zip.train, package='ElemStatLearn')). first\nload data, extract images corresponding digits 0, 1 \n8. challenging enough discriminate given \nsimilar shapes.values pixes image rows \ncorresponding matrix (columns 2:256), true class image \nfirst column. Note relatively 8’s training set:display 16x16 images adapt simple function plot matrices:Next choose 9 images random training set,\ndisplay 3x3 array images:can also show “average 8” training set:now use LDA, QDA multinomial logistic model. \nlatter natural extension logistic regression \n2 classes. can easily derive assuming response\nvariable multinomial distribution modeling \nconditional probability (different) logistic function \nvector X features. Note K classes need\nmodel K-1 conditional class probabilities. derivation\nleft easy exercise .Note data stored matrix, use \nlda(), qda(), etc. clearer data\ndata frame (can refer features \nnames use data argument). , first transform\nmatrix data frame, name resulting variables\nV1, V2, …, V257:Now use lda multinom (last one package nnet) train\nLDA multinomial classifier 3-class data:(Question: remove variable V257 models ?)side commment: note slow convergence multinom. unusual,\nneural networks trained. Refer \ncorresponding help page information. probably\ndiscuss later course.now obtain predictions test set build \nmatrix classification errors classifier. LDA :logistic multinomial classifier :now attempt train QDA classifier:classifier trained data. problem \ntraining set least one class rank deficient (can\nfound looking error message stored returned\nobject .qdaIndeed, :questions :rank deficiency problem QDA, LDA, multinomial model?can anything train (possibly different) QDA classifier data?","code":"\ndata(zip.train, package = \"ElemStatLearn\")\ndata(zip.test, package = \"ElemStatLearn\")\nx.tr <- zip.train[zip.train[, 1] %in% c(0, 1, 8), ]\nx.te <- zip.test[zip.test[, 1] %in% c(0, 1, 8), ]\ntable(x.tr[, 1])\n#> \n#>    0    1    8 \n#> 1194 1005  542\n# ----- Define a function for plotting a matrix ----- #\n# modified from: http://www.phaget4.org/R/image_matrix.html\nmyImagePlot <- function(x) {\n  min <- min(x)\n  max <- max(x)\n  ColorRamp <- grey(seq(1, 0, length = 256))\n  ColorLevels <- seq(min, max, length = length(ColorRamp))\n  # Reverse Y axis\n  reverse <- nrow(x):1\n  x <- x[reverse, ]\n  image(1:ncol(x), 1:nrow(x), t(x),\n    col = ColorRamp, xlab = \"\",\n    ylab = \"\", axes = FALSE, zlim = c(min, max)\n  )\n}\na <- x.tr\nset.seed(987)\nsa <- sample(dim(a)[1], 9)\npar(mfrow = c(3, 3))\nfor (j in 1:9) {\n  myImagePlot(t(matrix(unlist(a[sa[j], -1]), 16, 16)))\n}\npar(mfrow = c(1, 1))\nmyImagePlot(t(matrix(colMeans(subset(x.tr, subset = (x.tr[, 1] == 8), select = -1)), 16, 16)))\n# alternatively: myImagePlot(t(matrix(colMeans(a[a[,1]==8,-1]), 16, 16)))\nx.tr <- data.frame(x.tr)\nx.te <- data.frame(x.te)\nnames(x.te) <- names(x.tr) <- paste(\"V\", 1:257, sep = \"\")\na <- lda(V1 ~ . - V257, data = x.tr) # x.tr[,1] ~ x[, 2:256])\nlibrary(nnet)\na.log <- multinom(V1 ~ . - V257, data = x.tr, maxit = 5000)\n#> # weights:  771 (512 variable)\n#> initial  value 3011.296283 \n#> iter  10 value 27.327939\n#> iter  20 value 8.491334\n#> iter  30 value 2.640128\n#> iter  40 value 1.228798\n#> iter  50 value 0.663474\n#> iter  60 value 0.391984\n#> iter  70 value 0.212952\n#> iter  80 value 0.114876\n#> iter  90 value 0.053465\n#> iter 100 value 0.026628\n#> iter 110 value 0.014534\n#> iter 120 value 0.009281\n#> iter 130 value 0.006623\n#> iter 140 value 0.004210\n#> iter 150 value 0.002723\n#> iter 160 value 0.001851\n#> iter 170 value 0.001318\n#> iter 180 value 0.001036\n#> iter 190 value 0.000580\n#> iter 200 value 0.000516\n#> iter 210 value 0.000304\n#> iter 220 value 0.000249\n#> iter 230 value 0.000218\n#> final  value 0.000090 \n#> converged\npr.lda <- predict(a, newdata = x.te)$class\ntable(pr.lda, x.te$V1)\n#>       \n#> pr.lda   0   1   8\n#>      0 353   2   9\n#>      1   0 258   0\n#>      8   6   4 157\npr.log <- predict(a.log, newdata = x.te)\ntable(pr.log, x.te$V1)\n#>       \n#> pr.log   0   1   8\n#>      0 342   3  13\n#>      1  12 258  10\n#>      8   5   3 143\na.qda <- try(qda(V1 ~ . - V257, data = x.tr))\n#> Error in qda.default(x, grouping, ...) : rank deficiency in group 0\nclass(a.qda)\n#> [1] \"try-error\"\na.qda\n#> [1] \"Error in qda.default(x, grouping, ...) : rank deficiency in group 0\\n\"\n#> attr(,\"class\")\n#> [1] \"try-error\"\n#> attr(,\"condition\")\n#> <simpleError in qda.default(x, grouping, ...): rank deficiency in group 0>\nx1 <- x.tr[x.tr$V1 == 0, ]\ndim(x1)\n#> [1] 1194  257\nqr(x1)$rank\n#> [1] 254"},{"path":"qda.html","id":"k-nearest-neighbours-k-nn","chapter":"12 QDA","heading":"12.3 K-Nearest Neighbours (K-NN)","text":"Perhaps intuitively simplest model-free estimator conditional class probabilities\ngiven set feature values X one based nearest neighbours\n(discussed class). similar (spirit) kernel regression\nestimator continuous-response regression setting. specifically,\ncan thought variable-bandwidth kernel estimator. \npoint X feature space look proportion observations\nclass among X’s K-th closest neighbours. , course, equivalent\nlooking points \\((Y_i, \\mathbf{X}_i)\\) training set \n\\(\\left\\| \\mathbf{X}_i - \\mathbf{X} \\right\\| \\le h_k\\), \\(h_k\\) \ndistance X K-th closest neighbour training set.\nRefer discussion class details.illustrate K-NN classifiers toy vaso example (able \nvisualize results easily), also hand written digits data.\nuse function knn package class. function takes \ntraining set, also test set (.e. different data set containing \nobservations predicted). example first create (\ndone ) 200 x 200 grid points display resulting\npredicted probabilities (corresponding class highest conditional\nprobability).first use trivial 1-NN classifier: estimated conditional probabilities\nclass point X, simply 0 1 depending class closest\nneighbour X training set.repeat analysis 5-NN classifier. Now estimated\nconditional probabilities\nX grid can 0, 0.20, 0.40, 0.60, 0.80 1 (?)\nfunction knn returns estimated probabilities \n'prob' attribute returned object, need use \nfunction attr extract (usual, R help pages \ngood source information questions \ncode ):now turn digits data. now look images\ndigits 1, 3 8 create corresponding\ntraining test sets:now train 1-, 5-, 10- 50-NN classifiers evaluate \ntest set. report misclassification rate test set,\nalong corresponding tables:Note performance K-NN classifier case\nstops improving K larger 5. Since number K \nnearest neighbours fact tuning constant needs \nchosen user, objective way?\ndidn’t test set available?","code":"\nlibrary(class)\ndata(vaso, package = \"robustbase\")\nx1 <- seq(0, 4, length = 200)\nx2 <- seq(0, 4, length = 200)\nxx <- expand.grid(x1, x2)\nu1 <- knn(train = vaso[, c(2, 1)], cl = vaso[, 3], test = xx, k = 1)\nu1 <- as.numeric(u1)\nimage(x1, x2, matrix(u1, 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", main = \"1-NN\"\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\nu5 <- attr(\n  knn(train = vaso[, c(2, 1)], cl = vaso[, 3], test = xx, k = 5, prob = TRUE),\n  \"prob\"\n)\nimage(x1, x2, matrix(u5, 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", main = \"5-NN\"\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\ndata(zip.train, package = \"ElemStatLearn\")\ndata(zip.test, package = \"ElemStatLearn\")\nx.tr <- data.frame(zip.train[zip.train[, 1] %in% c(1, 3, 8), ])\nx.te <- data.frame(zip.test[zip.test[, 1] %in% c(1, 3, 8), ])\nnames(x.te) <- names(x.tr) <- paste(\"V\", 1:257, sep = \"\")\nu1 <- knn(train = x.tr[, -1], cl = x.tr[, 1], test = x.te[, -1], k = 1)\ntable(u1, x.te$V1)\n#>    \n#> u1    1   3   8\n#>   1 261   0   0\n#>   3   3 162   9\n#>   8   0   4 157\nmean(u1 != x.te$V1)\n#> [1] 0.02684564\n\nu5 <- knn(train = x.tr[, -1], cl = x.tr[, 1], test = x.te[, -1], k = 5)\ntable(u5, x.te$V1)\n#>    \n#> u5    1   3   8\n#>   1 261   1   0\n#>   3   3 161   7\n#>   8   0   4 159\nmean(u5 != x.te$V1)\n#> [1] 0.02516779\n\nu10 <- knn(train = x.tr[, -1], cl = x.tr[, 1], test = x.te[, -1], k = 10)\ntable(u10, x.te$V1)\n#>    \n#> u10   1   3   8\n#>   1 261   1   3\n#>   3   3 163  12\n#>   8   0   2 151\nmean(u10 != x.te$V1)\n#> [1] 0.0352349\n\nu50 <- knn(train = x.tr[, -1], cl = x.tr[, 1], test = x.te[, -1], k = 50)\ntable(u50, x.te$V1)\n#>    \n#> u50   1   3   8\n#>   1 261   2   7\n#>   3   3 159  18\n#>   8   0   5 141\nmean(u50 != x.te$V1)\n#> [1] 0.05872483"},{"path":"qda.html","id":"challenges-for-k-nn-classifiers","chapter":"12 QDA","heading":"12.4 Challenges for K-NN classifiers","text":"easy see suffer curse dimensionality.Factor binary features need treated care.Euclidean distances reflect shape features class (.e. \nconditional distribution X class). Class-wise pre-standardization\n(whitening) might useful.illustrate last point, consider toy synthetic example discussed class:","code":"\n# create example\nset.seed(123)\nx <- matrix(runif(250 * 2, min = -1, max = 1), 250, 2)\nnorm2 <- function(a) sqrt(sum(a^2))\nr <- apply(x, 1, norm2)\na <- (r > .4) & (r < .7)\nx <- x[a, ]\n# plot(x, xlim=c(-1,1), ylim=c(-1,1))\nl1 <- (x[, 1] > 0)\nl2 <- (x[, 2] > 0)\na <- l1 & !l2\nb <- l1 & l2\nd <- !l1 & l2\nla <- rep(\"C\", nrow(x))\nla[a] <- \"A\"\nla[b] <- \"B\"\nla[d] <- \"D\"\n# plot(x, pch=la)\nx2 <- x\nx2[, 1] <- x2[, 1] * 1e5\n\n\n# plot(x2, pch=la, cex=1.5)\n#\n# # pick a point\n# points(x2[26,1], x2[26, 2], pch='A', col='red', cex=1.9)\n\n# find closest neighbour\nx0 <- x2[26, ]\nd <- apply(scale(x2, center = x0, scale = FALSE), 1, norm2)\nh <- sort(d)[2]\ne <- (1:nrow(x2))[d == h]\nplot(x2, pch = la, cex = 1.5, xlab = expression(X[1]), ylab = expression(X[2]))\npoints(x2[26, 1], x2[26, 2], pch = \"A\", col = \"red\", cex = 1.9)\npoints(x2[e, 1], x2[e, 2], pch = \"O\", col = \"red\", cex = 1.9)\ntext(-5000, 0, labels = \"Closest neighbour\", cex = 1.5, col = \"red\")\narrows(x2[26, 1], x2[26, 2] + .1, x2[e, 1], x2[e, 2] - .1, lwd = 5, col = \"red\")\n\n# pdf('knn-challenge.pdf', bg='transparent')\n# plot(x2, pch=la, cex=1.5, col='gray30', xlab='', ylab='')\n# points(x2[26,1], x2[26, 2], pch='A', col='red', cex=1.9)\n# points(x2[e,1], x2[e, 2], pch=19, col='red', cex=3)\n# arrows(x2[26, 1], x2[26,2] + .15, x2[e,1], x2[e,2]-.15, lwd=7, col='red')\n# text(-5000, 0, labels='Closest neighbour', cex=1.5, col='red')\n# dev.off()"},{"path":"classification-trees.html","id":"classification-trees","chapter":"13 Classification Trees","heading":"13 Classification Trees","text":"Just continuous regression case, number available\nexplanatory variables moderate large, methods like nearest neighbours\nquickly become unfeasible, performance satisfactory. Classification\ntrees provide good alternative: still model-free (need \nassume anything true conditional probabilities class given\nvector features X), constrained fairly specifc\nform. Intuitively (informally) say (nobody listening) \nrestriction provides form regularization penalization.Classification trees constructed much regression trees.\nconstruct partition feature space (“rectangular” areas),\nwithin region predict class common class\namong training points region. reasonable \ntry find partition feature space area \none class (least, one class clearly dominates others\nregion). Hence, build classification tree need quantitative measure \nhomogeneity classes present node. Given \nnumerical measure, can build tree\nselecting, step, optimal split sense yielding \nhomogeneous child leaves possible (.e. maximizing step \nchosen homogeneity measure). two common homogeneity measures\nGini Index deviance (refer discussion class).\nAlthough resulting trees generally different depending \nloss function used, later see difference \ncritical practice.usual, order able visualize going , \nillustrate training use classification trees simple toy example.\nexample contains data admissions graduate school. \n2 explanatory variables (GPA GMAT scores), response 3\nlevels: Admitted, decision, admitted. purpose \nbuild classifier decide student admitted \ngraduate school based /GMAT GPA scores.first read data, convert response proper factor\nvariable, visualize training set:Next build classification tree using Gini index splitting\ncriterion.use deviance splitting criterion instead, obtain following\nclassification tree (also using default stopping criteria):predicted conditional probabilities class range values\nexplanatory variables present training set can visualized\nexactly :display estimated conditional probabilities class:","code":"\nmm <- read.table(\"data/T11-6.DAT\", header = FALSE)\nmm$V3 <- as.factor(mm$V3)\n# re-scale one feature, for better plots\nmm[, 2] <- mm[, 2] / 150\nplot(mm[, 1:2],\n  pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]],\n  xlab = \"GPA\", \"GMAT\", xlim = c(2, 5), ylim = c(2, 5)\n)\nlibrary(rpart)\na.t <- rpart(V3 ~ V1 + V2, data = mm, method = \"class\", parms = list(split = \"gini\"))\nplot(a.t, margin = 0.05)\ntext(a.t, use.n = TRUE)\na.t <- rpart(V3 ~ V1 + V2, data = mm, method = \"class\", parms = list(split = \"information\"))\nplot(a.t, margin = 0.05)\ntext(a.t, use.n = TRUE)\naa <- seq(2, 5, length = 200)\nbb <- seq(2, 5, length = 200)\ndd <- expand.grid(aa, bb)\nnames(dd) <- names(mm)[1:2]\np.t <- predict(a.t, newdata = dd, type = \"prob\")\nfilled.contour(aa, bb, matrix(p.t[, 1], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\nfilled.contour(aa, bb, matrix(p.t[, 2], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\nfilled.contour(aa, bb, matrix(p.t[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)"},{"path":"classification-trees.html","id":"pruning","chapter":"13 Classification Trees","heading":"13.1 Pruning","text":"Just like regression trees, classification trees generally perform better built\npruning overfitting one. done way done classification\ntrees. graduate school admissions data indeed obtain estimated\nconditional probabilities appear sensible (less “simple”):","code":"\nset.seed(123)\na.t <- rpart(V3 ~ V1 + V2,\n  data = mm, method = \"class\", control = rpart.control(minsplit = 3, cp = 1e-8, xval = 10),\n  parms = list(split = \"information\")\n)\nb <- a.t$cptable[which.min(a.t$cptable[, \"xerror\"]), \"CP\"]\na.t <- prune(a.t, cp = b)\np.t <- predict(a.t, newdata = dd, type = \"prob\")\nfilled.contour(aa, bb, matrix(p.t[, 1], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\nfilled.contour(aa, bb, matrix(p.t[, 2], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\nfilled.contour(aa, bb, matrix(p.t[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)"},{"path":"bagging-for-regression.html","id":"bagging-for-regression","chapter":"14 Bagging for regression","heading":"14 Bagging for regression","text":"One strategy obtain stable predictors called\nBootstrap AGGregatING (bagging). can applied \nmany predictors (trees), generally results\nlarger improvements prediction quality used predictors\nflexible (low bias), highly variable.justification motivation discussed class. Intuitively\naveraging predictions obtained estimate \n“average prediction” computed access \nseveral (many?) independent training sets (samples).several (many?) R packages implementing\nbagging different predictors, varying degrees \nflexibility (implementations) user-friendliness.\nHowever, pedagogical illustrative purposes, notes \nbagg hand.","code":""},{"path":"bagging-for-regression.html","id":"bagging-by-hand","chapter":"14 Bagging for regression","heading":"14.0.1 Bagging by hand","text":", simplify discussion presentation, order evaluate\nprediction quality split \ndata (Boston) training test set. now:now train \\(N = 5\\) trees average predictions.\nNote , order illustrate process \nclearly, compute store \\(n_e \\times N\\)\npredictions two-dimensional array (aka matrix),\n\\(n_e\\) denotes number observations \ntest set. best (.e. efficient) way implementing bagging,\nmain purpose understand exactly . Also note \nalternative (better terms reusability \nensemble, maybe still efficient option) \nstore \\(N\\) trees directly. also allow \nelegant easy read code, \nillustrated , first use \nformer (hopefully clearer) strategy.First create array store predictions:last object (con) contains options train large\n(potentially overfitting) trees.\ndiscussed class, now generate N bootstrap samples\ngenerating vectors randomly sampled indices (replacement), \nrelevant lines code :train trees data set dat.tr[ii, ], boostrap sample.\n, trees compute corresponding (vector ) predictions \ntest set (predict(tmp, newdata=dat.te, type='vector')) store .\nPutting pieces together get:bagged predictions average predictions obtained tree \nensamble. words, point dat.te need compute average \npredictions obtained N different trees. way\nstored results matrix myps, bagged prediction point \ndat.te average corresponding row matrix myps. can compute\n(\\(n_e\\)) averages using rowMeans() follows:Finally, estimated MSPE bagged ensemble trees obtained specific\ntest set :can now compare similarly estimated\nMSPE pruned tree:quality bagged predictions improve use\nlarger ensemble? example, happens bagg \\(N = 10\\) trees?\\(N = 100\\) trees?\\(N = 1000\\) trees?Note , least test set, increasing number bagged trees\nseems improve MSPE. However, gain appears decrease, may\nworth computational effort use larger bag / ensemble.\nFurthermore, one may also want investigate whether \nartifact specific training / test partition, similar\npatterns MSPE observed random training / test splits.\ntry different test/training split repeat \nbagging experiment :pattern fact similar one observed :\nincreasing size ensemble \\(N\\) helps, improvements\nbecome smaller certain value \\(N\\).\nstrongly encouraged verify repeating experiment\ndifferent train/test splits. Furthermore, good exercise explore\nhappens MSPE bagged ensembles (different values \\(N\\))\nMSPE estimated using cross-validation\n(instead using specific test set). !","code":"\nlibrary(rpart)\ndata(Boston, package = \"MASS\")\nset.seed(123456)\nn <- nrow(Boston)\nii <- sample(n, floor(n / 4))\ndat.te <- Boston[ii, ]\ndat.tr <- Boston[-ii, ]\nN <- 5\nmyps <- array(NA, dim = c(nrow(dat.te), N))\ncon <- rpart.control(minsplit = 3, cp = 1e-3, xval = 1)\nii <- sample(n.tr, replace = TRUE)\ntmp <- rpart(..., data = dat.tr[ii, ], ...)\nn.tr <- nrow(dat.tr)\nset.seed(123)\nfor (j in 1:N) {\n  ii <- sample(n.tr, replace = TRUE)\n  tmp <- rpart(medv ~ ., data = dat.tr[ii, ], method = \"anova\", control = con)\n  myps[, j] <- predict(tmp, newdata = dat.te, type = \"vector\")\n}\npr.bagg <- rowMeans(myps)\nwith(dat.te, mean((medv - pr.bagg)^2))\n#> [1] 14.54751\nmyc <- rpart.control(minsplit = 2, cp = 1e-5, xval = 10)\nset.seed(123456)\nbos.to <- rpart(medv ~ .,\n  data = dat.tr, method = \"anova\",\n  control = myc\n)\nb <- bos.to$cptable[which.min(bos.to$cptable[, \"xerror\"]), \"CP\"]\nbos.t3 <- prune(bos.to, cp = b)\npr.t3 <- predict(bos.t3, newdata = dat.te, type = \"vector\")\nwith(dat.te, mean((medv - pr.t3)^2))\n#> [1] 16.59113\nN <- 10\nmyps <- array(NA, dim = c(nrow(dat.te), N))\nn.tr <- nrow(dat.tr)\nset.seed(123)\nfor (j in 1:N) {\n  ii <- sample(n.tr, replace = TRUE)\n  tmp <- rpart(medv ~ ., data = dat.tr[ii, ], method = \"anova\", control = con)\n  myps[, j] <- predict(tmp, newdata = dat.te, type = \"vector\")\n}\npr.bagg <- rowMeans(myps)\nwith(dat.te, mean((medv - pr.bagg)^2))\n#> [1] 13.97641\nN <- 100\nmyps <- array(NA, dim = c(nrow(dat.te), N))\nn.tr <- nrow(dat.tr)\nset.seed(123)\nfor (j in 1:N) {\n  ii <- sample(n.tr, replace = TRUE)\n  tmp <- rpart(medv ~ ., data = dat.tr[ii, ], method = \"anova\", control = con)\n  myps[, j] <- predict(tmp, newdata = dat.te, type = \"vector\")\n}\npr.bagg <- rowMeans(myps)\nwith(dat.te, mean((medv - pr.bagg)^2))\n#> [1] 12.10982\nN <- 1000\nmyps <- array(NA, dim = c(nrow(dat.te), N))\nn.tr <- nrow(dat.tr)\nset.seed(123)\nfor (j in 1:N) {\n  ii <- sample(n.tr, replace = TRUE)\n  tmp <- rpart(medv ~ ., data = dat.tr[ii, ], method = \"anova\", control = con)\n  myps[, j] <- predict(tmp, newdata = dat.te, type = \"vector\")\n}\npr.bagg <- rowMeans(myps)\nwith(dat.te, mean((medv - pr.bagg)^2))\n#> [1] 11.48381\nset.seed(1234)\nn <- nrow(Boston)\nii <- sample(n, floor(n / 4))\ndat.te <- Boston[ii, ]\ndat.tr <- Boston[-ii, ]\nNs <- c(5, 10, 100, 1000)\nall.results <- matrix(NA, length(Ns), 2)\ncolnames(all.results) <- c(\"N\", \"MSPE\")\nn.tr <- nrow(dat.tr)\nfor (hh in 1:length(Ns)) {\n  N <- Ns[hh]\n  myps <- array(NA, dim = c(nrow(dat.te), N))\n  set.seed(123)\n  for (j in 1:N) {\n    ii <- sample(n.tr, replace = TRUE)\n    tmp <- rpart(medv ~ ., data = dat.tr[ii, ], method = \"anova\", control = con)\n    myps[, j] <- predict(tmp, newdata = dat.te, type = \"vector\")\n  }\n  pr.bagg <- rowMeans(myps)\n  all.results[hh, ] <- c(N, with(dat.te, mean((medv - pr.bagg)^2)))\n}\nprint(all.results)\n#>         N     MSPE\n#> [1,]    5 18.27651\n#> [2,]   10 17.50426\n#> [3,]  100 14.52966\n#> [4,] 1000 14.27511"},{"path":"bagging-for-regression.html","id":"more-efficient-useful-and-elegant-implementation","chapter":"14 Bagging for regression","heading":"14.1 More efficient, useful and elegant implementation","text":"now illustrate possibly efficient way implement bagging, namely\nstoring \\(N\\) trees (rather predictions given data set).\nway one can re-use ensemble (future data set) without\nre-train elements bag. Since idea \n, just ensemble \\(N = 100\\) trees.\nsimplify comparison implementation \nbagging one used , first re-create\noriginal training / test splitNow, let’s create list 100 (empty) elements, element \nlist store regression tree:Now, train \\(N\\) trees , store list (without\ncomputing predictions):Given new data set, order obtain corresponding predictions \ntree ensemble, one either:loop \\(N\\) trees, averaging corresponding \\(N\\) vectors predictions; oruse sapply (check help page familiar apply functions R).later option results code much elegant,\nefficient (allowing future uses ensemble),\ncompact. course give exactly results. \nillustrate strategies. use first approach (loop)\nobtain following estimated MSPE using test set:(compare results obtained ). Using second approach (sapply):results course identical.","code":"\nset.seed(123456)\nn <- nrow(Boston)\nii <- sample(n, floor(n / 4))\ndat.te <- Boston[ii, ]\ndat.tr <- Boston[-ii, ]\nN <- 100\nmybag <- vector(\"list\", N)\nset.seed(123)\nfor (j in 1:N) {\n  ii <- sample(n.tr, replace = TRUE)\n  mybag[[j]] <- rpart(medv ~ ., data = dat.tr[ii, ], method = \"anova\", control = con)\n}\npr.bagg2 <- rep(0, nrow(dat.te))\nfor (j in 1:N) {\n  pr.bagg2 <- pr.bagg2 + predict(mybag[[j]], newdata = dat.te) / N\n}\nwith(dat.te, mean((medv - pr.bagg2)^2))\n#> [1] 12.10982\npr.bagg3 <- rowMeans(sapply(mybag, predict, newdata = dat.te))\nwith(dat.te, mean((medv - pr.bagg3)^2))\n#> [1] 12.10982"},{"path":"bagging-for-regression.html","id":"bagging-a-regression-spline","chapter":"14 Bagging for regression","heading":"14.2 Bagging a regression spline","text":"Bagging provide much advantage applied linear\npredictors (can explain ?) Nevertheless, let us try lidar data,\n, , randomly split training test set:Now fit cubic spline, estimate MSPE using test set:build ensemble 10 fits estimate corresponding\nMSPE using test set:Note estimated MSPE almost one \noriginal single spline.\nFurthermore, adding elements ensemble seem improve \nestimated MSPEs:","code":"\ndata(lidar, package = \"SemiPar\")\nset.seed(123456)\nn <- nrow(lidar)\nii <- sample(n, floor(n / 5))\nlid.te <- lidar[ii, ]\nlid.tr <- lidar[-ii, ]\nlibrary(splines)\na <- lm(logratio ~ bs(x = range, df = 10, degree = 3), data = lid.tr)\noo <- order(lid.tr$range)\npr.of <- predict(a, newdata = lid.te)\nmean((lid.te$logratio - pr.of)^2)\n#> [1] 0.007427559\nN <- 10\nmyps <- matrix(NA, nrow(lid.te), N)\nset.seed(123)\nn.tr <- nrow(lid.tr)\nfor (i in 1:N) {\n  ii <- sample(n.tr, replace = TRUE)\n  a.b <- lm(logratio ~ bs(x = range, df = 10, degree = 3), data = lid.tr[ii, ])\n  myps[, i] <- predict(a.b, newdata = lid.te)\n}\npr.ba <- rowMeans(myps) # , na.rm=TRUE)\nmean((lid.te$logratio - pr.ba)^2)\n#> [1] 0.007552562\nN <- 100\nmyps <- matrix(NA, nrow(lid.te), N)\nset.seed(123)\nn.tr <- nrow(lid.tr)\nfor (i in 1:N) {\n  ii <- sample(n.tr, replace = TRUE)\n  a.b <- lm(logratio ~ bs(x = range, df = 10, degree = 3), data = lid.tr[ii, ])\n  myps[, i] <- predict(a.b, newdata = lid.te)\n}\npr.ba <- rowMeans(myps) # , na.rm=TRUE)\nmean((lid.te$logratio - pr.ba)^2)\n#> [1] 0.0075887"},{"path":"bagging-for-classification.html","id":"bagging-for-classification","chapter":"15 Bagging for classification","heading":"15 Bagging for classification","text":"","code":""},{"path":"bagging-for-classification.html","id":"instability-of-trees-motivation","chapter":"15 Bagging for classification","heading":"15.1 Instability of trees (motivation)","text":"Just like regression case, classification\ntrees can highly unstable (specifically: relatively small\nchanges training set may result \ncomparably large changes corresponding tree).\nillustrate problem simple graduate\nschool admissions example\n(3-class 2-dimensional covariates) used class. First\nread data:transform response variable V3 factor (class labels\nrepresented R, rpart() expects values response\nvariable build classifier):obtain better looking plots later, now re-scale one features\n(explanatory variables similar ranges):use function rpart train classification tree data, using\ndeviance-based (information) splits:illustrate instability tree (.e. tree changes data perturbed\nslightly), create new training set (mm2) identical \noriginal one (mm), except two observations \nchange responses class 1 class 2:following plot contains new training set, two changed\nobservations (can find around point (GPA, GMAT) = (3, 4))\nhighlighted blue dot (new class) red ring around \n(old class “red”):, now train classification tree perturbed data mm2:visualize differences two trees build fine grid points\ncompare predicted probabilities class point grid.\nFirst, construct grid:Now, compute estimated conditional probabilities 3 classes\n40,000 points grid dd:next figures show estimated probabilities class “red” two\ntrees:Similarly, estimated conditional probabilities class “blue” point grid :finally, class “green”:Note , example, regions feature space (\nexplanatory variables) classified “red” “green” \ntrees trained original slightly changed training sets\nchange quite noticeably, even though difference training sets \nrelatively small. show ensemble classifiers\nconstructed via bagging can provide stable classifier.","code":"\nmm <- read.table(\"data/T11-6.DAT\", header = FALSE)\nmm$V3 <- as.factor(mm$V3)\nmm[, 2] <- mm[, 2] / 150\nlibrary(rpart)\na.t <- rpart(V3 ~ V1 + V2, data = mm, method = \"class\", parms = list(split = \"information\"))\nmm2 <- mm\nmm2[1, 3] <- 2\nmm2[7, 3] <- 2\nplot(mm2[, 1:2],\n  pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]],\n  xlab = \"GPA\", \"GMAT\", xlim = c(2, 5), ylim = c(2, 5)\n)\npoints(mm[c(1, 7), -3], pch = \"O\", cex = 1.1, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\na2.t <- rpart(V3 ~ V1 + V2, data = mm2, method = \"class\", parms = list(split = \"information\"))\naa <- seq(2, 5, length = 200)\nbb <- seq(2, 5, length = 200)\ndd <- expand.grid(aa, bb)\nnames(dd) <- names(mm)[1:2]\np.t <- predict(a.t, newdata = dd, type = \"prob\")\np2.t <- predict(a2.t, newdata = dd, type = \"prob\")\nfilled.contour(aa, bb, matrix(p.t[, 1], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n\nfilled.contour(aa, bb, matrix(p2.t[, 1], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm2[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]])\n    points(mm[c(1, 7), -3], pch = \"O\", cex = 1.1, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n  }\n)\n# blues\nfilled.contour(aa, bb, matrix(p.t[, 2], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n\nfilled.contour(aa, bb, matrix(p2.t[, 2], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  pane.last = {\n    points(mm2[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]])\n    points(mm[c(1, 7), -3], pch = \"O\", cex = 1.1, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n  }\n)\n# greens\nfilled.contour(aa, bb, matrix(p.t[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  }, panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n\nfilled.contour(aa, bb, matrix(p2.t[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  pane.last = {\n    points(mm2[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]])\n    points(mm[c(1, 7), -3], pch = \"O\", cex = 1.1, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n  }\n)"},{"path":"bagging-for-classification.html","id":"bagging-trees","chapter":"15 Bagging for classification","heading":"15.2 Bagging trees","text":"Just regression, bagging consists building ensemble \npredictors (case, classifiers) using bootstrap samples.\nusing B bootstrap samples, construct B classifiers, \ngiven point x, now B estimated conditional probabilities\npossible K classes.\nUnlike happens regression problems, now choice make\ndeciding combine B outputs point. can take\neither: majority vote B separate decisions, can\naverage B estimated probabilities K classes, obtain\nbagged estimated conditional probabilities. discussed illustrated\nclass, latter approach usually preferred.illustrate increased stability bagged classification trees,\nrepeat experiment : build ensemble 1000 classification\ntrees trained original data, second ensemble (also 1000\ntrees) using slightly modified data.\nensemble constructed exactly way \nregression case.\nfirst ensemble train NB = 1000 trees\nstore list (called ts) future use:","code":"\nmy.c <- rpart.control(minsplit = 3, cp = 1e-6, xval = 10)\nNB <- 1000\nts <- vector(\"list\", NB)\nset.seed(123)\nn <- nrow(mm)\nfor (j in 1:NB) {\n  ii <- sample(1:n, replace = TRUE)\n  ts[[j]] <- rpart(V3 ~ V1 + V2, data = mm[ii, ], method = \"class\", parms = list(split = \"information\"), control = my.c)\n}"},{"path":"bagging-for-classification.html","id":"using-the-ensemble","chapter":"15 Bagging for classification","heading":"15.2.1 Using the ensemble","text":"discussed class, two possible ways use ensemble given\nnew observation: can classify class votes among \nB bagged classifiers, can compute average conditional probabilities\nB classifiers, use average esimated conditional\nprobability. illustrate point (GPA, GMAT) = (3.3, 3.0).","code":""},{"path":"bagging-for-classification.html","id":"majority-vote","chapter":"15 Bagging for classification","heading":"15.2.1.1 Majority vote","text":"simplest, less elegant way compute votes class\nacross B trees ensemble loop \ncount:see class voted 1.calculation can made elegantly function sapply\n(lapply):","code":"\nx0 <- t(c(V1 = 3.3, V2 = 3.0))\nvotes <- vector(\"numeric\", 3)\nnames(votes) <- 1:3\nfor (j in 1:NB) {\n  k <- predict(ts[[j]], newdata = data.frame(x0), type = \"class\")\n  votes[k] <- votes[k] + 1\n}\n(votes)\n#>   1   2   3 \n#> 909   0  91\nvotes2 <- sapply(ts, FUN = function(a, newx) predict(a, newdata = newx, type = \"class\"), newx = data.frame(x0))\ntable(votes2)\n#> votes2\n#>   1   2   3 \n#> 909   0  91"},{"path":"bagging-for-classification.html","id":"average-probabilities-over-the-ensemble","chapter":"15 Bagging for classification","heading":"15.2.1.2 Average probabilities (over the ensemble)","text":"wanted compute average conditional probabilities across\nB different estimates, similar way. show \nusing sapply. strongly encouraged verify calculations\ncomputing average conditional probabilities using -loop., see class 1 much higher probability occuring\npoint.","code":"\nvotes2 <- sapply(ts, FUN = function(a, newx) predict(a, newdata = newx, type = \"prob\"), newx = data.frame(x0))\n(rowMeans(votes2))\n#> [1] 0.90881555 0.00000000 0.09118445"},{"path":"bagging-for-classification.html","id":"increased-stability-of-ensembles","chapter":"15 Bagging for classification","heading":"15.2.2 Increased stability of ensembles","text":"illustrate ensembles tree-based classifiers tend stable\nsingle tree, construct another example, time using\nslightly modified data. ensemble stored list ts2:use fine grid show \nestimated conditional probabilities, time\nobtained two ensembles.combine (average) NB = 1000 estimated probabilities\n3 classes 40,000 points\ngrid dd use function vapply\nstore result 3-dimensional array. \naveraged probabilities 1000 bagged trees\ncan obtained averaging across 3rd dimension.\napproach may intuitively clear \nfirst sight. strongly encouraged ignore code\ncompute bagged conditional probabilites \n3 classes point grid way \nclear . main goal understand method\nable . Efficient / elegant code\ncan written later, focus course.\nensemble trees trained original data:ensemble trees trained slightly modified data:plots show estimated conditional probabilities class “red”\npoint grid, two ensembles. Note\nsimilar (contrast results obtained \nwithout bagging):strongly encouraged obtain corresponding plots comparing\nestimated conditional probabilities ensembles\n2 classes (“blue” “green”).","code":"\nmm2 <- mm\nmm2[1, 3] <- 2\nmm2[7, 3] <- 2\nNB <- 1000\nts2 <- vector(\"list\", NB)\nset.seed(123)\nn <- nrow(mm)\nfor (j in 1:NB) {\n  ii <- sample(1:n, replace = TRUE)\n  ts2[[j]] <- rpart(V3 ~ V1 + V2, data = mm2[ii, ], method = \"class\", parms = list(split = \"information\"), control = my.c)\n}\naa <- seq(2, 5, length = 200)\nbb <- seq(2, 5, length = 200)\ndd <- expand.grid(aa, bb)\nnames(dd) <- names(mm)[1:2]\npp0 <- vapply(ts, FUN = predict, FUN.VALUE = matrix(0, 200 * 200, 3), newdata = dd, type = \"prob\")\npp <- apply(pp0, c(1, 2), mean)\npp02 <- vapply(ts2, FUN = predict, FUN.VALUE = matrix(0, 200 * 200, 3), newdata = dd, type = \"prob\")\npp2 <- apply(pp02, c(1, 2), mean)\nfilled.contour(aa, bb, matrix(pp[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\nfilled.contour(aa, bb, matrix(pp2[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm2[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]])\n    points(mm[c(1, 7), -3], pch = \"O\", cex = 1.2, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n  }\n)"},{"path":"random-forests.html","id":"random-forests","chapter":"16 Random Forests","heading":"16 Random Forests","text":"Even though using bagged ensemble trees usually results \nstable predictor / classifier, better ensemble can improved training\nmembers careful way. main idea try reduce \n(conditional) potential correlation among predictions bagged trees,\ndiscussed class. bootstrap trees ensemble \ngrown using randomly selected set features partitioning node.\nspecifically, node random subset explanatory variables\nconsidered determine optimal split. randomly chosen features\nselected independently node tree constructed.train Random Forest R use \nfuntion randomForest package name.\nsyntax rpart, tuning parameters\ntrees forest different rpart.\nRefer help page need modify .load prepare admissions data :train Random Forest 500 trees using default tuning parameters:Predictions can obtained using predict method, usual, \nspecify newdata argument. Refer help page\npredict.randomForest details different\nbehaviour predict Random Forest objects argument newdata \neither present missing.visualize predicted classes obtained Random Forest\nexample data, compute corresponding\npredicted conditional class probabilities \ngrid used :estimated conditional probabilities class red\nshown plot \n(estimated conditional probabilities computed exactly?)predicted conditional probabilities rest classes :interesting exercise train Random Forest perturbed data\n(mm2) verify predicted conditional probabilities change much,\ncase bagged classifier.","code":"\nmm <- read.table(\"data/T11-6.DAT\", header = FALSE)\nmm$V3 <- as.factor(mm$V3)\nmm[, 2] <- mm[, 2] / 150\nlibrary(randomForest)\na.rf <- randomForest(V3 ~ V1 + V2, data = mm, ntree = 500)\naa <- seq(2, 5, length = 200)\nbb <- seq(2, 5, length = 200)\ndd <- expand.grid(aa, bb)\nnames(dd) <- names(mm)[1:2]\npp.rf <- predict(a.rf, newdata = dd, type = \"prob\")\nfilled.contour(aa, bb, matrix(pp.rf[, 1], 200, 200),\n  col = terrain.colors(20),\n  xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3],\n      pch = 19, cex = 1.5,\n      col = c(\"red\", \"blue\", \"green\")[mm[, 3]]\n    )\n  }\n)\nfilled.contour(aa, bb, matrix(pp.rf[, 2], 200, 200),\n  col = terrain.colors(20),\n  xlab = \"GPA\", ylab = \"GMAT\", plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3],\n      pch = 19, cex = 1.5,\n      col = c(\"red\", \"blue\", \"green\")[mm[, 3]]\n    )\n  }\n)\nfilled.contour(aa, bb, matrix(pp.rf[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\",\n  ylab = \"GMAT\", plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)"},{"path":"random-forests.html","id":"another-example","chapter":"16 Random Forests","heading":"16.1 Another example","text":"now use interesting example. ISOLET data, available\n:\nhttp://archive.ics.uci.edu/ml/datasets/ISOLET,\ncontains data\nsound recordings 150 speakers saying letter \nalphabet (twice). See original source details. Since\nfull data set rather large, use subset\ncorresponding observations letters C Z.first load training test data sets, force response\nvariable categorical, R implementations \ndifferent predictors use build\nclassifiers regression counterparts:train Random Forest use function randomForest \npackage name. code underlying package originally\nwritten Leo Breiman. first train Random Forest, using default parametersWe now check performance test set:Note Random Forest makes one mistake 120 (approx 0.8%) observations\ntest set. However, OOB error rate estimate slightly 2%.\nnext plot shows evolution OOB error rate estimate function \nnumber classifiers ensemble (trees forest). Note 500 trees\nappears reasonable forest size, sense thate OOB error rate estimate stable.Consider ISOLET data, available\n:\nhttp://archive.ics.uci.edu/ml/datasets/ISOLET.\nuse subset\ncorresponding observations letters C Z.first load training test data sets, force response\nvariable categorical, R implementations \ndifferent predictors use build\nclassifiers regression counterparts:train Random Forest use function randomForest \npackage name. code underlying package originally\nwritten Leo Breiman. train RF leaving \nparamaters default values, check\nperformance test set:Note Random Forest makes one mistake 120 observations\ntest set. OOB error rate estimate slightly 2%,\nsee 500 trees reasonable forest size:","code":"\nxtr <- read.table(\"data/isolet-train-c-z.data\", sep = \",\")\nxte <- read.table(\"data/isolet-test-c-z.data\", sep = \",\")\nxtr$V618 <- as.factor(xtr$V618)\nxte$V618 <- as.factor(xte$V618)\nlibrary(randomForest)\nset.seed(123)\n(a.rf <- randomForest(V618 ~ ., data = xtr, ntree = 500))\n#> \n#> Call:\n#>  randomForest(formula = V618 ~ ., data = xtr, ntree = 500) \n#>                Type of random forest: classification\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 24\n#> \n#>         OOB estimate of  error rate: 2.29%\n#> Confusion matrix:\n#>      3  26 class.error\n#> 3  234   6  0.02500000\n#> 26   5 235  0.02083333\np.rf <- predict(a.rf, newdata = xte, type = \"response\")\ntable(p.rf, xte$V618)\n#>     \n#> p.rf  3 26\n#>   3  60  1\n#>   26  0 59\nplot(a.rf, lwd = 3, lty = 1)\nxtr <- read.table(\"data/isolet-train-c-z.data\", sep = \",\")\nxte <- read.table(\"data/isolet-test-c-z.data\", sep = \",\")\nxtr$V618 <- as.factor(xtr$V618)\nxte$V618 <- as.factor(xte$V618)\nlibrary(randomForest)\nset.seed(123)\na.rf <- randomForest(V618 ~ ., data = xtr, ntree = 500)\np.rf <- predict(a.rf, newdata = xte, type = \"response\")\ntable(p.rf, xte$V618)\n#>     \n#> p.rf  3 26\n#>   3  60  1\n#>   26  0 59\nplot(a.rf, lwd = 3, lty = 1)\na.rf\n#> \n#> Call:\n#>  randomForest(formula = V618 ~ ., data = xtr, ntree = 500) \n#>                Type of random forest: classification\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 24\n#> \n#>         OOB estimate of  error rate: 2.29%\n#> Confusion matrix:\n#>      3  26 class.error\n#> 3  234   6  0.02500000\n#> 26   5 235  0.02083333"},{"path":"random-forests.html","id":"using-a-test-set-instead-of-obb","chapter":"16 Random Forests","heading":"16.2 Using a test set instead of OBB","text":"Given case test set, can use \nmonitor error rate (instead using OOB error estimates):According help page plot method objects class\nrandomForest, following plot show error rates (OOB plus\ntest set):","code":"\nx.train <- model.matrix(V618 ~ ., data = xtr)\ny.train <- xtr$V618\nx.test <- model.matrix(V618 ~ ., data = xte)\ny.test <- xte$V618\nset.seed(123)\na.rf <- randomForest(x = x.train, y = y.train, xtest = x.test, ytest = y.test, ntree = 500)\ntest.err <- a.rf$test$err.rate\nma <- max(c(test.err))\nplot(test.err[, 2], lwd = 2, lty = 1, col = \"red\", type = \"l\", ylim = c(0, max(c(0, ma))))\nlines(test.err[, 3], lwd = 2, lty = 1, col = \"green\")\nlines(test.err[, 1], lwd = 2, lty = 1, col = \"black\")\nplot(a.rf, lwd = 2)"},{"path":"random-forests.html","id":"feature-sequencing-variable-ranking","chapter":"16 Random Forests","heading":"16.3 Feature sequencing / Variable ranking","text":"explore variables used forest,\nalso, importance rank discussed \nclass, can use function varImpPlot:","code":"\nvarImpPlot(a.rf, n.var = 20)"},{"path":"random-forests.html","id":"comparing-rf-with-other-classifiers","chapter":"16 Random Forests","heading":"16.4 Comparing RF with other classifiers","text":"now compare Random Forest classifiers saw class,\nusing classification error rate test set comparison measure.\nfirst start K-NN:use logistic regression first create new variable 1\nletter C 0 letter Z, use \nresponse variable.Question reader: think classifier’s performance\ndisappointing?interesting see simple LDA classifier :Finally, note carefully built classification tree\nperforms remarkably well, using 3 features:Finally, note train single classification tree \ndefault values stopping criterion tuning parameters, \ntree also uses 3 features, classification error rate\ntest set larger pruned one:","code":"\nlibrary(class)\nu1 <- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 1)\ntable(u1, xte$V618)\n#>     \n#> u1    3 26\n#>   3  57  9\n#>   26  3 51\n\nu5 <- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 5)\ntable(u5, xte$V618)\n#>     \n#> u5    3 26\n#>   3  58  5\n#>   26  2 55\n\nu10 <- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 10)\ntable(u10, xte$V618)\n#>     \n#> u10   3 26\n#>   3  58  6\n#>   26  2 54\n\nu20 <- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 20)\ntable(u20, xte$V618)\n#>     \n#> u20   3 26\n#>   3  58  5\n#>   26  2 55\n\nu50 <- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 50)\ntable(u50, xte$V618)\n#>     \n#> u50   3 26\n#>   3  58  7\n#>   26  2 53\nxtr$V619 <- as.numeric(xtr$V618 == 3)\nd.glm <- glm(V619 ~ . - V618, data = xtr, family = binomial)\npr.glm <- as.numeric(predict(d.glm, newdata = xte, type = \"response\") > 0.5)\ntable(pr.glm, xte$V618)\n#>       \n#> pr.glm  3 26\n#>      0 25 33\n#>      1 35 27\nlibrary(MASS)\nxtr$V619 <- NULL\nd.lda <- lda(V618 ~ ., data = xtr)\npr.lda <- predict(d.lda, newdata = xte)$class\ntable(pr.lda, xte$V618)\n#>       \n#> pr.lda  3 26\n#>     3  58  3\n#>     26  2 57\nlibrary(rpart)\nmy.c <- rpart.control(minsplit = 5, cp = 1e-8, xval = 10)\nset.seed(987)\na.tree <- rpart(V618 ~ ., data = xtr, method = \"class\", parms = list(split = \"information\"), control = my.c)\ncp <- a.tree$cptable[which.min(a.tree$cptable[, \"xerror\"]), \"CP\"]\na.tp <- prune(a.tree, cp = cp)\np.t <- predict(a.tp, newdata = xte, type = \"vector\")\ntable(p.t, xte$V618)\n#>    \n#> p.t  3 26\n#>   1 59  0\n#>   2  1 60\nset.seed(987)\na2.tree <- rpart(V618 ~ ., data = xtr, method = \"class\", parms = list(split = \"information\"))\np2.t <- predict(a2.tree, newdata = xte, type = \"vector\")\ntable(p2.t, xte$V618)\n#>     \n#> p2.t  3 26\n#>    1 57  2\n#>    2  3 58"},{"path":"boosting-a-statistical-learning-perspective.html","id":"boosting-a-statistical-learning-perspective","chapter":"17 Boosting (a Statistical Learning perspective)","heading":"17 Boosting (a Statistical Learning perspective)","text":"notes discuss boosting. starting point one first\nincarnations (Adaboost.M1 algorithm). goal two-fold: introduce boosting\ndifferent way building ensemble weak classifiers, \nalso show statistical analysis method offers valuable\ninsight opens wide range extensions new methodologies.\nfollow presentation Chapter 10 [HTF09].","code":""},{"path":"boosting-a-statistical-learning-perspective.html","id":"a-different-kind-of-ensembles","chapter":"17 Boosting (a Statistical Learning perspective)","heading":"17.1 A different kind of ensembles","text":"far course seen ensembles classifiers\n(regression estimators) based idea bagging: combininig\npredictions number predictors trained bootstrap\nsamples taken original training set. construction\npredictors ensemble treated equally (e.g. \npredictions receive weight \ncombined). Another characteristic ensembles \npredictors trained parallel\n(independent ).Boosting algorithms go back late 90s. One first ones\nappear Machine Learning literature probably Adaboost.M1\nintroduced inFreund, Y. Schapire, R. (1997). decision-theoretic generalization \nonline learning application boosting, Journal Computer\nSystem Sciences, 55:119-139.discussed specifics algorithm class. important\ndifference ensembles\ndiscussed class (can name ?)\nAdaboost.M1 elements\nensemble trained sequentially way \ncompute -th predictor \\(T_i\\) need \nprevious one \\(T_{-1}\\) available. Furthemore, \nweights final combination predictions generally\ndifferent member ensemble.use implementation available \nadabag package, specifically function\nboosting. function can rather slow, \nstraight implementation Adaboost algorithm,\nreturns many useful objects (e.g. \nindividual weak lerners, etc.) usual, suggest \ninvest minutes reading help\npages also exploring returned objects hand.Note Adaboost originally proposed 2-class\nproblems. illustrate use, look \nzip code digits example. consider problem \nbuilding classifier determine whether image\n1 9. use 1-split classification\ntrees weak lerners ensemble.\nSince boosting uses rpart implementation\nclassification regression trees,\nuse function rpart.control \nspecify type weak lerners want.first load full training set, extract \n7’s 9’s. Since original data file \nfeature names, create “V1”, “V2”,\netc.force rpart (thus boosting)\ntrain classification ensemble (opposed \nregression one) force response variable \ncategorical.Now load adabag package, use rpart.control force\nuse 1- 2-split trees, train boosting ensemble:can explore evolution error rate training set\n(equivalent MSE classifiers) using function\nerrorevol:Note approximately 10 iterations error rate \ntraining set drops zero stays . questions :algorithm converged approximately 10 iterations?predictors trained (approximately) 10th iteration irrelevant?know pretty well now, reliable measure \nexpected performance ensemble can obtained\nusing test set (cross-validation) (OOB?)First load full test set, extract cases corresponding\ndigits using , check performance\npredictor, including plot error rate\nfunction number elements ensemble:Just make sure boosting good job, \ncompare another ensemble classifier: Random Forest.\nuse number elements ensembles\n(500), even though complexity \ndifferent – boosting used stumps (1-split\ntrees), random forest trees (purposedly)\nlarge (deep).first train random forest\nlook error rates\ndisplayed plot method \nobjects class randomForest:Now evaluate performance Random Forest \ntraining set obtaining fitted values (“predictions” \nobservations training set) looking \ncorresponding “confusion table”:interesting question ask point : \n“confusion table” match information error plot ?\nCan describe (explain!) apparent problem?know well, course, classification error rate test set\nbetter measure predicition performance:see case random forest marginally worse\nboosting ensemble, even though ensemble elements\nusing boosting extremely simple trees.","code":"\ndata(zip.train, package = \"ElemStatLearn\")\nx.tr <- data.frame(zip.train)\nnames(x.tr) <- paste(\"V\", 1:257, sep = \"\")\nx.tr <- x.tr[x.tr$V1 %in% c(1, 9), ]\nx.tr$V1 <- as.factor(x.tr$V1)\nlibrary(adabag)\nonesplit <- rpart.control(cp = -1, maxdepth = 1, minsplit = 0, xval = 0)\nbo1 <- boosting(V1 ~ ., data = x.tr, boos = FALSE, mfinal = 500, control = onesplit)\nplot(errorevol(bo1, newdata = x.tr))\ndata(zip.test, package = \"ElemStatLearn\")\nx.te <- data.frame(zip.test)\nnames(x.te) <- paste(\"V\", 1:257, sep = \"\")\nx.te <- x.te[x.te$V1 %in% c(1, 9), ]\nx.te$V1 <- as.factor(x.te$V1)\ntable(x.te$V1, predict(bo1, newdata = x.te)$class)\n#>    \n#>       1   9\n#>   1 260   4\n#>   9   1 176\nplot(errorevol(bo1, newdata = x.te))\nset.seed(987)\nlibrary(randomForest)\na <- randomForest(V1 ~ ., data = x.tr) # , ntree=500)\nplot(a)\ntable(x.tr$V1, predict(a, newdata = x.tr, type = \"response\"))\n#>    \n#>        1    9\n#>   1 1005    0\n#>   9    0  644\npr.rf <- predict(a, newdata = x.te, type = \"response\")\ntable(x.te$V1, pr.rf)\n#>    pr.rf\n#>       1   9\n#>   1 259   5\n#>   9   1 176"},{"path":"boosting-a-statistical-learning-perspective.html","id":"another-example-1","chapter":"17 Boosting (a Statistical Learning perspective)","heading":"17.1.1 Another example","text":"Consider ISOLET data introduced earlier. consider\nbuilding classifier discriminate letters H\nbased features extracted sound recordings.\nsteps analysis :First load training setNext, force response categorical variable:Now train boosting ensamble evaluate test set\n(needs loaded well):can also look error evolution test set decide whether\nsmaller ensemble satisfactory:Finally, compare results obtained Random Forest:","code":"\nxtr <- read.table(\"data/isolet-train-a-h.data\", sep = \",\", header = TRUE)\nxtr$V618 <- as.factor(xtr$V618)\nonesplit <- rpart.control(cp = -1, maxdepth = 1, minsplit = 0, xval = 0)\nbo1 <- boosting(V618 ~ ., data = xtr, boos = FALSE, mfinal = 200, control = onesplit)\nxte <- read.table(\"data/isolet-test-a-h.data\", sep = \",\", header = TRUE)\nxte$V618 <- as.factor(xte$V618)\ntable(xte$V618, predict(bo1, newdata = xte)$class)\n#>    \n#>      1  8\n#>   1 59  1\n#>   8  0 60\nplot(errorevol(bo1, newdata = xte))\nset.seed(123)\na.rf <- randomForest(V618 ~ ., data = xtr, ntree = 200)\nplot(a.rf)\np.rf <- predict(a.rf, newdata = xte, type = \"response\")\ntable(xte$V618, p.rf)\n#>    p.rf\n#>      1  8\n#>   1 58  2\n#>   8  0 60"},{"path":"what-is-adaboost-doing-really.html","id":"what-is-adaboost-doing-really","chapter":"18 What is Adaboost doing, really?","heading":"18 What is Adaboost doing, really?","text":"Following work (Friedman, Hastie, Tibshirani 2000) (see also\nChapter 10 [ESL]), saw class Adaboost can \ninterpreted fitting additive model stepwise (greedy) way,\nusing exponential loss.\neasy prove Adaboost.M1\ncomputing approximation optimal classifier\nG( x ) = log[ P( Y = 1 | X = x ) / P( Y = -1 | X = x ) ] / 2,\noptimal taken respect exponential loss\nfunction. specifically, Adaboost.M1 using \nadditive model approximate function. words, Boosting \nattempting find functions \\(f_1\\), \\(f_2\\), …, \\(f_N\\) \n\\(G(x) = \\sum_i f_i( x^{()} )\\), \\(x^{()}\\) sub-vector\n\\(x\\) (.e. function \\(f_i\\) depends \navailable features, typically : 1 2, say). Note\n\\(f_i\\) generally depends different subset \nfeatures \\(f_j\\)’s.Knowing function boosting algorithm approximating (even\ngreedy suboptimal way), allows us \nunderstand algorithm expected work well,\nalso may work well.\nparticular, provides one way choose complexity \nweak lerners used construct ensemble. example\ncan refer corresponding lab activity.","code":""},{"path":"what-is-adaboost-doing-really.html","id":"a-more-challenging-example-the-email-spam-data","chapter":"18 What is Adaboost doing, really?","heading":"18.0.1 A more challenging example, the email spam data","text":"email spam data set relatively classic data set\ncontaining 57 features (potentially explanatory variables)\nmeasured 4601 email messages. goal predict\nwhether email spam . 57 features \nmix continuous discrete variables. information\ncan found \nhttps://archive.ics.uci.edu/ml/datasets/spambase.first load data randomly separate training \ntest set. thorough analysis use\nfull K-fold cross-validation, given computational\ncomplexity, decided leave rest \n3-fold CV exercise reader.now use Adaboost 500 iterations, using stumps (1-split\ntrees) \nweak learners / classifiers, check performance \ntest set:classification error rate test set 0.055. now\ncompare Random Forest look fit:Note OOB estimate classification error rate\n0.051.\nnumber trees used seems appropriate terms\nstability OOB error rate estimate:Now use test set estimate error rate Random Forest\n(fair comparison one computed boosting) obtainThe performance Random Forests test set better \nboosting (recall estimated classification error rate\n1-split trees-based Adaboost \n0.055, Random Forest 0.047 test set 0.051 using OOB).room improvement Adaboost?\ndiscussed class, depending interactions may \npresent true classification function, might able \nimprove boosting classifier slightly increasing complexity\nbase ensemble members. try use 3-split classification\ntrees, instead 1-split ones used :number elements boosting ensemble (500) appears \nappropriate look error rate test set \nfunction number boosting iterations:, fact, noticeable improvement performance \ntest set compared AdaBoost using stumps.\nestimated classification error rate AdaBoost using 3-split trees test set \n0.049. Recall estimated classification error rate\nRandom Forest 0.047\n(0.051 using OOB).mentioned strongly encouraged finish analysis\ncomplete K-fold CV analysis order compare boosting random\nforests data.","code":"\ndata(spam, package = \"ElemStatLearn\")\nn <- nrow(spam)\nset.seed(987)\nii <- sample(n, floor(n / 3))\nspam.te <- spam[ii, ]\nspam.tr <- spam[-ii, ]\nlibrary(adabag)\nonesplit <- rpart.control(cp = -1, maxdepth = 1, minsplit = 0, xval = 0)\nbo1 <- boosting(spam ~ ., data = spam.tr, boos = FALSE, mfinal = 500, control = onesplit)\npr1 <- predict(bo1, newdata = spam.te)\ntable(spam.te$spam, pr1$class) # (pr1$confusion)\n#>        \n#>         email spam\n#>   email   883   39\n#>   spam     45  566\nlibrary(randomForest)\nset.seed(123)\n(a <- randomForest(spam ~ ., data = spam.tr, ntree = 500))\n#> \n#> Call:\n#>  randomForest(formula = spam ~ ., data = spam.tr, ntree = 500) \n#>                Type of random forest: classification\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 7\n#> \n#>         OOB estimate of  error rate: 5.05%\n#> Confusion matrix:\n#>       email spam class.error\n#> email  1813   53  0.02840300\n#> spam    102 1100  0.08485857\nplot(a)\npr.rf <- predict(a, newdata = spam.te, type = \"response\")\ntable(spam.te$spam, pr.rf)\n#>        pr.rf\n#>         email spam\n#>   email   886   36\n#>   spam     36  575\nthreesplits <- rpart.control(cp = -1, maxdepth = 3, minsplit = 0, xval = 0)\nbo3 <- boosting(spam ~ ., data = spam.tr, boos = FALSE, mfinal = 500, control = threesplits)\npr3 <- predict(bo3, newdata = spam.te)\n(pr3$confusion)\n#>                Observed Class\n#> Predicted Class email spam\n#>           email   881   34\n#>           spam     41  577\nplot(errorevol(bo3, newdata = spam.te))"},{"path":"what-is-adaboost-doing-really.html","id":"an-example-on-improving-adaboosts-performance-including-interactions","chapter":"18 What is Adaboost doing, really?","heading":"18.0.2 An example on improving Adaboost’s performance including interactions","text":"error can’t track happens belowConsider data set file boost.sim.csv. \nsynthetic data inspired \nwell-known Boston Housing data. response variable class\ntwo predictors lon lat. read data setWe split data randomly training test set:, use stumps base classifiersand run 300 iterations boosting algorithm:examine evolution ensemble test set:note peformance disappointing improve \nnumber iterations. error rate test set \n.\nBased discussion class effect \ncomplexity base classifiers,\nnow increase slightly complexity: \nstumps trees 2 splits:Note error rate improves noticeably \n.\nInterestingly, note well increasing number\nsplits base classifiers seem \nhelp much. 3-split trees:error rate test set iswhile 4-split trees error rate isThe explanation response variables\ndata set fact generated\nfollowing relationship:\\(x = (x_1, x_2)^\\top\\). Since stumps (1-split trees)\ndefinition functions single\nvariable, boosting able approximate function using\nlinear combination , regardless many terms use. Two-split\ntrees, hand, able model interactions two\nexplanatory variables \\(X_1\\) (lon) \n\\(X_2\\) (lat), thus, sufficient terms sum, able \napproximate function relatively well., note analysis may depend specific\ntraining / test split used, strongly suggested \nre-using proper cross-validation setup.","code":"\nsim <- read.table(\"data/boost.sim.csv\", header = TRUE, sep = \",\", row.names = 1)\nset.seed(123)\nii <- sample(nrow(sim), nrow(sim) / 3)\nsim.tr <- sim[-ii, ]\nsim.te <- sim[ii, ]\nlibrary(rpart)\nstump <- rpart.control(cp = -1, maxdepth = 1, minsplit = 0, xval = 0)\nset.seed(17)\nsim1 <- boosting(class ~ ., data = sim.tr, boos = FALSE, mfinal = 300, control = stump)\nplot(errorevol(sim1, newdata = sim.te))\ntwosplit <- rpart.control(cp = -1, maxdepth = 2, minsplit = 0, xval = 0)\nset.seed(17)\nsim2 <- boosting(class ~ ., data = sim.tr, boos = FALSE, mfinal = 300, control = twosplit)\nplot(errorevol(sim2, newdata = sim.te))\nthreesplit <- rpart.control(cp = -1, maxdepth = 3, minsplit = 0, xval = 0)\nset.seed(17)\nsim3 <- boosting(class ~ ., data = sim.tr, boos = FALSE, mfinal = 300, control = threesplit)\nplot(errorevol(sim3, newdata = sim.te))\nfoursplit <- rpart.control(cp = -1, maxdepth = 4, minsplit = 0, xval = 0)\nset.seed(17)\nsim4 <- boosting(class ~ ., data = sim.tr, boos = FALSE, mfinal = 300, control = foursplit)\nround(predict(sim3, newdata = sim.te)$error, 4)\nround(predict(sim4, newdata = sim.te)$error, 4)log [ P ( Y = 1 | X = x ) / P ( Y = -1 | X = x ) ] / 2\n = [ max( x2 - 2, 0) - max( x1 + 1, 0) ] ( 1- x1 + x2 )"},{"path":"single-layer-neural-network.html","id":"single-layer-neural-network","chapter":"19 Single layer neural network","heading":"19 Single layer neural network","text":"example using ISOLET data illustrates use simple\nneural networks (NNs), also highlights issues may\nimportant aware. discussed class, NNs typically \nparameters observations number tuning parameters\nneed chosen user. Among : number \nhidden layers, number units layer, activation function,\nloss function, decaying factor, initial point\nstart optimization iterations. example illustrate\ndifficulties can encountered trying find\ntuning parameters use train NN.order focus concepts behind NN, use nnet\npackage R. package simple implementation\nNNs single hidden layer, relies standard optimization\nalgorithms train . simple setting allow us \nseparate implementation / optimization issues underlying\nmodel ideas behind NN, carry naturally \ncomplex NNs.example use ISOLET data available : http://archive.ics.uci.edu/ml/datasets/ISOLET, along information . contains data sound recordings 150 speakers saying letter alphabet (twice). See original source details. full data file rather large available compressed form.\nInstead, read private copy plain text form made\navailable Dropbox.","code":""},{"path":"single-layer-neural-network.html","id":"c-and-z","chapter":"19 Single layer neural network","heading":"19.1 “C” and “Z”","text":"First look building classifier identify letters C Z. \nsimplest scenario help us fix ideas. now read \nfull data set, extract training test rows corresponding \ntwo letters:train NN single hidden layer, single unit hidden layer.Note slow convergence. final value objective value :error rate training set (“goodness fit”) isWe see NN fits training set perfectly. desirable?now run algorithm , different starting point.Compare\nattained value objective error rate training set\n(6.482722 0, respectively):, see second run NN produces much worse solution.\nperformances test set?second (worse) solution performs better test set.add units hidden layer? increase \nnumber units hidden layer 3 6.objective functions arerespectively, performance training test sets :note (seemingly much) worse solution (terms objective\nfunction whose optimization defines NN) performs better\ntest set.add decaying factor form regularization?Now two solutions starting random initial values\n(reader encouraged \ntry random starts). NN training test sets?Note “regularized” solution corresponds \nslightly better solution worse one terms\nobjective function (still much worse best ones)\nperforms noticeably better test set. seem suggest\neasy select many local extrema used\nbased objective function values attain.Another tuning parameter can vary number units\nhidden layer, also increase significantly \nnumber possible weight parameters model.\nsolution uses 1858 weights. now add \nunits hidden layer (6 instead 3) increase limit \nnumber allowable weights 4000:Note two distinct solutions fit training set\nexactly (0 apparent error rate), performance\ntest set. leave reader perform \nexhaustive study prediction properties solutions\nusing appropriate CV experiment.","code":"\nlibrary(nnet)\nxx.tr <- readRDS(\"data/isolet-train.RDS\")\nxx.te <- readRDS(\"data/isolet-test.RDS\")\nlets <- c(3, 26)\nLETTERS[lets]\n#> [1] \"C\" \"Z\"\n# Training set\nx.tr <- xx.tr[xx.tr$V618 %in% lets, ]\nx.tr$V618 <- as.factor(x.tr$V618)\n# Test set\nx.te <- xx.te[xx.te$V618 %in% lets, ]\ntruth <- x.te$V618 <- as.factor(x.te$V618)\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 1, decay = 0, maxit = 1500, MaxNWts = 2000)\n#> # weights:  620\n#> initial  value 350.425020 \n#> iter  10 value 41.176789\n#> iter  20 value 18.095256\n#> iter  30 value 18.052107\n#> iter  40 value 18.050646\n#> iter  50 value 18.050036\n#> iter  60 value 18.048042\n#> iter  70 value 12.957465\n#> iter  80 value 6.911704\n#> iter  90 value 6.483384\n#> iter 100 value 6.482800\n#> iter 110 value 6.482764\n#> iter 120 value 6.482733\n#> final  value 6.482722 \n#> converged\na1$value\n#> [1] 6.482722\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0.002083333\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 1, decay = 0, maxit = 1500, MaxNWts = 2000)\n#> # weights:  620\n#> initial  value 336.934868 \n#> iter  10 value 157.630462\n#> iter  20 value 61.525474\n#> iter  30 value 48.367801\n#> iter  40 value 42.822797\n#> iter  50 value 36.541767\n#> iter  60 value 36.476799\n#> iter  70 value 33.120685\n#> iter  80 value 26.860343\n#> iter  90 value 26.859999\n#> final  value 26.859999 \n#> converged\na2$value\n#> [1] 26.86\nb2 <- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#> [1] 0.01041667\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.03333333\nb2 <- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#> [1] 0.04166667\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 3, decay = 0, maxit = 1500, MaxNWts = 2000, trace = FALSE)\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 3, decay = 0, maxit = 1500, MaxNWts = 2000, trace = FALSE)\na1$value\n#> [1] 6.482738\na2$value\n#> [1] 9.052402e-05\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0.002083333\nb2 <- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#> [1] 0\n\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.03333333\nb2 <- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#> [1] 0.04166667\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 3, decay = 0.05, maxit = 500, MaxNWts = 2000, trace = FALSE)\na1$value\n#> [1] 5.345279\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 3, decay = 0.05, maxit = 500, MaxNWts = 2000, trace = FALSE)\na2$value\n#> [1] 5.345279\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.008333333\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.05, maxit = 500, MaxNWts = 4000, trace = FALSE)\na1$value\n#> [1] 4.777806\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.05, maxit = 500, MaxNWts = 4000, trace = FALSE)\na2$value\n#> [1] 4.172023\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0\n\nb2 <- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#> [1] 0\n\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.008333333\n\nb2 <- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#> [1] 0.008333333"},{"path":"single-layer-neural-network.html","id":"more-letters","chapter":"19 Single layer neural network","heading":"19.2 More letters","text":"now repeat exercise 4-class\nsetting.following tries show NN \none unit hidden layer perform well.\n, compare two local minima NN training\nalgorithm. First show values \ncorresponding local minima objective function, \nerror rates training test sets.Note error rates test set \n0.462 \n0.537, \nhigh.\nBetter results obtained 6 units hidden layer\nslightly regularized solution. ,\nuse two runs training\nalgorithm look corresponding values \nobjective function, error rates\nNNs training test sets.error rates test set now\n0.013 \n0.013, \nmuch better .","code":"\nlets <- c(3, 7, 9, 26)\nx.tr <- xx.tr[xx.tr$V618 %in% lets, ]\nx.tr$V618 <- as.factor(x.tr$V618)\n# testing set\nx.te <- xx.te[xx.te$V618 %in% lets, ]\ntruth <- x.te$V618 <- as.factor(x.te$V618)\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 1, decay = 0, maxit = 1500, MaxNWts = 2000, trace = FALSE)\na1$value\n#> [1] 9.711177e-05\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 1, decay = 0, maxit = 1500, MaxNWts = 2000, trace = FALSE)\na2$value\n#> [1] 789.9009\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0\nb2 <- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#> [1] 0.4875\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.4625\nb2 <- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#> [1] 0.5375\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.05, maxit = 500, MaxNWts = 4000, trace = FALSE)\na1$value\n#> [1] 9.037808\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.05, maxit = 500, MaxNWts = 4000, trace = FALSE)\na2$value\n#> [1] 9.171046\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0\nb2 <- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#> [1] 0\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.0125\nb2 <- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#> [1] 0.0125"},{"path":"single-layer-neural-network.html","id":"even-more-letters","chapter":"19 Single layer neural network","heading":"19.3 Even more letters","text":"now consider building classifier 7 classes, \nchallenging problem.following code trains NN 6 units hidden layer \nmoderate regularization (via decaying factor 0.3 \nupper limit 4000 weights).Note case NN better objective\nfunction (100.5937916 versus 102.1805383) achieves better performance \ntest set (0.012\nversus 0.019), although \ndifference rather small. Conclusions based \nproper CV study much reliable.strongly encouraged study happens \ncombinations decay, number weights number units\nhidden layer, using proper CV setting evaluate\nresults.","code":"\nlets <- c(3, 5, 7, 9, 12, 13, 26)\nLETTERS[lets]\n#> [1] \"C\" \"E\" \"G\" \"I\" \"L\" \"M\" \"Z\"\nx.tr <- xx.tr[xx.tr$V618 %in% lets, ]\nx.tr$V618 <- as.factor(x.tr$V618)\n# testing set\nx.te <- xx.te[xx.te$V618 %in% lets, ]\ntruth <- x.te$V618 <- as.factor(x.te$V618)\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.3, maxit = 1500, MaxNWts = 4000, trace = FALSE)\na1$value\n#> [1] 102.1805\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.3, maxit = 1500, MaxNWts = 4000, trace = FALSE)\na2$value\n#> [1] 100.5938\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0\nb2 <- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#> [1] 0\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.01909308\nb2 <- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#> [1] 0.01193317"},{"path":"introduction.html","id":"introduction","chapter":"20 Introduction","heading":"20 Introduction","text":"Unsupervised learning methods differ \nsupervised ones studied far \nresponse variable. objective \nrelated prediction rather\nidentification different possible structures may \npresent data. example, one may \ninterested determining whether observations\n“grouped” way (clustering), data can \nefficiently represented using fewer variables features\n(dimension reduction).Many methods rely probabilistic\nmodel, thus may clear target \nestimated approximated. consequence,\nconclusions can reached type\nanalyses often exploratory nature.","code":""},{"path":"introduction.html","id":"principal-components-analysis","chapter":"20 Introduction","heading":"20.1 Principal Components Analysis","text":"Although principal components can easily computed spectral\ndecomposition covariance matrix data (using function\nsvd R, example), dedicated implementations\nR, among prcomp princomp). main difference \ntwo internal function used compute eigenvalues \neigenvectors: prcomp uses svd princomp uses less preferred\nfunction eigen. princomp prcomp return matrix \nloadings (eigenvectors), scores (projections data \nbasis eigenvectors), auxiliary objects. also include\nplot summary methods.Instead reviewing (can easily done individually), \nnotes reproduce two examples used class (simple\n2-dimensional one used motivate topic, interesting\n256-dimensional one using digits data).Finally, also\nshow principal components can computed using iterative\nalgorithm (alternate regression), may faster factorizing\ncovariance matrix, particularly one interested \nprincipal components dimension data large (\nalso look arguments nu nv function svd R).","code":""},{"path":"introduction.html","id":"simple-2-dimensional-example","chapter":"20 Introduction","heading":"20.2 Simple 2-dimensional example","text":"first read data simple illustration PC’s best lower\ndimensional approximations.Note data 5 explanatory variables. \nuse two order able visualize analysis\neasily:discussed class, standardize data avoid large difference\nscales “hijacking” principal components:now define two auxiliary functions compute Euclidean\nnorms squared Euclidean norms (less general probably\nfaster R’s base::norm):start looking data scatter plot:now compute projections along direction vector\n\\(\\mathbf{v} \\propto (1, 0.05)^\\top\\). Recall linear algebra courses \northogonal projection point \\(\\mathbf{x}\\) linear subspace spanned \\(\\mathbf{v}\\)\n(\\(\\| \\mathbf{v} \\| = 1\\))\ngiven \\(\\pi_{\\mathbf{v}} ( \\mathbf{x} ) = \\langle \\mathbf{x}, \\mathbf{v} \\rangle \\, \\mathbf{v}\\)\ncan also written \n\\(\\pi_{\\mathbf{v}} ( \\mathbf{x} ) = ( \\mathbf{v} \\, \\mathbf{v}^\\top) \\mathbf{x}\\).\nfirst find coordinates orthogonal projects observation\nalong subspace generated \\(\\mathbf{v} = (1, 0.05)^\\top\\) (\nscalars\n\\(\\langle \\mathbf{x}_i, \\mathbf{v} \\rangle = \\mathbf{x}_i^\\top \\mathbf{v}\\) \npoint \\(\\mathbf{x}_i\\):now compute projections\n\\(\\pi_{\\mathbf{v}} ( \\mathbf{x}_i ) = \\langle \\mathbf{x}_i, \\mathbf{v} \\rangle \\, \\mathbf{v}\\):add plot, observations highlighted. subspace\nshown red, orthogonal projections solid red dots line:repeat projecting different direction\n\\(\\mathbf{v} \\propto (-1, 3)^\\top\\):saw class direction \\(\\mathbf{v}\\) results\northogonal projections closest original data (sense \nminimizing mean (sum) residuals Euclidean norm squared)\ngiven “first” eigenvector covariance\nmatrix data. first principal component.\nRefer class slides discussion details\ndefinition properties principal components.","code":"\nx <- read.table(\"data/t8-5.dat\", header = FALSE)\nxx <- x[, c(2, 5)]\ncolnames(xx) <- c(\"Prof degree\", \"Median home value\")\nxx <- scale(xx, center = colMeans(xx), scale = TRUE)\nnorm2 <- function(a) sum(a^2)\nnorm <- function(a) sqrt(norm2(a))\nplot(xx, pch = 19, col = \"black\", cex = 2, xlim = c(-1.5, 4.5), ylim = c(-1.5, 4.5))\nabline(h = 0, lwd = 2, col = \"grey\", lty = 2)\nabline(v = 0, lwd = 2, col = \"grey\", lty = 2)\na <- c(1, 0.05)\na <- a / norm(a)\n# Find the projections (coordinates of the\n# observations on this basis of size 1)\nprs <- (xx %*% a)\npr <- prs %*% a\n# Plot the data\nplot(xx, pch = 19, col = \"black\", cex = 2, xlim = c(-1.5, 4.5), ylim = c(-1.5, 4.5))\nabline(h = 0, lwd = 2, col = \"grey\", lty = 2)\nabline(v = 0, lwd = 2, col = \"grey\", lty = 2)\n# Show the subspace on which we are projecting\nabline(0, a[2] / a[1], lwd = 2, col = \"red\")\n# Add the projections of the data on this subspace\npoints(pr[, 1], pr[, 2], pch = 19, cex = 1.5, col = \"red\")\n# Highlight a few of them\nind <- c(26, 25, 48, 36)\npr2 <- pr[ind, ]\nfor (j in 1:length(ind)) {\n  lines(c(xx[ind[j], 1], pr2[j, 1]), c(xx[ind[j], 2], pr2[j, 2]),\n    col = \"blue\", lwd = 3.5, lty = 2\n  )\n}\na <- c(-1, 3)\na <- a / norm(a)\n# Find the projections (coordinates of the\n# observations on this basis of size 1)\nprs <- (xx %*% a)\n# Find the orthogonal projections of each\n# observation on this subspace of dimension 1\npr <- prs %*% a\n# Plot the data\nplot(xx, pch = 19, col = \"black\", cex = 2, xlim = c(-1.5, 4.5), ylim = c(-1.5, 4.5))\nabline(h = 0, lwd = 2, col = \"grey\", lty = 2)\nabline(v = 0, lwd = 2, col = \"grey\", lty = 2)\n\n# Show the subspace on which we are projecting\nabline(0, a[2] / a[1], lwd = 2, col = \"red\")\n\n# Add the projections of the data on this subspace\npoints(pr[, 1], pr[, 2], pch = 19, cex = 1.5, col = \"red\")\n\n# Highlight a few of them\nind <- c(26, 25, 48, 36)\npr2 <- pr[ind, ]\nfor (j in 1:length(ind)) {\n  lines(c(xx[ind[j], 1], pr2[j, 1]), c(xx[ind[j], 2], pr2[j, 2]),\n    col = \"blue\", lwd = 3.5, lty = 2\n  )\n}"},{"path":"introduction.html","id":"digits-example","chapter":"20 Introduction","heading":"20.3 Digits example","text":"example use principal components explore zip code\ndata. particular, focus images single digit (\nuse 3, reader strongly encouraged re-analysis\ndigits explore whether similar conclusions hold \n). load training data ElemStatLearn package R,\nextract images correspond digit 3. \ninformation use help(zip.train, package='ElemStatLearn').Define auxiliary function compute\nsquared Euclidean distance two vectors\n(recall already defined function norm2 ):display images adapt following function\nplotting matrix, \noriginally available http://www.phaget4.org/R/image_matrix.html:Using function, plot 9 randomly chosen images data set:Next, centre observations order compute eigenvectors\neigenvalues covariance matrix efficiently. fact,\nnote need even compute covariance matrix\ncan use SVD centred data.Using relationship eigenvectors covariance matrix\nSVD \\(n \\times p\\) data matrix, compute coordinates \ncentered data orthogonal projections along first 2nd 3rd\nprincipal directions (eigenvectors covariance matrix). Recall data\nstored rows matrix :discussed class, identify 5 quantiles \ncoordinates use 2-dimensional grid:can visualize grid 5 x 5 = 25 points\nscatter plot 2-dimensional projections\ndata (coordinates principal components\nbasis):now find points data set (images)\nprojections closest 5 x 5 = 25 points grid\n(note distances points principal-subspace,\n256 dimensional space) can computed\nterms coordinates principal-basis (\n2-dimensional points):now add points plot (use color blue ):Using “blue” coordinates, construct corresponding points\n256-dimensional space:identify images data set closest pointsThese actual images closest points array app\n. Now add column means display 25 images according points\nrepresent red grid:Note images change “traverse” 256-dimensional space\nalong 2 principal directions.","code":"\ndata(zip.train, package = \"ElemStatLearn\")\na <- zip.train[zip.train[, 1] == 3, -1]\ndist <- function(a, b) norm2(a - b)\nmyImagePlot <- function(x) {\n  min <- min(x)\n  max <- max(x)\n  ColorRamp <- grey(seq(1, 0, length = 256))\n  ColorLevels <- seq(min, max, length = length(ColorRamp))\n  # Reverse Y axis\n  reverse <- nrow(x):1\n  x <- x[reverse, ]\n  image(1:ncol(x), 1:nrow(x), t(x),\n    col = ColorRamp, xlab = \"\",\n    ylab = \"\", axes = FALSE, zlim = c(min, max)\n  )\n}\nset.seed(31)\nsa <- sample(nrow(a), 9)\npar(mai = c(1, 1, 1, 1) / 5, xaxs = \"i\", yaxs = \"i\")\npar(mfrow = c(3, 3))\nfor (j in 1:9) myImagePlot(t(matrix(unlist(a[sa[j], ]), 16, 16)))\nac <- scale(a, center = TRUE, scale = FALSE)\nsi.svd <- svd(ac)\nv1 <- as.vector(ac %*% si.svd$v[, 1])\nv2 <- as.vector(ac %*% si.svd$v[, 2])\nqv1 <- quantile(v1, c(.05, .25, .5, .75, .95))\nqv2 <- quantile(v2, c(.05, .25, .5, .75, .95))\nqv <- expand.grid(qv1, qv2)\nplot(v1, v2, pch = 19, cex = 1, col = \"grey\")\npoints(qv[, 1], qv[, 2], pch = 19, cex = 1.5, col = \"red\")\nvs <- cbind(v1, v2)\ncvs <- array(0, dim = dim(qv))\nfor (j in 1:dim(qv)[1]) cvs[j, ] <- vs[which.min(apply(vs, 1, dist, b = qv[j, ])), ]\nplot(v1, v2, pch = 19, cex = 1, col = \"grey\")\npoints(qv[, 1], qv[, 2], pch = 19, cex = 1.5, col = \"red\")\nfor (j in 1:dim(qv)[1]) points(cvs[j, 1], cvs[j, 2], pch = 19, col = \"blue\")\napp <- t(si.svd$v[, 1:2] %*% t(cvs))\nrepre <- matrix(0, dim(qv)[1], dim(app)[2])\nfor (j in 1:dim(qv)[1]) repre[j, ] <- ac[which.min(apply(ac, 1, dist, b = app[j, ])), ]\nrepre <- scale(repre, center = -colMeans(a), scale = FALSE)\npar(mai = c(1, 1, 1, 1) / 5, xaxs = \"i\", yaxs = \"i\")\npar(mfrow = c(5, 5))\nfor (j in 1:dim(repre)[1]) {\n  myImagePlot(t(matrix(unlist(repre[j, ]), 16, 16)))\n}"},{"path":"introduction.html","id":"alternating-regression-to-compute-principal-components","chapter":"20 Introduction","heading":"20.4 Alternating regression to compute principal components","text":"details see Appendix .function implementing method compute first\nprincipal component :use digits data compute \nfirst principal component (also time ):compare one given svd,\nalso time. Note \nsign eigenvectors arbitrary, adjust\nvectors order first elements \nsign.Note eigenvectors essentially identical, \nalternating regression method typically faster \nfull SVD decomposition covariance matrix.difference speed striking problems higer dimensions.illustrate potential gain speed larger dimensions,\nconsider following synthetic data set n = 2000 observation\np = 1000, compare timing results\n(even forcing svd compute single component).First generate data setCompute first eigenvector using alternating regression, \ntime :Compute first eigenvector using svd, \ntime :Asking svd compute one component \nseem make algorithm faster (results \nidentical):Finally, check first eigenvector computed \nsvd alternating regression approach\npractially identical:","code":"\nalter.pca.k1 <- function(x, max.it = 500, eps = 1e-10) {\n  n2 <- function(a) sum(a^2)\n  p <- dim(x)[2]\n  x <- scale(x, scale = FALSE)\n  it <- 0\n  old.a <- c(1, rep(0, p - 1))\n  err <- 10 * eps\n  while (((it <- it + 1) < max.it) & (abs(err) > eps)) {\n    b <- as.vector(x %*% old.a) / n2(old.a)\n    a <- as.vector(t(x) %*% b) / n2(b)\n    a <- a / sqrt(n2(a))\n    err <- sqrt(n2(a - old.a))\n    old.a <- a\n  }\n  conv <- (it < max.it)\n  return(list(a = a, b = b, conv = conv))\n}\nsystem.time(tmp <- alter.pca.k1(ac)$a)\n#>    user  system elapsed \n#>   0.071   0.024   0.057\nsystem.time(tmp2 <- svd(ac)$v[, 1])\n#>    user  system elapsed \n#>   0.042   0.005   0.023\ntmp <- tmp * sign(tmp2[1] * tmp[1])\nsummary(abs(tmp - tmp2))\n#>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#> 4.200e-16 1.195e-12 4.012e-12 7.272e-12 1.169e-11 3.524e-11\nn <- 2000\np <- 1000\nx <- matrix(rt(n * p, df = 2), n, p)\nsystem.time(tmp <- alter.pca.k1(x))\n#>    user  system elapsed \n#>   0.273   0.077   0.187\na1 <- tmp$a\nsystem.time(e1 <- svd(cov(x))$u[, 1])\n#>    user  system elapsed \n#>   1.871   0.080   1.633\nsystem.time(e1.1 <- svd(cov(x), nu = 1, nv = 1)$u[, 1])\n#>    user  system elapsed \n#>   1.863   0.075   1.624\nsummary(abs(e1 - e1.1))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0       0       0       0       0       0\na1 <- a1 * sign(e1[1] * a1[1])\nsummary(abs(e1 - a1))\n#>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#> 5.400e-20 9.004e-18 2.032e-17 5.703e-17 4.011e-17 2.232e-14"},{"path":"clustering.html","id":"clustering","chapter":"21 Clustering","heading":"21 Clustering","text":"large class unsupervised learning methods \ncollectively called clustering. objective\ncan described identifying different groups\nobservations closer (“clustered”) \ngroups. data consist n observations \\(X_1\\), \\(X_2\\), …,\n\\(X_n\\), p features. general, number groups \nunknown needs determined data.course discuss model-free\nmodel-based clustering methods. first\ngroup present K-means (\nrelated methods), hierarchical (agglomerative) clustering\nmethods. Model-based clustering based assumption data\nrandom sample, distribution \nvector features X combination \ndifferent distributions (technically: mixture model).\nlatter case, observations belonging \ngroup, vector X assumed\ndistribution specific (generally parametric) family.\nmodel-based methods treat group labels \nmissing (unobserved) responses, rely assumed model \ninfer missing labels.","code":""},{"path":"clustering.html","id":"k-means-k-means-k-medoids","chapter":"21 Clustering","heading":"21.1 K-means, K-means++, K-medoids","text":"Probably intuitive easier explain\nunsupervised clustering algorithm K-means (\nvariants K-means++ K-medoids, .k.. pam,\npartition around medoids). specifics K-means\nalgorithm discussed class. illustrate use\nexamples.","code":""},{"path":"clustering.html","id":"un-votes-example.","chapter":"21 Clustering","heading":"21.1.1 UN votes example.","text":"data contain historical voting patterns\nUnited Nations members. details can \nfound (Voeten, Strezhnev, Bailey 2009).\nUN founded 1946 contains 193 member states.\ndata include “important” votes, classified\nU.S. State Department. votes country\ncoded follows: Yes (1), Abstain (2), (3),\nAbsent (8), Member (9). \n368 important votes, 77 countries\nvoted least 95% . focus \nUN members. goal explore whether\nvoting patterns reflect political\nalignments, also whether countries vote along known\npolitical blocks. data consists 77 observations\n368 variables . information data can found\n.dataset organized vote (resolution), one per row,\ncolumns contain corresponding vote country\n(one country per column).\nfirst read data, limit resolutions \nevery country voted without missing votes:now compute K-means partition using function kmeans\nK = 5, look resulting groups:run kmeans , might get different partition:better consider large number random starts \ntake best found solution (best mean \ncontext? words, algorithm decide one \nsolution return?)may better look groups map:can compare partition one obtain using PAM (K-medoids),\nimplemented function pam package cluster. Recall\ndiscussion class pam need manipulate \nactual observations, pairwise distances (dissimilarities). \ncase use Euclidean distances, may interesting \nexplore distances, particulary light categorical\nnature data. Furthermore, obtain clusters may\neasier interpret use K = 3:Compare resulting groups K-means:better visualization done using map. interesting\nplot 3 groups found pam map, followed \nfound K-means:use L_1 norm instead?mentioned , since data set include true label, \ncomparison different results somewhat subjective, often\nrelies knowledge subject matter experts. example , \nmean asking opinion political scientist whether \ngroupings correspond known international political blocks alignments.","code":"\nX <- read.table(file = \"data/unvotes.csv\", sep = \",\", row.names = 1, header = TRUE)\nX2 <- X[complete.cases(X), ]\nset.seed(123)\nb <- kmeans(t(X2), centers = 5, iter.max = 20, nstart = 1)\ntable(b$cluster)\n#> \n#>  1  2  3  4  5 \n#> 26  2 15  9 25\nb <- kmeans(t(X2), centers = 5, iter.max = 20, nstart = 1)\ntable(b$cluster)\n#> \n#>  1  2  3  4  5 \n#> 27 13 12  5 20\n# Take the best solution out of 1000 random starts\nb <- kmeans(t(X2), centers = 5, iter.max = 20, nstart = 1000)\nsplit(colnames(X2), b$cluster)\n#> $`1`\n#>  [1] \"Bolivia\"             \"Botswana\"            \"Burkina.Faso\"       \n#>  [4] \"Ecuador\"             \"Ethiopia\"            \"Ghana\"              \n#>  [7] \"Guyana\"              \"Jamaica\"             \"Jordan\"             \n#> [10] \"Kenya\"               \"Mali\"                \"Nepal\"              \n#> [13] \"Nigeria\"             \"Philippines\"         \"Singapore\"          \n#> [16] \"Sri.Lanka\"           \"Tanzania\"            \"Thailand\"           \n#> [19] \"Togo\"                \"Trinidad.and.Tobago\" \"Zambia\"             \n#> \n#> $`2`\n#>  [1] \"Argentina\"  \"Bahamas\"    \"Brazil\"     \"Chile\"      \"Colombia\"  \n#>  [6] \"Costa.Rica\" \"Mexico\"     \"Panama\"     \"Paraguay\"   \"Peru\"      \n#> [11] \"Uruguay\"   \n#> \n#> $`3`\n#>  [1] \"Australia\"   \"Austria\"     \"Belgium\"     \"Bulgaria\"    \"Canada\"     \n#>  [6] \"Cyprus\"      \"Denmark\"     \"Finland\"     \"France\"      \"Greece\"     \n#> [11] \"Hungary\"     \"Iceland\"     \"Ireland\"     \"Italy\"       \"Japan\"      \n#> [16] \"Luxembourg\"  \"Malta\"       \"Netherlands\" \"New.Zealand\" \"Norway\"     \n#> [21] \"Poland\"      \"Portugal\"    \"Spain\"       \"Sweden\"      \"UK\"         \n#> [26] \"Ukraine\"    \n#> \n#> $`4`\n#> [1] \"Israel\" \"USA\"   \n#> \n#> $`5`\n#>  [1] \"Algeria\"              \"Bangladesh\"           \"Belarus\"             \n#>  [4] \"Brunei.Darussalam\"    \"China\"                \"Cuba\"                \n#>  [7] \"Egypt\"                \"India\"                \"Indonesia\"           \n#> [10] \"Kuwait\"               \"Libya\"                \"Malaysia\"            \n#> [13] \"Pakistan\"             \"Russian.Federation\"   \"Sudan\"               \n#> [16] \"Syrian.Arab.Republic\" \"Venezuela\"\nlibrary(rworldmap)\nlibrary(countrycode)\nthese <- countrycode(colnames(X2), \"country.name\", \"iso3c\")\nmalDF <- data.frame(country = these, cluster = b$cluster)\n# malDF is a data.frame with the ISO3 country names plus a variable to merge to\n# the map data\n\n# This line will join your malDF data.frame to the country map data\nmalMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\n#> 77 codes from your data successfully matched countries in the map\n#> 0 codes from your data failed to match with a country code in the map\n#> 166 codes from the map weren't represented in your data\n# colors()[grep('blue', colors())] fill the space on the graphical device\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", missingCountryCol = \"white\",\n    addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\",\n        \"blueviolet\", \"yellow\"), oceanCol = \"dodgerblue\")\nlibrary(cluster)\n# Use Euclidean distances\nd <- dist(t(X))\n# what happens with missing values?\nset.seed(123)\na <- pam(d, k = 3)\nb <- kmeans(t(X2), centers = 3, iter.max = 20, nstart = 1000)\ntable(a$clustering)\n#> \n#>  1  2  3 \n#> 26 24 27\ntable(b$cluster)\n#> \n#>  1  2  3 \n#> 16 28 33\nthese <- countrycode(colnames(X), \"country.name\", \"iso3c\")\nmalDF <- data.frame(country = these, cluster = a$clustering)\nmalMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\n#> 77 codes from your data successfully matched countries in the map\n#> 0 codes from your data failed to match with a country code in the map\n#> 166 codes from the map weren't represented in your data\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", missingCountryCol = \"white\",\n    addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\",\n        \"blueviolet\", \"yellow\"), oceanCol = \"dodgerblue\")\nthese <- countrycode(colnames(X2), \"country.name\", \"iso3c\")\nmalDF <- data.frame(country = these, cluster = b$cluster)\nmalMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\n#> 77 codes from your data successfully matched countries in the map\n#> 0 codes from your data failed to match with a country code in the map\n#> 166 codes from the map weren't represented in your data\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", missingCountryCol = \"white\",\n    addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"yellow\", \"tomato\", \"blueviolet\"),\n    oceanCol = \"dodgerblue\")\nd <- dist(t(X), method = \"manhattan\")\nset.seed(123)\na <- pam(d, k = 3)\nthese <- countrycode(colnames(X), \"country.name\", \"iso3c\")\nmalDF <- data.frame(country = these, cluster = a$clustering)\nmalMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\n#> 77 codes from your data successfully matched countries in the map\n#> 0 codes from your data failed to match with a country code in the map\n#> 166 codes from the map weren't represented in your data\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", missingCountryCol = \"white\",\n    addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\",\n        \"blueviolet\", \"yellow\"), oceanCol = \"dodgerblue\")"},{"path":"clustering.html","id":"breweries","chapter":"21 Clustering","heading":"21.1.2 Breweries","text":"example beer drinkers asked rate 9 breweries one 26\nattributes, e.g. whether brewery rich tradition; \nwhether makes good pilsner beer, etc. \nquestions, judges reported score 6-point scale\nranging 1: “true ” 6: “true”. data\nfile breweries.dat:illustration purposes use \\(L_1\\) distance \nPAM clustering method.visualize strength cluster partition use\nsilhouette plot discussed class:Since distances may produce different partitions, interesting\nexercise compare clusters found using\nEuclidean \\(L_\\infty\\) norms, example.","code":"\nx <- read.table(\"data/breweries.dat\", header = FALSE)\nx <- t(x)\nd <- dist(x, method = \"manhattan\")\nset.seed(123)\na <- pam(d, k = 3)\ntable(a$clustering)\n#> \n#> 1 2 3 \n#> 3 3 3\nplot(a)"},{"path":"clustering.html","id":"cancer-example","chapter":"21 Clustering","heading":"21.1.3 Cancer example","text":"data contains gene expression levels \n6830 genes (rows) 64 cell samples (columns).\ninformation can found :\nhttp://genome-www.stanford.edu/nci60/.\ndata included ElemStatLearn package, also\navailable -line:\nhttps://web.stanford.edu/~hastie/ElemStatLearn/.use K-means identify 8 possible clusters among 64 cell samples.\ndiscussed class exercise can (perhaps interestingly) formulated\nterms feature selection. load data use K-means find 8 clusters:Note application know group \nobservation belongs (cancer type). can look cancer types \ngrouped together 8 clusters:Note clusters 3, 4, 6 7 dominated one type cancer.\nSimilarly, almost melanoma renal samples clusters 7 8,\nrespectively, CNS samples cluster 4. Cluster 5 \nharder interpret. Although one ovarian cancer samples \ncluster, also contains 2/3 NSCLC samples. may interest\ncompare results using different numbers clusters.","code":"\ndata(nci, package = \"ElemStatLearn\")\nncit <- t(nci)\nset.seed(31)\na <- kmeans(ncit, centers = 8, iter.max = 5000, nstart = 100)\ntable(a$cluster)\n#> \n#>  1  2  3  4  5  6  7  8 \n#>  3  4  5  8 14  6  9 15\nsapply(split(colnames(nci), a$cluster), table)\n#> $`1`\n#> \n#> K562A-repro K562B-repro    LEUKEMIA \n#>           1           1           1 \n#> \n#> $`2`\n#> \n#>      BREAST MCF7A-repro MCF7D-repro \n#>           2           1           1 \n#> \n#> $`3`\n#> \n#> LEUKEMIA \n#>        5 \n#> \n#> $`4`\n#> \n#> BREAST    CNS  RENAL \n#>      2      5      1 \n#> \n#> $`5`\n#> \n#>    COLON    NSCLC  OVARIAN PROSTATE \n#>        1        6        5        2 \n#> \n#> $`6`\n#> \n#> COLON \n#>     6 \n#> \n#> $`7`\n#> \n#>   BREAST MELANOMA \n#>        2        7 \n#> \n#> $`8`\n#> \n#>   BREAST MELANOMA    NSCLC  OVARIAN    RENAL  UNKNOWN \n#>        1        1        3        1        8        1"},{"path":"model-based-clustering.html","id":"model-based-clustering","chapter":"22 Model based clustering","heading":"22 Model based clustering","text":"Model-based clustering methods depend probabilistic\nmodel specifies distribution observed features\n(whole population). distribution \ntypically modelled mixture several\ndifferent distributions. Given sample n vectors \nfeatures \\(X_1\\), \\(X_2\\), …, \\(X_n\\), clustering problem becomes\nestimation n unobserved labels indicate \nsub-population (cluster, group) \\(X_i\\) belongs. addition,\none generally also estimate parameters specify \ndistribution \\(X\\) assumed group.Given method based full specificification \ndistribution observed vector features, \nsurprising parameters generally estimated using maximum\nlikelihood. difficulty n unobserved (missing)\nvariables (group labels) also need estimated (imputed).\ncommonly used approach uses EM algorithm \nperform maximum likelihood estimation missing observations.","code":""},{"path":"model-based-clustering.html","id":"em-algorithm","chapter":"22 Model based clustering","heading":"22.1 EM algorithm","text":"specifics EM algorithm introduced discussed \nclass. Although algorithm may seem clear first sight,\nfairly subtle, mistakes misunderstandings \n() common. Many applications EM algorithm\nfound -line either wrong, wrongly derived.\ndetailed discussion different\n(also useful) application algorithm, see \nSection Imputation via EM .","code":""},{"path":"model-based-clustering.html","id":"bivariate-gaussian-mixture-model-via-em-by-hand","chapter":"22 Model based clustering","heading":"22.2 Bivariate Gaussian mixture model via EM “by hand”","text":"use 2-dimensional representation \nUN votes data. lower-dimensional representation\nobtained using multidimensional scaling, topic\ncover later course. formulas\nspecific steps algorithm please refer\nclass notes.\nfirst load data reduce 2-dimensional\nproblem, order able plot results.\nnice exercise reader \nre-analysis original data set.data work:now use EM algorithm find (Gaussian-ly\ndistributed) clusters data. First\nfind initial maximum likelihood estimators (.e. initial\nvalues EM algorithm), using random\npartition data:Note loop computed efficiently\nusing fact initial\nstep gamma coefficients either 0’s 1’s.\nHowever, following steps EM algorithm \nneed use weighted averages computations, since\ngeneral weights 0 1.initial configuration (pure noise):now launch iterations. run 120 iterations. Can \nthink appropriate convergence criterion? \nlook parameter\nestimates, gammas (posterior class probabilities),\nlikelihood function?now plot estimated density X, \ncombination 3 gaussian densities.\nevaluating estimated densities\nrelatively fine grid points displaying .\ncolor points according estimated\ngroup labels (largest estimated posterior\nprobability point). first compute thoseWe can also show separate estimated component:","code":"\nX <- read.table(file = \"data/unvotes.csv\", sep = \",\", row.names = 1, header = TRUE)\n# Compute pairwise distances and use MDS\ndd <- dist(t(X))\ntmp <- cmdscale(dd, k = 2)\nplot(tmp, pch = 19, col = \"gray50\", cex = 2, xlab = \"\", ylab = \"\")\nk <- 3\nn <- nrow(tmp)\nset.seed(123456)\nb <- sample((1:n) %% k + 1)\ngammas <- matrix(0, n, k)\nfor (j in 1:k) gammas[b == j, j] <- 1\npis <- colSums(gammas) / n\nmus <- sigmas <- vector(\"list\", k)\nfor (j in 1:k) {\n  mus[[j]] <- colSums(tmp * gammas[, j]) / sum(gammas[, j])\n  sigmas[[j]] <- t(tmp * gammas[, j]) %*% tmp / sum(gammas[, j])\n}\nplot(tmp[, 1], tmp[, 2], pch = 19, cex = 2, \n     col = c(\"black\", \"red\", \"darkblue\")[b], xlab = \"\", ylab = \"\")\nlibrary(mvtnorm)\nniter <- 120\nfor (i in 1:niter) {\n  # E step\n  # compute posterior probabilites f(x_i, \\theta^k)\n  for (j in 1:k) {\n    gammas[, j] <- apply(tmp, 1, dmvnorm, mean = mus[[j]], \n                         sigma = sigmas[[j]])\n  }\n  # multiply by probs of each class\n  # f(x_i, \\theta^k) * pi_k\n  gammas <- gammas %*% diag(pis)\n  # standardize: f(x_i, \\theta^k) * pi_k / [ sum_s { f(x_i, \\theta^s) * pi_s } ]\n  gammas <- gammas / rowSums(gammas)\n  # M step\n  # the maximizers of the expected likelihood have\n  # a closed form in the Gaussian case, they are\n  # just weighted means and covariance matrices\n  for (j in 1:k) {\n    mus[[j]] <- colSums(tmp * gammas[, j]) / sum(gammas[, j])\n    tmp2 <- scale(tmp, scale = FALSE, center = mus[[j]])\n    sigmas[[j]] <- t(tmp2 * gammas[, j]) %*% tmp2 / sum(gammas[, j])\n  }\n  # update pi's\n  pis <- colSums(gammas) / n # n = sum(colSums(gammas))\n}\n# estimated groups\nemlab <- apply(gammas, 1, which.max)\n# build a 100 x 100 grid\nngr <- 100\nx1 <- seq(-15, 15, length = ngr)\nx2 <- seq(-10, 7, length = ngr)\nxx <- expand.grid(x1, x2)\n# evaluate each density component on each grid point\nm <- matrix(NA, ngr * ngr, k)\nfor (j in 1:k) {\n  m[, j] <- apply(xx, 1, dmvnorm, mean = mus[[j]], sigma = sigmas[[j]])\n}\n# apply weights\nmm <- m %*% pis # apply(m, 1, max)\nfilled.contour(x1, x2, matrix(mm, ngr, ngr),\n  col = terrain.colors(35),\n  xlab = \"\", ylab = \"\",\n  panel.last = {\n    points(tmp[, 1], tmp[, 2], pch = 19, cex = 1, col = c(\"black\", \"red\", \"darkblue\")[emlab])\n  }\n)\nm2 <- m %*% diag(pis)\nfor (j in 1:k) {\n  filled.contour(x1, x2, matrix(m2[, j], ngr, ngr),\n    col = terrain.colors(35), xlab = \"\", ylab = \"\",\n    panel.last = {\n      points(tmp[, 1], tmp[, 2], pch = 19, cex = 1, col = c(\"black\", \"red\", \"darkblue\")[emlab])\n    }\n  )\n}"},{"path":"model-based-clustering.html","id":"model-assumptions-may-be-important","chapter":"22 Model based clustering","heading":"22.3 Model assumptions may be important","text":"illustrate problem synthetic data set.\n3 groups 300 observations ,\n3 variables / features.data lookIt surprise model-based clustering works\nwell case:now create data set satisfy model:problem likelihood-based criterion used \nmclust() select number clusters. Note \nfunction increases k = 3, \nalmost stops growing k = 4. \nmaximum nonetheless attained k = 8.interesting note \nK-means found\nright number clusters cluster memberships\nrather easily. sum--squares plot based\nK-means, indicates K = 3 sensible\nchoice:clusters found K-means run kK = 3 :Furthermore, force mclust() use 3 classes\nworks fairly well, even thought model wrong. \nmain problem BIC depends heavily \nassumed likelihood / probabilistic model:","code":"\n# sample size\nn <- 300\n\n# covariance matrices for two of the groups\ns1 <- matrix(c(2, -1, -1, -1, 2, 1, -1, 1, 1), ncol = 3, byrow = TRUE)\ns2 <- matrix(c(4, 0, -1, 0, 4, 3, -1, 3, 5), ncol = 3, byrow = TRUE)\ns1.sqrt <- chol(s1)\ns2.sqrt <- chol(s2)\n\n# easy case, well separated groups\nset.seed(31)\nx1 <- matrix(rnorm(n * 3), n, 3) %*% s1.sqrt\nmu2 <- c(8, 8, 3)\nx2 <- scale(matrix(rnorm(n * 3), n, 3) %*% s2.sqrt, center = -mu2, scale = FALSE)\nmu3 <- c(-5, -5, -10)\nx3 <- scale(matrix(rnorm(n * 3), n, 3), center = -mu3, scale = FALSE)\nx <- rbind(x1, x2, x3)\npairs(x, col = \"gray\", pch = 19)\nlibrary(mclust)\n# select the number of clusters using likelihood-base criterion\nm <- Mclust(x)\n# show the data, color-coded according to the groups found\npairs(x, col = m$class)\nset.seed(31)\nx1 <- matrix(rexp(n * 3, rate = .2), n, 3)\nmu2 <- c(10, 20, 20)\nx2 <- scale(matrix(runif(n * 3, min = -6, max = 6), n, 3), center = -mu2, scale = FALSE)\nmu3 <- c(-5, -5, -5)\nx3 <- scale(matrix(rnorm(n * 3, sd = 3), n, 3), center = -mu3, scale = FALSE)\nx.3 <- rbind(x1, x2, x3)\n\n# run model-based clustering,\n# select the number of clusters using likelihood-base criterion\nm3 <- Mclust(x.3)\n\n# show the data, colors according to groups found\npairs(x.3, col = m3$class)\nplot(m3$BIC[, 6], type = \"b\", xlab = \"K\", ylab = \"BIC\", lwd = 2, pch = 19)\n# run k-means with k = 2, 2, ..., 10\n# to try to identify how many clusters are present\nm3.l <- vector(\"list\", 10)\nss <- rep(0, 10)\nfor (i in 2:10) {\n  ss[i] <- sum((m3.l[[i]] <- kmeans(x.3, centers = i, nstart = 500))$within)\n}\nplot(2:10, ss[-1], xlab = \"K\", ylab = \"W_k\", type = \"b\", lwd = 2, pch = 19)\npairs(x.3, col = m3.l[[3]]$cluster)\nm3.3 <- Mclust(x.3, G = 3)\npairs(x.3, col = m3.3$class)"},{"path":"model-based-clustering.html","id":"behaviour-when-there-are-noise-variables","chapter":"22 Model based clustering","heading":"22.4 Behaviour when there are noise variables","text":"presence noise variables (.e. features \nnon-informative clusters may\npresent data) can quite damaging \nmethods (K-means mclust)\ncreate two data sets “noise” features:\none Gaussian noise, \none uniformly distributed noise.now find clusters using Gaussian model,\nselect number clusters using likelihood-base criterion:use first 3\nfeatures (ones determine \ncluster structure)\n\nshow clusters found mclust\nnoise Gaussian, get:even noise Gaussian\ndistribution, identify ``right’’ clusters:force mclust() identify 3 clusters, things look\nmuch better Gaussian non-Gaussian noise:Note noise also affects K-means seriously.\nrefer robust sparse K-means\nmethod (links module’s main page).Within sum--squares plot\nK-means non-Gaussian noise:Within sum--squares plot\nK-means Gaussian noise:even forcing k-means identify 3 clusters helps \nnoise features:","code":"\nset.seed(31)\nx1 <- matrix(rnorm(n * 3, mean = 3), n, 3) %*% s1.sqrt\nmu2 <- c(9, 9, 3)\nx2 <- scale(matrix(rnorm(n * 3), n, 3) %*% s2.sqrt, center = -mu2, scale = FALSE)\nmu3 <- c(5, 5, -10)\nx3 <- scale(matrix(rnorm(n * 3), n, 3), center = -mu3, scale = FALSE)\nx <- rbind(x1, x2, x3)\n# non-normal \"noise\" features\nx.4 <- cbind(x, matrix(rexp(n * 3 * 3, rate = 1 / 10), n * 3, 3))\n# normal \"noise\" features\nx.5 <- cbind(x, matrix(rnorm(n * 3 * 3, mean = 0, sd = 150), n * 3, 3))\nm4 <- Mclust(x.4)\nm5 <- Mclust(x.5)\npairs(x.4[, 1:3], col = m4$class, pch = 19)\n# pairs(x.5[,1:3], col=m5$class, pch=19)\ntable(m5$class, rep(1:3, each = n))\n#>    \n#>       1   2   3\n#>   1 300   1   0\n#>   2   0 299   0\n#>   3   0   0 300\nm4.3 <- Mclust(x.4, G = 3)\nm5.3 <- Mclust(x.5, G = 3)\n# it works well\npairs(x.4[, 1:3], col = m4.3$class, pch = 19)\npairs(x.5[, 1:3], col = m5.3$class, pch = 19)\ntable(m4.3$class, rep(1:3, each = n))\n#>    \n#>       1   2   3\n#>   1 300   5   0\n#>   2   0 295   0\n#>   3   0   0 300\ntable(m5.3$class, rep(1:3, each = n))\n#>    \n#>       1   2   3\n#>   1 300   1   0\n#>   2   0 299   0\n#>   3   0   0 300\nm4.l <- vector(\"list\", 10)\nss <- rep(0, 10)\nfor (i in 2:10) {\n  ss[i] <- sum((m4.l[[i]] <- kmeans(x.4, centers = i, nstart = 100, iter.max = 20))$within)\n}\nplot(2:10, ss[-1], xlab = \"K\", ylab = \"W_k\", type = \"b\", lwd = 2, pch = 19)\nm5.l <- vector(\"list\", 10)\nss <- rep(0, 10)\nfor (i in 2:10) {\n  ss[i] <- sum((m5.l[[i]] <- kmeans(x.5, centers = i, nstart = 100, iter.max = 20))$within)\n}\nplot(2:10, ss[-1], xlab = \"K\", ylab = \"W_k\", type = \"b\", lwd = 2, pch = 19)\npairs(x.4[, 1:3], col = m4.l[[3]]$cluster, pch = 19)\npairs(x.5[, 1:3], col = m5.l[[3]]$cluster, pch = 19)"},{"path":"model-based-clustering.html","id":"imputation-via-em-a-detailed-example-by-hand","chapter":"22 Model based clustering","heading":"22.5 Imputation via EM (a detailed example “by hand”)","text":"Missing data rather prevalent problem,\ndifferent strategies replace sensible\n“predictions” exit. collectively\ncalled “imputation methods”. notes \nfollow missing data example discussed class \nuse EM algorithm impute partially unobserved data points \nsynthetic bivariate Gaussian data set. Furthemore, scripts\ndesigned case one\nentry may missing observation. \ndifficult extend data coordinates\none entry missing. Please refer \nclass notes formulas details.","code":""},{"path":"model-based-clustering.html","id":"a-synthetic-example","chapter":"22 Model based clustering","heading":"22.5.1 A synthetic example","text":"illustrate method simple setting \ncan visualize ideas 2-dimensional scatter\nplot, work toy example.\nfirst create simple synthetic data set \n50 observations 2 dimensions, normally distributed center\npoint (3,7), fairly strong correlation\ntwo coordinates:data. larger red point indicates\nsample mean (3.13, 7.15):Assume observation (5, NA) \nsecond coordinate missing, \nanother one (NA, 5.5) first coordinate\nmissing. indicate grey lines\nindicate uncertainty missing\nentries:simple method impute missing coordinates \nreplace mean missing variable \nrest data. Hence (5, NA) becomes (5, 7.15) \n(NA, 5.5) becomes (3.13, 5.5). imputed points\nshown blue dots:Note imputed points fact away bulk\ndata, even though \napparent look \ncoordinate separately.\nbetter imputation method uses EM algorithm.assume points data can modelled \noccurences bivariate random vector normal / Gaussian\ndistribution. unknown parameters mean vector\n2x2 variance/covariance matrix. EM algorithm alternate\ncomputing expected value log-likelihood \nfull (non-missing) data set conditional actually observed\npoints (even incompletely observed ones), finding \nparameters (mean vector covariance matrix) maximize\nconditional expected log-likelihood.trivial see conditional expected log-likelihood\nequals constant (depends parameters \nprevious iteration) plus log-likelihood data set \nmissing coordinates observation \nreplaced conditional expectation\n(given observed entries unit). Refer \ndiscussion class details.now implement imputation method R. First add\ntwo incomplete observations \ndata set , append “bottom” \nmatrix x:Next, compute initial values estimates parameters\nmodel. can , example, sample mean \nsample covariance matrix using fully observed\ndata points:start EM iterations helpful \nkeep track wich observations missing coordinate\n(store indices vector mi):n (52) rows x, ones \nmissing coordinates : 51, 52.Now run 100 iterations EM algorithm, although convergence\nachieved much sooner:imputed data now much line \nshape distribution points data set:","code":"\nlibrary(mvtnorm)\n# mean vector\nmu <- c(3, 7)\n# variance/covariance matrix\nsi <- matrix(c(1, 1.2, 1.2, 2), 2, 2)\n# generate data\nset.seed(123)\nx <- rmvnorm(50, mean = mu, sigma = si)\nplot(x, pch = 19, col = \"gray30\", xlim = c(0, 8), ylim = c(0, 13), xlab = \"X1\", ylab = \"X2\", cex = 1.5)\nxbar <- colMeans(x)\npoints(xbar[1], xbar[2], pch = 19, col = \"tomato\", cex = 2)\nplot(x, pch = 19, col = \"gray30\", xlim = c(0, 8), ylim = c(0, 13), xlab = \"X1\", ylab = \"X2\", cex = 1.5)\nabline(v = 5, lwd = 6, col = \"gray80\")\nabline(h = 5.5, lwd = 6, col = \"gray80\")\npoints(x, pch = 19, col = \"gray30\", cex = 1.5)\npoints(xbar[1], xbar[2], pch = 19, col = \"tomato\", cex = 2)\ntext(1, 6, \"(NA, 5.5)\")\ntext(6, 2, \"(5, NA)\")\nplot(x, pch = 19, col = \"gray30\", xlim = c(0, 8), ylim = c(0, 13), xlab = \"X1\", ylab = \"X2\", cex = 1.5)\nabline(h = 5.5, lwd = 6, col = \"gray80\")\nabline(v = 5, lwd = 6, col = \"gray80\")\npoints(x, pch = 19, col = \"gray30\", cex = 1.5)\npoints(xbar[1], xbar[2], pch = 19, col = \"tomato\", cex = 2)\npoints(5, xbar[2], pch = 19, col = \"steelblue\", cex = 2)\npoints(xbar[1], 5.5, pch = 19, col = \"steelblue\", cex = 2)\nset.seed(123)\ndat <- rbind(x, c(5, NA), c(NA, 5.5))\nmu <- colMeans(dat, na.rm = TRUE)\nsi <- var(dat, na.rm = TRUE)\nn <- nrow(dat)\np <- 2\n# find observations with a missing coordinate\nmi <- (1:n)[!complete.cases(dat)]\n# For this data we don't need many iterations\nniter <- 100\n# how many observations with missing entries:\nlen.mi <- length(mi)\n# Start the EM iterations\nfor (i in 1:niter) {\n  # E step\n  # impute the data points with missing entries\n  for (h in 1:len.mi) {\n    # which entries are not missing?\n    nm <- !is.na(dat[mi[h], ])\n    dat[mi[h], !nm] <- mu[!nm] + si[!nm, nm] * solve(si[nm, nm], dat[mi[h], nm] - mu[nm])\n  }\n  # M step, luckily we have a closed form for the maximizers of the\n  # conditional expected likelihood\n  mu <- colMeans(dat)\n  si <- var(dat)\n}\nplot(x, pch = 19, col = \"gray30\", xlim = c(0, 8), ylim = c(0, 13), xlab = \"X1\", ylab = \"X2\", cex = 1.5)\nabline(h = 5.5, lwd = 6, col = \"gray80\")\nabline(v = 5, lwd = 6, col = \"gray80\")\npoints(x, pch = 19, col = \"gray30\", cex = 1.5)\npoints(xbar[1], xbar[2], pch = 19, col = \"tomato\", cex = 2)\nfor (h in 1:length(mi)) points(dat[mi[h], 1], dat[mi[h], 2], pch = 19, col = \"steelblue\", cex = 2)"},{"path":"hierarchical-clustering.html","id":"hierarchical-clustering","chapter":"23 Hierarchical clustering","heading":"23 Hierarchical clustering","text":"Hierarchical clustering refers class algorithms work \ndifferent way ones seen far. k-means model-based\nclustering try find pre-specified number clusters simultaneously.\nHierarchical methods agglomerative–start n clusters\n(one singleton cluster observation data set), form \nhierarchical\nsequence clusters sizes n-1, n-2, …, 3, 2, final “cluster”\ncomposed observations. user needs decide \ncut sequence, words, many clusters identify.\nAlgorithms class also called\nagglomerative, obvious reasons.general algorithm can described follows:Set K = n (number observations data), Start n clusters;K > 1:\nMerge 2 clusters form K-1 clusters;\nSet K = K - 1 (.e. decrease K one).\nMerge 2 clusters form K-1 clusters;Set K = K - 1 (.e. decrease K one).different versions (flavours) method obtained\nvarying criteria decide 2 clusters merge \nrun step 2() , depends measure distance\n(dissimilarity) clusters.different tools decide many clusters may present\ndata following hierarchical clustering algorithm. \ncommonly used graphical representation sequence \nclusters, called dendogram.Please refer class notes details different\nmerging criteria (.e. deciding clusters combine\nstep)\ninterpretation dendogram. \nillustrate use algorithms \nexamples.","code":""},{"path":"hierarchical-clustering.html","id":"breweries-example","chapter":"23 Hierarchical clustering","heading":"23.1 Breweries example","text":"Beer drinkers asked rate 9 breweries 26\nattributes. attributes , e.g., Brewery rich tradition; \nBrewery makes good Pils beer. Relative attribute, \ninformant assign brewery score 6-point scale\nranging 1=true 6=true.\nread data, use function dist\ncompute pairwise L_1 distances \n9 breweries. Note data available\ncolumnwise (\\(p \\times x\\)) first transpose\ncompute distances. also\nchange misleading column names assigned\nread.table, features \nrather observation numbers:One implementation hierarchical clustering methods R \nfunction hclust package cluster.\nfirst use Ward’s information criterion (corrected \nappropriately use squared distances). plot method objects\nclass hclust produces associated dendogram. function rect.hclust\ncomputes height one shuld cut dendogram \nobtain desired number k clusters. show\nresult K = 3 clusters:Now repeat analysis using Euclidean distances single linkage,\nshow K = 3 clusters:Note 3 clusters somewhat different \nones found . However, (V1, V4, V7) cluster\npresent partitions, also triplet\n(V3, V6, V8) stays together well.\ninteresting compare clusters \nfound K-means (see previous notes), particular,\ndendograms resemble\ninformation silhouette plots extent.","code":"\nx <- read.table(\"data/breweries.dat\", header = FALSE)\ncolnames(x) <- paste0(\"Brew-\", 1:ncol(x))\nx <- t(x)\nd <- dist(x, method = \"manhattan\")\n# hierarchical\nlibrary(cluster)\n# show the dendogram\nplot(cl <- hclust(d, method = \"ward.D2\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\n# identify 3 clusters\nrect.hclust(cl, k = 3, border = \"red\")\nbr.dis <- dist(x) # L2\nbr.hc <- hclust(br.dis, method = \"single\")\nplot(br.hc)\nbr.hc.3 <- rect.hclust(br.hc, k = 3)"},{"path":"hierarchical-clustering.html","id":"languages-example","chapter":"23 Hierarchical clustering","heading":"23.2 Languages example","text":"details example discussed class. \npresent results three commonly used merging\ncriteria:\nsingle linkage, complete linkage, average linkage, Ward’s\ncriterion. usual, start reading\ndata, case\nspecific dissimilarities languages\ndiscussed class, arrange \nmatrix can used hclust:Now compute hierarchical clustering sequence using single linkage,\nplot corresponding dendogram identify 4 clusters:Compare results obtained complete linkage:average linkage obtain:finally, using Ward’s criterion results following\ndendogram 4 clusters:","code":"\ndd <- read.table(\"data/languages.dat\", header = FALSE)\nnames(dd) <- c(\"E\", \"N\", \"Da\", \"Du\", \"G\", \"Fr\", \"S\", \"I\", \"P\", \"H\", \"Fi\")\ndd <- (dd + t(dd) / 2)\nd <- as.dist(dd)\nplot(cl <- hclust(d, method = \"single\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\nrect.hclust(cl, k = 4, border = \"red\")\nplot(cl <- hclust(d, method = \"complete\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\nrect.hclust(cl, k = 4, border = \"red\")\nplot(cl <- hclust(d, method = \"average\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\nrect.hclust(cl, k = 4, border = \"red\")\nplot(cl <- hclust(d, method = \"ward.D2\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\nrect.hclust(cl, k = 4, border = \"red\")"},{"path":"hierarchical-clustering.html","id":"cancer-example-1","chapter":"23 Hierarchical clustering","heading":"23.3 Cancer example","text":"revisit Cancer example discussed . use Euclidean\ndistances Ward’s information criterion. show \nclusters identified stop algorithm K = 8, \nbased dendogram seems reasonable choice:completeness, show results obtained linkage criteria,\nincluding Ward’s:Note 3 criteria clear structure\nseems apparent data.","code":"\ndata(nci, package = \"ElemStatLearn\")\nnci.dis <- dist(t(nci), method = \"euclidean\")\nplot(nci.hc.w <- hclust(nci.dis, method = \"ward.D2\"),\n  main = \"\",\n  xlab = \"\", sub = \"\", hang = -1, labels = rownames(nci)\n)\nrect.hclust(nci.hc.w, k = 8, border = \"red\")\nnci.hc.s <- hclust(nci.dis, method = \"single\")\nnci.hc.c <- hclust(nci.dis, method = \"complete\")\nnci.hc.a <- hclust(nci.dis, method = \"average\")\n\n# plot them\nplot(nci.hc.s, labels = colnames(nci), cex = .5)\n\nplot(nci.hc.c, labels = colnames(nci), cex = .5)\n\nplot(nci.hc.a, labels = colnames(nci), cex = .5)"},{"path":"hierarchical-clustering.html","id":"nations-example","chapter":"23 Hierarchical clustering","heading":"23.4 Nations example","text":"smaller Political Science dataset. Twelve countries\nassessed perceived “likeness” Political Science\nstudents. Note (Languages example\n) example raw observations (features),\naccess already determined parwise dissimilarities.\nshow results using hierarchical clustering complete average\nlinkage merging criteria, produce identical clusters. \nencouraged investigate can found \nmerging criteria.","code":"\n# read the pairwise dissimilarities\na2 <- read.table(\"data/nations2.dat\", header = FALSE)\n\n# since only the lower triangular matrix is available\n# we need to copy it on the upper half\na2 <- a2 + t(a2)\n\n# create a vector of country names, to be used later\nnams2 <- c(\n  \"BEL\", \"BRA\", \"CHI\", \"CUB\", \"EGY\", \"FRA\",\n  \"IND\", \"ISR\", \"USA\", \"USS\", \"YUG\", \"ZAI\"\n)\n\n# compute hierarchical clustering using complete linkage\nna.hc <- hclust(as.dist(a2), method = \"complete\")\nplot(na.hc, labels = nams2)\n\n# compute hierarchical clustering using average linkage\nna.hc <- hclust(as.dist(a2), method = \"average\")\nplot(na.hc, labels = nams2)"},{"path":"hierarchical-clustering.html","id":"un-votes","chapter":"23 Hierarchical clustering","heading":"23.5 UN Votes","text":"revisit UN votes example (see Lecture 19). Using Euclidean\ndistances Ward’s criterion obtain following 3 clusters:repeat exercise using \\(L_1\\) distances obtain\ndifferent clusters.easier compare 2 sets clusters\nshow map. first find \ncluster labels corresponding 3 clusters\nusing Euclidean \\(L_1\\) distances:can now use labels color map, \npreviously. Euclidean distances\nobtain:\\(L_1\\) distances get:Recall , discussed class, analyses may \nquestionable, distance measures take \naccount actual nature available features.","code":"\nX <- read.table(file = \"data/unvotes.csv\", sep = \",\", row.names = 1, header = TRUE)\nun.dis <- dist(t(X), method = \"euclidean\")\nun.hc <- hclust(un.dis, method = \"ward.D2\")\nplot(un.hc, cex = .5)\nun.hc.3 <- rect.hclust(un.hc, k = 3)\nlapply(un.hc.3, names)\n#> [[1]]\n#>  [1] \"Australia\"   \"Austria\"     \"Belgium\"     \"Bulgaria\"    \"Canada\"     \n#>  [6] \"Denmark\"     \"Finland\"     \"France\"      \"Greece\"      \"Hungary\"    \n#> [11] \"Iceland\"     \"Ireland\"     \"Israel\"      \"Italy\"       \"Japan\"      \n#> [16] \"Luxembourg\"  \"Netherlands\" \"New.Zealand\" \"Norway\"      \"Poland\"     \n#> [21] \"Portugal\"    \"Spain\"       \"Sweden\"      \"UK\"          \"Ukraine\"    \n#> [26] \"USA\"        \n#> \n#> [[2]]\n#>  [1] \"Argentina\"  \"Bahamas\"    \"Chile\"      \"Colombia\"   \"Costa.Rica\"\n#>  [6] \"Cyprus\"     \"Malta\"      \"Mexico\"     \"Panama\"     \"Paraguay\"  \n#> [11] \"Peru\"       \"Uruguay\"   \n#> \n#> [[3]]\n#>  [1] \"Algeria\"              \"Bangladesh\"           \"Belarus\"             \n#>  [4] \"Bolivia\"              \"Botswana\"             \"Brazil\"              \n#>  [7] \"Brunei.Darussalam\"    \"Burkina.Faso\"         \"China\"               \n#> [10] \"Cuba\"                 \"Ecuador\"              \"Egypt\"               \n#> [13] \"Ethiopia\"             \"Ghana\"                \"Guyana\"              \n#> [16] \"India\"                \"Indonesia\"            \"Jamaica\"             \n#> [19] \"Jordan\"               \"Kenya\"                \"Kuwait\"              \n#> [22] \"Libya\"                \"Malaysia\"             \"Mali\"                \n#> [25] \"Nepal\"                \"Nigeria\"              \"Pakistan\"            \n#> [28] \"Philippines\"          \"Russian.Federation\"   \"Singapore\"           \n#> [31] \"Sri.Lanka\"            \"Sudan\"                \"Syrian.Arab.Republic\"\n#> [34] \"Tanzania\"             \"Thailand\"             \"Togo\"                \n#> [37] \"Trinidad.and.Tobago\"  \"Venezuela\"            \"Zambia\"\nun.dis.l1 <- dist(t(X), method = \"manhattan\")\nun.hc.l1 <- hclust(un.dis.l1, method = \"ward.D2\")\nplot(un.hc.l1, cex = .5)\nun.hc.l1.3 <- rect.hclust(un.hc.l1, k = 3)\nlapply(un.hc.l1.3, names)\n#> [[1]]\n#>  [1] \"Australia\"   \"Austria\"     \"Belgium\"     \"Bulgaria\"    \"Canada\"     \n#>  [6] \"Cyprus\"      \"Denmark\"     \"Finland\"     \"France\"      \"Greece\"     \n#> [11] \"Hungary\"     \"Iceland\"     \"Ireland\"     \"Israel\"      \"Italy\"      \n#> [16] \"Japan\"       \"Luxembourg\"  \"Malta\"       \"Netherlands\" \"New.Zealand\"\n#> [21] \"Norway\"      \"Poland\"      \"Portugal\"    \"Spain\"       \"Sweden\"     \n#> [26] \"UK\"          \"Ukraine\"     \"USA\"        \n#> \n#> [[2]]\n#>  [1] \"Algeria\"              \"Bangladesh\"           \"Belarus\"             \n#>  [4] \"Brunei.Darussalam\"    \"China\"                \"Cuba\"                \n#>  [7] \"Egypt\"                \"India\"                \"Indonesia\"           \n#> [10] \"Jordan\"               \"Kuwait\"               \"Libya\"               \n#> [13] \"Malaysia\"             \"Pakistan\"             \"Russian.Federation\"  \n#> [16] \"Sri.Lanka\"            \"Sudan\"                \"Syrian.Arab.Republic\"\n#> [19] \"Venezuela\"           \n#> \n#> [[3]]\n#>  [1] \"Argentina\"           \"Bahamas\"             \"Bolivia\"            \n#>  [4] \"Botswana\"            \"Brazil\"              \"Burkina.Faso\"       \n#>  [7] \"Chile\"               \"Colombia\"            \"Costa.Rica\"         \n#> [10] \"Ecuador\"             \"Ethiopia\"            \"Ghana\"              \n#> [13] \"Guyana\"              \"Jamaica\"             \"Kenya\"              \n#> [16] \"Mali\"                \"Mexico\"              \"Nepal\"              \n#> [19] \"Nigeria\"             \"Panama\"              \"Paraguay\"           \n#> [22] \"Peru\"                \"Philippines\"         \"Singapore\"          \n#> [25] \"Tanzania\"            \"Thailand\"            \"Togo\"               \n#> [28] \"Trinidad.and.Tobago\" \"Uruguay\"             \"Zambia\"\nlabs <- cutree(un.hc, k = 3)\nlabs.l1 <- cutree(un.hc.l1, k = 3)\nlibrary(rworldmap)\nlibrary(countrycode)\nthese <- countrycode(colnames(X), \"country.name\", \"iso3c\")\nmalDF <- data.frame(country = these, cluster = labs)\nmalMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap,\n  nameColumnToPlot = \"cluster\", catMethod = \"categorical\",\n  missingCountryCol = \"white\", addLegend = FALSE, mapTitle = \"\",\n  colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\", \"blueviolet\", \"yellow\"),\n  oceanCol = \"dodgerblue\"\n)\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap,\n  nameColumnToPlot = \"cluster\", catMethod = \"categorical\",\n  missingCountryCol = \"white\", addLegend = FALSE, mapTitle = \"\",\n  colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\", \"blueviolet\", \"yellow\"),\n  oceanCol = \"dodgerblue\"\n)"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""},{"path":"alt-pca.html","id":"alt-pca","chapter":"A PCA and alternating regression","heading":"A PCA and alternating regression","text":"Let \\(X_1, \\ldots, X_n \\\\mathbb{R}^p\\) observations \nwant compute corresponding PCA. Without loss generality can\nalways assume \n\\[\n\\frac{1}{n} \\sum_{=1}^n X_i \\ = (0,\\ldots,0)^\\top \\, ,\n\\]\nsample covariance matrix \\(S_n\\) \n\\[\nS_n \\ = \\ \\frac{1}{n-1} \\, \\sum_{=1}^n X_i \\, X_i^\\top \\, .\n\\]\nsaw class \\(B \\\\mathbb{R}^{p \\times k}\\) \ncolumns eigenvectors \\(S_n\\) associated \\(k\\) largest\neigenvalues, \n\\[\n\\frac{1}{n} \\, \\sum_{=1}^n \\left\\| X_i - P( L_{B}, X_i\n) \\right\\|^2 \\ \\le \\\n\\frac{1}{n} \\, \\sum_{=1}^n \\left\\| X_i - P( L, X_i\n) \\right\\|^2 \\, ,\n\\]\n\\(k\\)-dimensional linear subspace \\(L \\subset \\mathbb{R}^p\\)\n\\(P( L, X)\\) denotes orthogonal projection \\(X\\) onto \nsubspace \\(L\\), \\(P( L_{B}, X) = {B} {B}^\\top X\\) (whenever \\({B}\\) chosen\n\\({B}^\\top {B} = \\)) \\(L_{B}\\) denotes subspace spanned \ncolumns \\(B\\).show now , instead finding spectral decomposition \n\\(S_n\\), principal components can also computed via sequence \n“alternating least squares” problems. fix ideas consider \ncase \\(k=1\\), method trivially extended arbitrary values \n\\(k\\).\\(k=1\\) need solve following problem\n\\[\\begin{equation}\n\\min_{\\left\\| \\right\\|=1, v \\\\mathbb{R}^n} \\\n\\sum_{=1}^n \\left\\| X_i - \\, v_i \\right\\|^2,\n\\tag{.1}\n\\end{equation}\\]\\end{equation}\n\\(v = (v_1, \\ldots v_n)^\\top\\) (general, \\(k\\) \n\\[\n\\min_{ ^\\top = , v_1, \\ldots, v_n \\\\mathbb{R}^k} \\\n\\sum_{=1}^n \\left\\| X_i - \\, v_i \\right\\|^2 \\, ).\n\\]\nobjective function Equation (.1) can also written\n\n\\[\\begin{equation}\n\\sum_{=1}^n \\sum_{j=1}^p \\left( X_{,j} - a_j \\, v_i \\right)^2 \\, , \\tag{.2}\n\\end{equation}\\]\nhence, given vector \\(\\), minimizing values \n\\(v_1, \\ldots, v_n\\) Equation (.2) can found solving \\(n\\)\nseparate least squares problems:\n\\[\nv_\\ell \\, = \\, \\arg \\,  \\min_{d \\\\mathbb{R}}\n\\sum_{j=1}^p \\left( X_{\\ell,j} - a_j \\, d \\right)^2 \\, , \\qquad\n\\ell = 1, \\ldots, n \\, .\n\\]\nSimilarly, given set \\(v_1, \\ldots, v_n\\) entries \\(\\) can\nfound solving \\(p\\) separate least squares problems: \\[\na_r \\, = \\, \\arg \\,  \\min_{d \\\\mathbb{R}}\n\\sum_{=1}^n \\left( X_{, r} - d \\, v_i \\right)^2 \\, , \\qquad\nr = 1, \\ldots, p \\, .\n\\] can set \\(\\leftarrow / \\| \\|\\) iterate find new\n\\(v\\)’s, new \\(\\), etc.","code":""}]
