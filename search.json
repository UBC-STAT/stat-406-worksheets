[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UBC Stat 406 Worksheets",
    "section": "",
    "text": "Preface\nThis worksheet collection was originally created by Prof. Salibián-Barrera when he taught this course in 2019 and previous iterations. For the 2021 version, Prof. McDonald made some revisions and turned them into a book as a supplement to the course readings."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "UBC Stat 406 Worksheets",
    "section": "Installation",
    "text": "Installation\nTo run these, you need a number packages. To attempt to install them all at once, try:\n\nif (!suppressWarnings(require(remotes, quietly = TRUE))) {\n  install.packages(\"remotes\")\n}\ntmp &lt;- tempdir()\ndp &lt;- file.path(tmp, \"DESCRIPTION\")\ndownload.file(\n  \"https://raw.githubusercontent.com/UBC-STAT/stat-406-worksheets/main/DESCRIPTION\",\n  dp\n)\nremotes::install_deps(tmp)\nunlink(tmp)\nrm(tmp, dp)\n\n\n\n\n\n\n\n\n\n\n\npackage\nversion\nsource\n\n\n\n\nadabag\n5.0\nCRAN (R 4.3.0)\n\n\nbookdown\n0.35\nCRAN (R 4.3.0)\n\n\nbslib\n0.5.1\nCRAN (R 4.3.0)\n\n\nclass\n7.3-22\nCRAN (R 4.3.1)\n\n\ncluster\n2.1.4\nCRAN (R 4.3.1)\n\n\ncountrycode\n1.5.0\nCRAN (R 4.3.0)\n\n\ndesc\n1.4.2\nCRAN (R 4.3.0)\n\n\ndownlit\n0.4.3\nCRAN (R 4.3.0)\n\n\nElemStatLearn\n2015.6.26.2\nGithub (cran/ElemStatLearn@253e54016d004ac67879b594311651be3debbed4)\n\n\nflexclust\n1.4-1\nCRAN (R 4.3.0)\n\n\nformatR\n1.14\nCRAN (R 4.3.0)\n\n\nggcorrplot\n0.1.4\nCRAN (R 4.3.0)\n\n\nggplot2\n3.4.3\nCRAN (R 4.3.0)\n\n\nglmnet\n4.1-7\nCRAN (R 4.3.0)\n\n\nISLR2\n1.3-2\nCRAN (R 4.2.0)\n\n\nKernSmooth\n2.23-22\nCRAN (R 4.3.0)\n\n\nknitr\n1.43\nCRAN (R 4.3.0)\n\n\nlars\n1.3\nCRAN (R 4.3.0)\n\n\nleaps\n3.1\nCRAN (R 4.3.0)\n\n\nMASS\n7.3-60\nCRAN (R 4.3.1)\n\n\nmclust\n6.0.0\nCRAN (R 4.3.0)\n\n\nmvtnorm\n1.2-2\nCRAN (R 4.3.0)\n\n\nnnet\n7.3-19\nCRAN (R 4.3.1)\n\n\nrandomForest\n4.7-1.1\nCRAN (R 4.3.0)\n\n\nrmarkdown\n2.24\nCRAN (R 4.3.0)\n\n\nrobustbase\n0.99-0\nCRAN (R 4.3.0)\n\n\nrpart\n4.1.19\nCRAN (R 4.3.1)\n\n\nrworldmap\n1.3-6\nCRAN (R 4.3.0)\n\n\nscales\n1.2.1\nCRAN (R 4.3.0)\n\n\nSemiPar\n1.0-4.2\nCRAN (R 4.3.0)\n\n\nsessioninfo\n1.2.2\nCRAN (R 4.3.0)\n\n\ntibble\n3.2.1\nCRAN (R 4.3.0)\n\n\ntidyverse\n2.0.0\nCRAN (R 4.3.0)\n\n\ntree\n1.0-43\nCRAN (R 4.3.0)\n\n\n\n\n\nThese notes are licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. See the human-readable version here and the real thing here.\n\n\n\n\n\nAlfaro, Esteban, Matías Gámez, and Noelia García. 2013. “adabag: An R Package for Classification with Boosting and Bagging.” Journal of Statistical Software 54 (2): 1–35. https://doi.org/10.18637/jss.v054.i02.\n\n\nAlfaro, Esteban; Gamez, Matias, Garcia, Noelia; with contributions from L. Guo, A. Albano, M. Sciandra, and A. Plaia. 2023. Adabag: Applies Multiclass AdaBoost.M1, SAMME and Bagging. https://CRAN.R-project.org/package=adabag.\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2023. Rmarkdown: Dynamic Documents for r. https://CRAN.R-project.org/package=rmarkdown.\n\n\nAmbroise, Christophe, and Geoffrey J. McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66. https://doi.org/10.1073/pnas.102102699.\n\n\nArel-Bundock, Vincent. 2023. Countrycode: Convert Country Names and Country Codes. https://vincentarelbundock.github.io/countrycode/.\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018. “Countrycode: An r Package to Convert Country Names and Country Codes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nBoente, Graciela, Alejandra Martínez, and Matías Salibián-Barrera. 2017. “Robust Estimators for Additive Models Using Backfitting.” Journal of Nonparametric Statistics 29 (4): 744–67. https://doi.org/10.1080/10485252.2017.1369077.\n\n\nBreiman, Leo, Adele Cutler, Andy Liaw, and Matthew Wiener. 2022. randomForest: Breiman and Cutler’s Random Forests for Classification and Regression. https://www.stat.berkeley.edu/~breiman/RandomForests/.\n\n\nCsárdi, Gábor, Kirill Müller, and Jim Hester. 2022. Desc: Manipulate DESCRIPTION Files. https://CRAN.R-project.org/package=desc.\n\n\nDolnicar, Sara, Bettina Gruen, and Friedrich Leisch. 2018. Market Segmentation Analysis — Understanding It, Doing It, and Making It Useful. Singapore: Springer. https://doi.org/10.1007/978-981-10-8818-6.\n\n\nEfron, Bradley. 1986. “How Biased Is the Apparent Error Rate of a Prediction Rule?” Journal of the American Statistical Association 81 (394): 461–70.\n\n\nFraley, Chris, Adrian E. Raftery, and Luca Scrucca. 2022. Mclust: Gaussian Mixture Modelling for Model-Based Clustering, Classification, and Density Estimation. https://mclust-org.github.io/mclust/.\n\n\nFriedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2000. “Additive logistic regression: a statistical view of boosting (With discussion and a rejoinder by the authors).” The Annals of Statistics 28 (2): 337–407. https://doi.org/10.1214/aos/1016218223.\n\n\nFriedman, Jerome, Trevor Hastie, Rob Tibshirani, Balasubramanian Narasimhan, Kenneth Tay, Noah Simon, and James Yang. 2023. Glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models. https://glmnet.stanford.edu.\n\n\nFriedman, Jerome, Robert Tibshirani, and Trevor Hastie. 2010. “Regularization Paths for Generalized Linear Models via Coordinate Descent.” Journal of Statistical Software 33 (1): 1–22. https://doi.org/10.18637/jss.v033.i01.\n\n\nGenz, Alan, and Frank Bretz. 2009. Computation of Multivariate Normal and t Probabilities. Lecture Notes in Statistics. Heidelberg: Springer-Verlag.\n\n\nGenz, Alan, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, and Torsten Hothorn. 2023. Mvtnorm: Multivariate Normal and t Distributions. http://mvtnorm.R-forge.R-project.org.\n\n\nHalvorsen, Kjetil. 2019. ElemStatLearn: Data Sets, Functions and Examples from the Book: \"The Elements of Statistical Learning, Data Mining, Inference, and Prediction\" by Trevor Hastie, Robert Tibshirani and Jerome Friedman. http://www-stat.stanford.edu/~tibs/ElemStatLearn/.\n\n\nHastie, Trevor, and Brad Efron. 2022. Lars: Least Angle Regression, Lasso and Forward Stagewise. https://doi.org/10.1214/009053604000000067.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2022. ISLR2: Introduction to Statistical Learning, Second Edition. https://www.statlearning.com.\n\n\nKassambara, Alboukadel. 2022. Ggcorrplot: Visualization of a Correlation Matrix Using Ggplot2. http://www.sthda.com/english/wiki/ggcorrplot-visualization-of-a-correlation-matrix-using-ggplot2.\n\n\nLeisch, Friedrich. 2006. “A Toolbox for k-Centroids Cluster Analysis.” Computational Statistics and Data Analysis 51 (2): 526–44.\n\n\n———. 2010. “Neighborhood Graphs, Stripes and Shadow Plots for Cluster Visualization.” Statistics and Computing 20: 457–69. https://doi.org/10.1007/s11222-009-9137-8.\n\n\n———. 2022. Flexclust: Flexible Cluster Algorithms. https://CRAN.R-project.org/package=flexclust.\n\n\nLeisch, Friedrich, and Bettina Gruen. 2006. “Extending Standard Cluster Algorithms to Allow for Group Constraints.” In Compstat 2006—Proceedings in Computational Statistics, edited by Alfredo Rizzi and Maurizio Vichi, 885–92. Physica Verlag, Heidelberg, Germany.\n\n\nLiaw, Andy, and Matthew Wiener. 2002. “Classification and Regression by randomForest.” R News 2 (3): 18–22. https://CRAN.R-project.org/doc/Rnews/.\n\n\nLumley, Thomas. 2020. Leaps: Regression Subset Selection. https://CRAN.R-project.org/package=leaps.\n\n\nMaechler, Martin, Peter Rousseeuw, Anja Struyf, and Mia Hubert. 2022. Cluster: \"Finding Groups in Data\": Cluster Analysis Extended Rousseeuw Et Al. https://svn.r-project.org/R-packages/trunk/cluster/.\n\n\nMüller, Kirill, and Hadley Wickham. 2023. Tibble: Simple Data Frames. https://CRAN.R-project.org/package=tibble.\n\n\nRipley, Brian. 2023a. Class: Functions for Classification. http://www.stats.ox.ac.uk/pub/MASS4/.\n\n\n———. 2023b. MASS: Support Functions and Datasets for Venables and Ripley’s MASS. http://www.stats.ox.ac.uk/pub/MASS4/.\n\n\n———. 2023c. Nnet: Feed-Forward Neural Networks and Multinomial Log-Linear Models. http://www.stats.ox.ac.uk/pub/MASS4/.\n\n\n———. 2023d. Tree: Classification and Regression Trees. https://CRAN.R-project.org/package=tree.\n\n\nScharl, Theresa, and Friedrich Leisch. 2006. “The Stochastic QT–Clust Algorithm: Evaluation of Stability and Variance on Time–Course Microarray Data.” In Compstat 2006—Proceedings in Computational Statistics, edited by Alfredo Rizzi and Maurizio Vichi, 1015–22. Physica Verlag, Heidelberg, Germany.\n\n\nScrucca, Luca, Michael Fop, T. Brendan Murphy, and Adrian E. Raftery. 2016. “mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models.” The R Journal 8 (1): 289–317. https://doi.org/10.32614/RJ-2016-021.\n\n\nSievert, Carson, Joe Cheng, and Garrick Aden-Buie. 2023. Bslib: Custom Bootstrap ’Sass’ Themes for Shiny and Rmarkdown. https://CRAN.R-project.org/package=bslib.\n\n\nSimon, Noah, Jerome Friedman, Robert Tibshirani, and Trevor Hastie. 2011. “Regularization Paths for Cox’s Proportional Hazards Model via Coordinate Descent.” Journal of Statistical Software 39 (5): 1–13. https://doi.org/10.18637/jss.v039.i05.\n\n\nSouth, Andy. 2011. “Rworldmap: A New r Package for Mapping Global Data.” The R Journal 3 (1): 35–43. http://journal.r-project.org/archive/2011-1/RJournal_2011-1_South.pdf.\n\n\n———. 2016. Rworldmap: Mapping Global Data. https://CRAN.R-project.org/package=rworldmap.\n\n\nTay, J. Kenneth, Balasubramanian Narasimhan, and Trevor Hastie. 2023. “Elastic Net Regularization Paths for All Generalized Linear Models.” Journal of Statistical Software 106 (1): 1–31. https://doi.org/10.18637/jss.v106.i01.\n\n\nTharmaratnam, Kukatharmini, Gerda Claeskens, Christophe Croux, and Matias Salibián-Barrera. 2010. “S-Estimation for Penalized Regression Splines.” Journal of Computational and Graphical Statistics 19 (3): 609–25. https://doi.org/10.1198/jcgs.2010.08149.\n\n\nTherneau, Terry, and Beth Atkinson. 2022. Rpart: Recursive Partitioning and Regression Trees. https://CRAN.R-project.org/package=rpart.\n\n\nTodorov, Valentin, and Peter Filzmoser. 2009. “An Object-Oriented Framework for Robust Multivariate Analysis.” Journal of Statistical Software 32 (3): 1–47. https://www.jstatsoft.org/article/view/v032i03/.\n\n\nTodorov, Valentin, Andreas Ruckstuhl, Matias Salibian-Barrera, Tobias Verbeke, Manuel Koller, and Martin Maechler. 2023. Robustbase: Basic Robust Statistics. https://robustbase.R-forge.R-project.org/.\n\n\nVenables, W. N., and B. D. Ripley. 2002b. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\n———. 2002a. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\n———. 2002c. Modern Applied Statistics with s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nVoeten, Erik, Anton Strezhnev, and Michael Bailey. 2009. “United Nations General Assembly Voting Data.” Harvard Dataverse. https://doi.org/10.7910/DVN/LEJUQZ.\n\n\nWand, Matt. 2018. SemiPar: Semiparametic Regression. http://matt-wand.utsacademics.info/SPmanu.pdf.\n\n\n———. 2023. KernSmooth: Functions for Kernel Smoothing Supporting Wand & Jones (1995). https://CRAN.R-project.org/package=KernSmooth.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2023a. Downlit: Syntax Highlighting and Automatic Linking. https://CRAN.R-project.org/package=downlit.\n\n\n———. 2023b. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Winston Chang, Robert Flight, Kirill Müller, and Jim Hester. 2021. Sessioninfo: R Session Information. https://CRAN.R-project.org/package=sessioninfo.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey Dunnington. 2023. Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, and Dana Seidel. 2022. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales.\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction with r. CRC press.\n\n\nXie, Yihui. 2014. “Knitr: A Comprehensive Tool for Reproducible Research in R.” In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC.\n\n\n———. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\n———. 2016. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/bookdown.\n\n\n———. 2023a. Bookdown: Authoring Books and Technical Documents with r Markdown. https://CRAN.R-project.org/package=bookdown.\n\n\n———. 2023b. formatR: Format r Code Automatically. https://github.com/yihui/formatR.\n\n\n———. 2023c. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.\n\n\nZou, Hui. 2006. “The Adaptive Lasso and Its Oracle Properties.” Journal of the American Statistical Association 101 (476): 1418–29.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67 (2): 301–20."
  },
  {
    "objectID": "02-lm-diagnostics-review.html",
    "href": "02-lm-diagnostics-review.html",
    "title": "1  Predictions using a linear model",
    "section": "",
    "text": "In this document we will explore (rather superficially) some challenges found when trying to estimate the forecasting properties (e.g. the mean squared prediction error) of a (linear) predictor. We will use the air-pollution data set, which I have split into a training set and a test set. The test set will be ignored when training our model (in the case of a linear model, “training” simply means “when estimating the vector of linear regression parameters”).\nIf you are interested in how these sets (training and test) were constructed: I ran the following script (you do not need to do this, as I am providing both data sets to you, but you can re-create them yourself if you want to):\n\nx &lt;- read.csv(\"data/rutgers-lib-30861_CSV-1.csv\")\nset.seed(123)\nii &lt;- sample(rep(1:4, each = 15))\n# this is the training set `pollution-train.dat`\nx.tr &lt;- x[ii != 2, ]\n# this is the test set `pollution-test.dat`\nx.te &lt;- x[ii == 2, ]\n# then I saved them to disk:\n# write.csv(x.tr, file='pollution-train.dat', row.names=FALSE, quote=FALSE)\n# write.csv(x.te, file='pollution-test.dat', row.names=FALSE, quote=FALSE)\n\nWe now read the training data set from the file pollution-train.dat, which is available here, and check that it was read properly:\n\nx.tr &lt;- read.table(\"data/pollution-train.dat\", header = TRUE, sep = \",\")\n# sanity check\nhead(x.tr)\n#&gt;   PREC JANT JULT OVR65 POPN EDUC HOUS DENS NONW WWDRK POOR HC NOX SO. HUMID\n#&gt; 1   36   27   71   8.1 3.34 11.4 81.5 3243  8.8  42.6 11.7 21  15  59    59\n#&gt; 2   35   23   72  11.1 3.14 11.0 78.8 4281  3.5  50.7 14.4  8  10  39    57\n#&gt; 3   44   29   74  10.4 3.21  9.8 81.6 4260  0.8  39.4 12.4  6   6  33    54\n#&gt; 4   47   45   79   6.5 3.41 11.1 77.5 3125 27.1  50.2 20.6 18   8  24    56\n#&gt; 5   43   35   77   7.6 3.44  9.6 84.6 6441 24.4  43.7 14.3 43  38 206    55\n#&gt; 6   53   45   80   7.7 3.45 10.2 66.8 3325 38.5  43.1 25.5 30  32  72    54\n#&gt;       MORT\n#&gt; 1  921.870\n#&gt; 2  997.875\n#&gt; 3  962.354\n#&gt; 4  982.291\n#&gt; 5 1071.289\n#&gt; 6 1030.380\n\nThe response variable is MORT. Our first step is to fit a linear regression model with all available predictors and look at a few diagnostic plots where everything looks fine:\n\nfull &lt;- lm(MORT ~ ., data = x.tr)\nplot(full, which = 1)\n\n\n\n\n\n\n\nplot(full, which = 2)\n\n\n\n\n\n\n\n\nWe also take a look at the estimated coeficients:\n\nsummary(full)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = MORT ~ ., data = x.tr)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -66.06 -14.11  -0.78  17.13  66.09 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  2.210e+03  5.091e+02   4.341 0.000157 ***\n#&gt; PREC         1.786e+00  1.306e+00   1.367 0.181994    \n#&gt; JANT        -1.794e+00  1.205e+00  -1.489 0.147375    \n#&gt; JULT        -4.767e+00  2.913e+00  -1.636 0.112591    \n#&gt; OVR65       -1.150e+01  9.335e+00  -1.232 0.227734    \n#&gt; POPN        -1.586e+02  7.373e+01  -2.151 0.039980 *  \n#&gt; EDUC        -1.278e+01  1.421e+01  -0.899 0.376043    \n#&gt; HOUS        -8.500e-01  2.013e+00  -0.422 0.676023    \n#&gt; DENS         8.253e-03  5.274e-03   1.565 0.128473    \n#&gt; NONW         4.844e+00  1.566e+00   3.093 0.004357 ** \n#&gt; WWDRK       -1.666e-01  1.947e+00  -0.086 0.932408    \n#&gt; POOR        -1.755e+00  3.530e+00  -0.497 0.622938    \n#&gt; HC          -4.090e-01  5.452e-01  -0.750 0.459193    \n#&gt; NOX          5.607e-01  1.109e+00   0.506 0.616884    \n#&gt; SO.          1.762e-01  1.848e-01   0.954 0.348033    \n#&gt; HUMID       -2.647e+00  2.160e+00  -1.225 0.230307    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 32.55 on 29 degrees of freedom\n#&gt; Multiple R-squared:  0.7978, Adjusted R-squared:  0.6931 \n#&gt; F-statistic: 7.626 on 15 and 29 DF,  p-value: 1.805e-06\n\nThe fit appears to be routine, and reasonable (why? what did I check to come to this conclusion?).\n\n1.0.1 A new focus: prediction\nThis course will be primarily concerned with making (good) predictions for cases (data points) that we may have not observed yet (future predictions). This is a bit different from the focus of other Statistics courses you may have taken. You will see later in the course that what you learned in other Statistics courses (e.g. trade-offs between flexibility and stability of different models, uncertainty and standard techniques to reduce it, etc.) will prove to be critical for building good predictions.\nAs a simple example, in the rest of this note we will compare the quality of this model’s predictions with those of a simpler (smaller) linear model with only 5 predictors. For this illustrative example, we will not worry about how these 5 explanatory variables were selected, however, this will play a critical role later in the course).\nWe now fit this reduced model and look at the estimated parameters and diagnostic plots\n\nreduced &lt;- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\nsummary(reduced)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -95.654 -21.848  -1.995  21.555  81.039 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 1117.2254   164.3972   6.796 4.09e-08 ***\n#&gt; POOR          -4.7667     2.5516  -1.868 0.069268 .  \n#&gt; HC            -1.4237     0.3705  -3.843 0.000437 ***\n#&gt; NOX            2.6880     0.7262   3.702 0.000660 ***\n#&gt; HOUS          -2.0595     1.7940  -1.148 0.257957    \n#&gt; NONW           4.3004     1.0140   4.241 0.000132 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 39.44 on 39 degrees of freedom\n#&gt; Multiple R-squared:  0.6007, Adjusted R-squared:  0.5495 \n#&gt; F-statistic: 11.73 on 5 and 39 DF,  p-value: 5.844e-07\nplot(reduced, which = 1)\n\n\n\n\n\n\n\nplot(reduced, which = 2)\n\n\n\n\n\n\n\n\nAlthough the reduced linear model (with 5 predictors) does not seem to provide a fit as good as the one we get with full model, it is still acceptable.\n\nsum(resid(reduced)^2)\n#&gt; [1] 60652.22\nsum(resid(full)^2)\n#&gt; [1] 30718.19\n\nThis observation should be obvious to you, since, as you already now, a model will always yield a better fit to the data in terms of residual sum of squares than any of its submodels (i.e. any model using a subset of the explanatory variables). I expect you to be able to formally prove the last satement.\nOur question of interest here is: “Which model produces better predictions?” In many cases one is interested in predicting future observations, i.e.  predicting the response variable for data that was not available when the model / predictor was fit or trained. As we discussed in class, a reasonably fair comparison can be obtined by comparing the mean squared predictions of these two linear models on the test set, which we read into R as follows:\n\nx.te &lt;- read.table(\"data/pollution-test.dat\", header = TRUE, sep = \",\")\nhead(x.te)\n#&gt;   PREC JANT JULT OVR65 POPN EDUC HOUS DENS NONW WWDRK POOR HC NOX SO. HUMID\n#&gt; 1   52   42   79   7.7 3.39  9.6 69.2 2302 22.2  41.3 24.2 18   8  27    56\n#&gt; 2   33   26   76   8.6 3.20 10.9 83.4 6122 16.3  44.9 10.7 88  63 278    58\n#&gt; 3   40   34   77   9.2 3.21 10.2 77.0 4101 13.0  45.7 15.1 26  26 146    57\n#&gt; 4   35   46   85   7.1 3.22 11.8 79.9 1441 14.8  51.2 16.1  1   1   1    54\n#&gt; 5   15   30   73   8.2 3.15 12.2 84.2 4824  4.7  53.1 12.7 17   8  28    38\n#&gt; 6   43   27   72   9.0 3.25 11.5 87.1 2909  7.2  51.6  9.5  7   3  10    56\n#&gt;       MORT\n#&gt; 1 1017.613\n#&gt; 2 1024.885\n#&gt; 3  970.467\n#&gt; 4  860.101\n#&gt; 5  871.766\n#&gt; 6  887.466\n\nNow compute the predicted values for the test set with both the full and reduced models:\n\nx.te$pr.full &lt;- predict(full, newdata = x.te)\nx.te$pr.reduced &lt;- predict(reduced, newdata = x.te)\n\nand compute the corresponding mean squared prediction errors:\n\nwith(x.te, mean((MORT - pr.full)^2))\n#&gt; [1] 2859.367\nwith(x.te, mean((MORT - pr.reduced)^2))\n#&gt; [1] 1861.884\n\nNote that the reduced model (that did not fit the data as well as the full model) nevertheless produced better predictions (smaller mean squared prediction errors) on the test set.\nAt this point you should put on your critical / skeptical hat and wonder if this did not happen by chance, i.e. if this may be just an artifact of the specific training/test partition we used. The following simple experiment shows that this is not the case. It would be a very good exercise for you to repeat it many times (100, say) to verify my claim.\nFirst, read the whole data and create a new training / test random split.\n\n# repeat with different partitions\nx &lt;- read.csv(\"data/rutgers-lib-30861_CSV-1.csv\")\nset.seed(456)\nii &lt;- sample(rep(1:4, each = 15))\nx.tr &lt;- x[ii != 2, ]\nx.te &lt;- x[ii == 2, ]\n\nIn the above code chunk, I used x.tr to denote the training set and x.te for the test set. Now, fit the full and reduced models on this new training set:\n\nfull &lt;- lm(MORT ~ ., data = x.tr)\nreduced &lt;- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\n\nFinally, estimate the mean squared prediction error of these models with their squared prediction error on the test set:\n\nx.te$pr.full &lt;- predict(full, newdata = x.te)\nx.te$pr.reduced &lt;- predict(reduced, newdata = x.te)\nwith(x.te, mean((MORT - pr.full)^2))\n#&gt; [1] 2194.79\nwith(x.te, mean((MORT - pr.reduced)^2))\n#&gt; [1] 1393.885\n\nNote that the estimated mean squared prediction error of the reduced model is again considerably smaller than that of the full model (even though the latter always fits the training set better than the reduced one)."
  },
  {
    "objectID": "10-test-set-and-cv.html#estimating-the-mspe-with-a-test-set",
    "href": "10-test-set-and-cv.html#estimating-the-mspe-with-a-test-set",
    "title": "2  Predictions using a linear model (continued)",
    "section": "2.1 Estimating the MSPE with a test set",
    "text": "2.1 Estimating the MSPE with a test set\nOne way to estimate the mean squared prediction error of a model or predictor is to use it on a test set (where the responses are known, but that was not used when training the predcitor or estimating the model), and the comparing the predictions with the actual responses.\nFirst, we load the training set and fit both models:\n\nx.tr &lt;- read.table(\"data/pollution-train.dat\", header = TRUE, sep = \",\")\nfull &lt;- lm(MORT ~ ., data = x.tr)\nreduced &lt;- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\n\nAlthough the full model fits the data better than the reduced one (see Lecture 1), its predictions on the test set are better. First, compute the two vectors of test set predictions:\n\nx.te &lt;- read.table(\"data/pollution-test.dat\", header = TRUE, sep = \",\")\npr.full &lt;- predict(full, newdata = x.te)\npr.reduced &lt;- predict(reduced, newdata = x.te)\n\nAnd now, use them to estimate the mean squared prediction error of each model:\n\nwith(x.te, mean((MORT - pr.full)^2))\n#&gt; [1] 2859.367\nwith(x.te, mean((MORT - pr.reduced)^2))\n#&gt; [1] 1861.884\n\nPreviously, we also saw that this is not just an artifact of the specific training / test split of the data. The reduced model generally produces better predictions, regardless of the specific training / test split we use. We can verify this repeating the procedure many times (100, say) and looking at the estimated mean squared prediction errors obtained each time for each model.\n\nx &lt;- read.csv(\"data/rutgers-lib-30861_CSV-1.csv\")\nset.seed(123)\nNsplits &lt;- 100\nmspe.full &lt;- mspe.red &lt;- vector(\"numeric\", Nsplits)\nfor (j in 1:Nsplits) {\n  g &lt;- sample(rep(1:4, each = 15))\n  a.tr &lt;- x[g != 2, ]\n  a.te &lt;- x[g == 2, ]\n  full &lt;- lm(MORT ~ ., data = a.tr)\n  reduced &lt;- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = a.tr)\n  a.te$pr.full &lt;- predict(full, newdata = a.te)\n  a.te$pr.reduced &lt;- predict(reduced, newdata = a.te)\n  mspe.full[j] &lt;- with(a.te, mean((MORT - pr.full)^2))\n  mspe.red[j] &lt;- with(a.te, mean((MORT - pr.reduced)^2))\n}\nboxplot(mspe.full, mspe.red,\n  names = c(\"Full\", \"Reduced\"),\n  col = c(\"gray80\", \"tomato3\"),\n  main = \"Air Pollution - 100 training/test splits\", ylim = c(0, 5000)\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"
  },
  {
    "objectID": "10-test-set-and-cv.html#leave-one-out-cross-validation",
    "href": "10-test-set-and-cv.html#leave-one-out-cross-validation",
    "title": "2  Predictions using a linear model (continued)",
    "section": "2.2 Leave-one-out cross-validation",
    "text": "2.2 Leave-one-out cross-validation\nA different procedure to estimate the prediction power of a model or method is called leave-one-out CV. One advantage of using this method is that the model we fit can use a larger training set. We discussed the procedure in class. Here we apply it to estimate the mean squared prediction error of the full and reduced models. Again, we assume that the reduced model was chosen independently from the training set.\n\nx &lt;- read.csv(\"data/rutgers-lib-30861_CSV-1.csv\")\nn &lt;- nrow(x)\npr.full &lt;- pr.reduced &lt;- rep(0, n)\nfor (i in 1:n) {\n  full &lt;- lm(MORT ~ ., data = x[-i, ])\n  reduced &lt;- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x[-i, ])\n  pr.full[i] &lt;- predict(full, newdata = x[i, ])\n  pr.reduced[i] &lt;- predict(reduced, newdata = x[i, ])\n}\n\nNow we have the leave-one-out predictions for each model and can compute the corresponding estimated mean squared prediction errors:\n\nmean((x$MORT - pr.full)^2)\n#&gt; [1] 2136.785\nmean((x$MORT - pr.reduced)^2)\n#&gt; [1] 1848.375\n\nNote that here again the reduced model seems to yield better prediction errors."
  },
  {
    "objectID": "10-test-set-and-cv.html#k-fold-cross-validation",
    "href": "10-test-set-and-cv.html#k-fold-cross-validation",
    "title": "2  Predictions using a linear model (continued)",
    "section": "2.3 K-fold cross-validation",
    "text": "2.3 K-fold cross-validation\nLeave-one-out cross-validation can be computationally very demanding (or even unfeasible) when the sample size is large and training the predictor is relatively costly. One solution is called K-fold CV. We split the data into K folds, train the predictor on the data without a fold, and use it to predict the responses in the removed fold. We cycle through the folds, and use the average of the squared prediction errors as an estimate of the mean squared prediction error. The following script does 5-fold CV for the full and reduced linear models on the pollution dataset, once again assuming that the reduced model was originally chosen without using the data.\n\nn &lt;- nrow(x)\nk &lt;- 5\npr.full &lt;- pr.reduced &lt;- rep(0, n)\n# Create labels for the \"folds\"\ninds &lt;- (1:n) %% k + 1\n# shuffle the rows of x, this is bad coding!\nset.seed(123)\nxs &lt;- x[sample(n, replace = FALSE), ]\n# loop through the folds\nfor (j in 1:k) {\n  x.tr &lt;- xs[inds != j, ]\n  x.te &lt;- xs[inds == j, ]\n  full &lt;- lm(MORT ~ ., data = x.tr)\n  reduced &lt;- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\n  pr.full[inds == j] &lt;- predict(full, newdata = x.te)\n  pr.reduced[inds == j] &lt;- predict(reduced, newdata = x.te)\n}\n\nWe now compute the estimated mean squared prediction error of each model:\n\nmean((xs$MORT - pr.full)^2)\n#&gt; [1] 2227.21\nmean((xs$MORT - pr.reduced)^2)\n#&gt; [1] 2003.857\n\nThis method is clearly faster than leave-one-out CV, but the results may depend on the specific fold partition, and on the number K of folds used.\n\nOne way to obtain more stable mean squared prediction errors using K-fold CV is to repeat the above procedure many times, and compare the distribution of the mean squared prediction errors for each estimator. First, fit the full and reduced models using the whole data set as training:\n\n\nm.f &lt;- lm(MORT ~ ., data = x)\nm.r &lt;- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x)\n\nWe will use 50 runs of 5-fold CV comparing the full and reduced models. Again, here we assume that the reduced model was not obtained using the training data.\n\nN &lt;- 50\nmspe1 &lt;- mspe2 &lt;- vector(\"double\", N)\nii &lt;- (1:(n &lt;- nrow(x))) %% 5 + 1\nset.seed(327)\nfor (i in 1:N) {\n  ii &lt;- sample(ii)\n  pr.f &lt;- pr.r &lt;- vector(\"double\", n)\n  for (j in 1:5) {\n    pr.f[ii == j] &lt;- predict(\n      update(m.f, data = x[ii != j, ]),\n      newdata = x[ii == j, ]\n    )\n    pr.r[ii == j] &lt;- predict(\n      update(m.r, data = x[ii != j, ]),\n      newdata = x[ii == j, ]\n    )\n  }\n  mspe1[i] &lt;- with(x, mean((MORT - pr.f)^2))\n  mspe2[i] &lt;- with(x, mean((MORT - pr.r)^2))\n}\nboxplot(mspe1, mspe2,\n  names = c(\"Full\", \"Reduced\"),\n  col = c(\"gray80\", \"tomato3\"),\n  main = \"Air Pollution - 50 runs 5-fold CV\", ylim = c(0, 5000)\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n\n\n\n\n\n\n\n\nNote that the estimated mean squared prediction error of the reduced model has a smaller mean / median than that of the full one. This tells us that the conclusion we reached favouring the reduced model (in terms of its prediction mean squared error) does not depend on a particular choice of folds. In other words, this provides more evidence to conclude that the reduced model will produce better predictions than the full one.\n\nA computationally simpler (albeit possibly less precise) way to account for the K-fold variability is to run K-fold CV once and use the sample standard error of the K smaller mean squared prediction errors to construct a rough confidence interval around the overall mean squared prediction error estimate (that is the average of the mean squared prediction errors over the K folds).\nThe dependency of this MSPE on K is more involved. We will discuss it later."
  },
  {
    "objectID": "11-cv-concerns.html#estimating-mspe-with-cv-when-the-model-was-built-using-the-data",
    "href": "11-cv-concerns.html#estimating-mspe-with-cv-when-the-model-was-built-using-the-data",
    "title": "3  Cross-validation concerns",
    "section": "3.1 Estimating MSPE with CV when the model was built using the data",
    "text": "3.1 Estimating MSPE with CV when the model was built using the data\n\nMisuse of cross-validation is, unfortunately, not unusual. For one example see (Ambroise and McLachlan 2002).\nIn particular, for every fold one needs to repeat everything that was done with the training set (selecting variables, looking at pairwise correlations, AIC values, etc.)"
  },
  {
    "objectID": "11-cv-concerns.html#correlated-covariates",
    "href": "11-cv-concerns.html#correlated-covariates",
    "title": "3  Cross-validation concerns",
    "section": "3.2 Correlated covariates",
    "text": "3.2 Correlated covariates\nTechnological advances in recent decades have resulted in data being collected in a fundamentally different manner from the way it was done when most “classical” statistical methods were developed (early to mid 1900’s). Specifically, it is now not at all uncommon to have data sets with an abundance of potentially useful explanatory variables (for example with more variables than observations). Sometimes the investigators are not sure which of the collected variables can be expected to be useful or meaningful.\nA consequence of this “wide net” data collection strategy is that many of the explanatory variables may be correlated with each other. In what follows we will illustrate some of the problems that this can cause both when training and interpreting models, and also with the resulting predictions.\n\n3.2.1 Variables that were important may suddenly “dissappear”\nConsider the air pollution data set we used earlier, and the reduced linear regression model discussed in class:\n\nx &lt;- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\nreduced &lt;- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x)\nround(summary(reduced)$coef, 3)\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept) 1172.831    143.241   8.188    0.000\n#&gt; POOR          -4.065      2.238  -1.817    0.075\n#&gt; HC            -1.480      0.333  -4.447    0.000\n#&gt; NOX            2.846      0.652   4.369    0.000\n#&gt; HOUS          -2.911      1.533  -1.899    0.063\n#&gt; NONW           4.470      0.846   5.283    0.000\n\nNote that all coefficients seem to be significant based on the individual tests of hypothesis (with POOR and HOUS maybe only marginally so). In this sense all 5 explanatory varibles in this model appear to be relevant.\nNow, we fit the full model, that is, we include all available explanatory variables in the data set:\n\nfull &lt;- lm(MORT ~ ., data = x)\nround(summary(full)$coef, 3)\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept) 1763.981    437.327   4.034    0.000\n#&gt; PREC           1.905      0.924   2.063    0.045\n#&gt; JANT          -1.938      1.108  -1.748    0.087\n#&gt; JULT          -3.100      1.902  -1.630    0.110\n#&gt; OVR65         -9.065      8.486  -1.068    0.291\n#&gt; POPN        -106.826     69.780  -1.531    0.133\n#&gt; EDUC         -17.157     11.860  -1.447    0.155\n#&gt; HOUS          -0.651      1.768  -0.368    0.714\n#&gt; DENS           0.004      0.004   0.894    0.376\n#&gt; NONW           4.460      1.327   3.360    0.002\n#&gt; WWDRK         -0.187      1.662  -0.113    0.911\n#&gt; POOR          -0.168      3.227  -0.052    0.959\n#&gt; HC            -0.672      0.491  -1.369    0.178\n#&gt; NOX            1.340      1.006   1.333    0.190\n#&gt; SO.            0.086      0.148   0.585    0.562\n#&gt; HUMID          0.107      1.169   0.091    0.928\n\nIn the full model there are many more parameters that need to be estimated, and while two of them appear to be significantly different from zero (NONW and PREC), all the others appear to be redundant. In particular, note that the p-values for the individual test of hypotheses for 4 out of the 5 regression coefficients for the variables of the reduced model have now become not significant.\n\nround(summary(full)$coef[names(coef(reduced)), ], 3)\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)\n#&gt; (Intercept) 1763.981    437.327   4.034    0.000\n#&gt; POOR          -0.168      3.227  -0.052    0.959\n#&gt; HC            -0.672      0.491  -1.369    0.178\n#&gt; NOX            1.340      1.006   1.333    0.190\n#&gt; HOUS          -0.651      1.768  -0.368    0.714\n#&gt; NONW           4.460      1.327   3.360    0.002\n\nIn other words, the coeffficients of explanatory variables that appeared to be relevant in one model may turn to be “not significant” when other variables are included. This could pose some challenges for interpreting the estimated parameters of the models.\n\n\n3.2.2 Why does this happen?\nRecall that the covariance matrix of the least squares estimator involves the inverse of (X’X), where X’ denotes the transpose of the n x p matrix X (that contains each vector of explanatory variables as a row). It is easy to see that if two columns of X are linearly dependent, then X’X will be rank deficient. When two columns of X are “close” to being linearly dependent (e.g. their linear corrleation is high), then the matrix X’X will be ill-conditioned, and its inverse will have very large entries. This means that the estimated standard errors of the least squares estimator will be unduly large, resulting in non-significant test of hypotheses for each parameter separately, even if the global test for all of them simultaneously is highly significant.\n\n\n3.2.3 Why is this a problem if we are interested in prediction?\nAlthough in many applications one is interested in interpreting the parameters of the model, even if one is only trying to fit / train a model to do predictions, highly variable parameter estimators will typically result in a noticeable loss of prediction accuracy. This can be easily seen from the bias / variance factorization of the mean squared prediction error (MSPE) mentioned in class. Hence, better predictions can be obtained if one uses less-variable parameter (or regression function) estimators.\n\n\n3.2.4 What can we do?\nA commonly used strategy is to remove some explanatory variables from the model, leaving only non-redundant covariates. However, this is easier said than done. You will have seen some strategies in previous Statistics courses (e.g. stepwise variable selection). In coming weeks we will investigate other methods to deal with this problem.\n\n\n\n\nAmbroise, Christophe, and Geoffrey J. McLachlan. 2002. “Selection Bias in Gene Extraction on the Basis of Microarray Gene-Expression Data.” Proceedings of the National Academy of Sciences 99 (10): 6562–66. https://doi.org/10.1073/pnas.102102699."
  },
  {
    "objectID": "12-model-selection-aic.html#general-strategy",
    "href": "12-model-selection-aic.html#general-strategy",
    "title": "4  Comparing models",
    "section": "4.1 General strategy",
    "text": "4.1 General strategy\nSuppose we have a set of competing models from which we want to choose the “best” one. In order to properly define our problem we need the following:\n\na list of models to be considered;\na numerical measure to compare any two models in our list;\na strategy (algorithm, criterion) to navigate the set of models; and\na criterion to stop the search.\n\nFor example, in stepwise methods the models under consideration in each step are those that differ from the current model only by one coefficient (variable). The numerical measure used to compare models could be AIC, or Mallow’s Cp, etc. The strategy is to only consider submodels with one fewer variable than the current one, and we stop if either none of these “p-1” submodels is better than the current one, or we reach an empty model."
  },
  {
    "objectID": "12-model-selection-aic.html#what-is-aic",
    "href": "12-model-selection-aic.html#what-is-aic",
    "title": "4  Comparing models",
    "section": "4.2 What is AIC?",
    "text": "4.2 What is AIC?\nOne intuitively sensible quantity that can be used to compare models is a distance measuring how “close” the distributions implied by these models are from the actual stochastic process generating the data (here “stochastic process” refers to the random mechanism that generated the observations). In order to do this we need:\n\na distance / metric (or at least a “quasimetric”) between models; and\na way of estimating this distance when the “true” model is unknown.\n\nAIC provides an unbiased estimator of the Kullback-Leibler divergence between the estimated model and the “true” one. See the lecture slides for more details."
  },
  {
    "objectID": "12-model-selection-aic.html#using-stepwise-aic-to-select-a-model",
    "href": "12-model-selection-aic.html#using-stepwise-aic-to-select-a-model",
    "title": "4  Comparing models",
    "section": "4.3 Using stepwise + AIC to select a model",
    "text": "4.3 Using stepwise + AIC to select a model\nWe apply stepwise regression based on AIC to select a linear regression model for the airpollution data. In R we can use the function stepAIC in package MASS to perform a stepwise search, for the synthetic data set discussed in class:\n\nset.seed(123)\nx1 &lt;- rnorm(506)\nx2 &lt;- rnorm(506, mean = 2, sd = 1)\nx3 &lt;- rexp(506, rate = 1)\nx4 &lt;- x2 + rnorm(506, sd = .1)\nx5 &lt;- x1 + rnorm(506, sd = .1)\nx6 &lt;- x1 - x2 + rnorm(506, sd = .1)\nx7 &lt;- x1 + x3 + rnorm(506, sd = .1)\ny &lt;- x1 * 3 + x2 / 3 + rnorm(506, sd = 2.2)\n\nx &lt;- data.frame(\n  y = y, x1 = x1, x2 = x2,\n  x3 = x3, x4 = x4, x5 = x5, x6 = x6, x7 = x7\n)\n\nlibrary(MASS)\nnull &lt;- lm(y ~ 1, data = x)\nfull &lt;- lm(y ~ ., data = x)\nst &lt;- stepAIC(null, scope = list(lower = null, upper = full), trace = FALSE)\n\nIf you want to see the progression of the search step-by-step, set the argument trace=TRUE in the call to stepAIC above. The selected model is automatically fit and returned, so that in the code above st is an object of class lm containing the “best” linear regression fit.\n\nst\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x1 + x6, data = x)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x6  \n#&gt;   -0.000706     3.175239    -0.282906\n\nWe will now compare the mean squared prediction errors of the full model and that selected with stepwise. We use 50 runs of 5-fold CV, and obtain the following:\n\nk &lt;- 5\nn &lt;- nrow(x)\nii &lt;- (1:n) %% k + 1\nset.seed(123)\nN &lt;- 50\nmspe.t &lt;- mspe.f &lt;- mspe.st &lt;- rep(0, N)\nfor (i in 1:N) {\n  ii &lt;- sample(ii)\n  pr.t &lt;- pr.f &lt;- pr.st &lt;- rep(0, n)\n  for (j in 1:k) {\n    x0 &lt;- x[ii != j, ]\n    null0 &lt;- lm(y ~ 1, data = x0)\n    full0 &lt;- lm(y ~ ., data = x0) # needed for stepwise\n    true0 &lt;- lm(y ~ x1 + x2, data = x0)\n    step.lm0 &lt;- stepAIC(null0, scope = list(lower = null0, upper = full0), trace = FALSE)\n    pr.st[ii == j] &lt;- predict(step.lm0, newdata = x[ii == j, ])\n    pr.f[ii == j] &lt;- predict(full0, newdata = x[ii == j, ])\n    pr.t[ii == j] &lt;- predict(true0, newdata = x[ii == j, ])\n  }\n  mspe.st[i] &lt;- mean((x$y - pr.st)^2)\n  mspe.f[i] &lt;- mean((x$y - pr.f)^2)\n  mspe.t[i] &lt;- mean((x$y - pr.t)^2)\n}\nboxplot(mspe.st, mspe.f,\n  names = c(\"Stepwise\", \"Full\"),\n  col = c(\"gray60\", \"hotpink\"), ylab = \"MSPE\"\n)\n\n\n\n\n\n\n\n\nNote that since this is a synthetic data set, we can also estimate the MSPE of the true model (could we compute it analytically instead?) and compare it with that of the full and stepwise models. We obtain:\n\nboxplot(mspe.t, mspe.st, mspe.f,\n  names = c(\"True\", \"Stepwise\", \"Full\"),\n  col = c(\"tomato\", \"gray60\", \"hotpink\"), ylab = \"MSPE\"\n)\n\n\n\n\n\n\n\n\n\n4.3.1 Stepwise applied to the “air pollution” data\nWe now use stepwise on the air pollution data to select a model, and estimate its MSPE using 5-fold CV. We compare the predictions of this model with that of the full model.\n\nlibrary(MASS)\nairp &lt;- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\nnull &lt;- lm(MORT ~ 1, data = airp)\nfull &lt;- lm(MORT ~ ., data = airp)\n(tmp.st &lt;- stepAIC(full, scope = list(lower = null), trace = FALSE))\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = MORT ~ PREC + JANT + JULT + OVR65 + POPN + EDUC + \n#&gt;     NONW + HC + NOX, data = airp)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)         PREC         JANT         JULT        OVR65         POPN  \n#&gt;   1934.0539       1.8565      -2.2620      -3.3200     -10.9205    -137.3831  \n#&gt;        EDUC         NONW           HC          NOX  \n#&gt;    -23.4211       4.6623      -0.9221       1.8710\n\n\nk &lt;- 5\nn &lt;- nrow(airp)\nii &lt;- (1:n) %% k + 1\nset.seed(123)\nN &lt;- 50\nmspe.f &lt;- mspe.st &lt;- rep(0, N)\nfor (i in 1:N) {\n  ii &lt;- sample(ii)\n  pr.f &lt;- pr.st &lt;- rep(0, n)\n  for (j in 1:k) {\n    x0 &lt;- airp[ii != j, ]\n    null0 &lt;- lm(MORT ~ 1, data = x0)\n    full0 &lt;- lm(MORT ~ ., data = x0) # needed for stepwise\n    step.lm0 &lt;- stepAIC(null0, scope = list(lower = null0, upper = full0), trace = FALSE)\n    pr.st[ii == j] &lt;- predict(step.lm0, newdata = airp[ii == j, ])\n    pr.f[ii == j] &lt;- predict(full0, newdata = airp[ii == j, ])\n  }\n  mspe.st[i] &lt;- mean((airp$MORT - pr.st)^2)\n  mspe.f[i] &lt;- mean((airp$MORT - pr.f)^2)\n}\nboxplot(mspe.st, mspe.f,\n  names = c(\"Stepwise\", \"Full\"),\n  col = c(\"gray60\", \"hotpink\"), ylab = \"MSPE\"\n)\n\n\n\n\n\n\n\n\nWe can also use the package leaps to run a more thorough search among all possible subsets. We do this with the air pollution data:\n\nlibrary(leaps)\na &lt;- leaps(x = as.matrix(airp[, -16]), y = airp$MORT, int = TRUE, method = \"Cp\", nbest = 10)\n\nIn the call above we asked leaps to compute the 10 best models of each size, according to Mallow’s Cp criterion. We can look at the returned object\n\nstr(a)\n#&gt; List of 4\n#&gt;  $ which: logi [1:141, 1:15] FALSE FALSE TRUE FALSE FALSE FALSE ...\n#&gt;   ..- attr(*, \"dimnames\")=List of 2\n#&gt;   .. ..$ : chr [1:141] \"1\" \"1\" \"1\" \"1\" ...\n#&gt;   .. ..$ : chr [1:15] \"1\" \"2\" \"3\" \"4\" ...\n#&gt;  $ label: chr [1:16] \"(Intercept)\" \"1\" \"2\" \"3\" ...\n#&gt;  $ size : num [1:141] 2 2 2 2 2 2 2 2 2 2 ...\n#&gt;  $ Cp   : num [1:141] 53.6 82.3 82.6 97 97.2 ...\n\nWe now find the best model (based on Mallow’s Cp), and fit the corresponding model:\n\nj0 &lt;- which.min(a$Cp)\n(m1 &lt;- lm(MORT ~ ., data = airp[, c(a$which[j0, ], TRUE)]))\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = MORT ~ ., data = airp[, c(a$which[j0, ], TRUE)])\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)         PREC         JANT         JULT         EDUC         NONW  \n#&gt;   1180.3565       1.7970      -1.4836      -2.3553     -13.6190       4.5853  \n#&gt;         SO.  \n#&gt;      0.2596\n\nWe compare which variables are used in this model with those used in the model found with stepwise:\n\nformula(m1)[[3]]\n#&gt; PREC + JANT + JULT + EDUC + NONW + SO.\nformula(tmp.st)[[3]]\n#&gt; PREC + JANT + JULT + OVR65 + POPN + EDUC + NONW + HC + NOX\n\nIt is reasonable to ask whether the model found by leaps is much better than the one returned by stepAIC:\n\nextractAIC(m1)\n#&gt; [1]   7.0000 429.0017\nextractAIC(tmp.st)\n#&gt; [1]  10.000 429.634\n\nFinally, what is the MSPE of the model found by leaps?\n\n# proper way\nk &lt;- 5\nn &lt;- nrow(airp)\nii &lt;- (1:n) %% k + 1\nset.seed(123)\nN &lt;- 50\nmspe.l &lt;- rep(0, N)\nfor (i in 1:N) {\n  ii &lt;- sample(ii)\n  pr.l &lt;- rep(0, n)\n  for (j in 1:k) {\n    x0 &lt;- airp[ii != j, ]\n    tmp.leaps &lt;- leaps(x = as.matrix(x0[, -16]), y = as.vector(x0[, 16]), int = TRUE, method = \"Cp\", nbest = 10)\n    j0 &lt;- which.min(tmp.leaps$Cp)\n    step.leaps &lt;- lm(MORT ~ ., data = x0[, c(tmp.leaps$which[j0, ], TRUE)])\n    pr.l[ii == j] &lt;- predict(step.leaps, newdata = airp[ii == j, ])\n  }\n  mspe.l[i] &lt;- mean((airp$MORT - pr.l)^2)\n}\nboxplot(mspe.st, mspe.f, mspe.l,\n  names = c(\"Stepwise\", \"Full\", \"Leaps\"),\n  col = c(\"gray60\", \"hotpink\", \"steelblue\"), ylab = \"MSPE\"\n)\n\n\n\n\n\n\n\n\nNote that a “suboptimal” model (stepwise) seems to be better than the one found with a “proper” (exhaustive) search, as that returned by leaps. This is intriguing, but we will see the same phenomenon occur in different contexts later in the course."
  },
  {
    "objectID": "12-model-selection-aic.html#an-example-where-one-may-not-need-to-select-variables",
    "href": "12-model-selection-aic.html#an-example-where-one-may-not-need-to-select-variables",
    "title": "4  Comparing models",
    "section": "4.4 An example where one may not need to select variables",
    "text": "4.4 An example where one may not need to select variables\nIn some cases one may not need to select a subset of explanatory variables, and in fact, doing so may affect negatively the accuracy of the resulting predictions. In what follows we discuss such an example. Consider the credit card data set that contains information on credit card users. The interest is in predicting the balance carried by a client. We first load the data, and to simplify the presentation here we consider only the numerical explanatory variables:\n\nx &lt;- read.table(\"data/Credit.csv\", sep = \",\", header = TRUE, row.names = 1)\nx &lt;- x[, c(1:6, 11)]\n\nThere are 6 available covariates, and a stepwise search selects a model with 5 of them (discarding Education):\n\nlibrary(MASS)\nnull &lt;- lm(Balance ~ 1, data = x)\nfull &lt;- lm(Balance ~ ., data = x)\n(tmp.st &lt;- stepAIC(null, scope = list(lower = null, upper = full), trace = 0))\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Balance ~ Rating + Income + Limit + Age + Cards, \n#&gt;     data = x)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)       Rating       Income        Limit          Age        Cards  \n#&gt;   -449.3610       2.0224      -7.5621       0.1286      -0.8883      11.5527\n\nIt is an easy exercise to check that the MSPE of this smaller model is in fact worse than the one for the full one:\n\nn &lt;- nrow(x)\nk &lt;- 5\nii &lt;- (1:n) %% k + 1\nset.seed(123)\nN &lt;- 100\nmspe.st &lt;- mspe.f &lt;- rep(0, N)\nfor (i in 1:N) {\n  ii &lt;- sample(ii)\n  pr.f &lt;- pr.st &lt;- rep(0, n)\n  for (j in 1:k) {\n    null &lt;- lm(Balance ~ 1, data = x[ii != j, ])\n    full &lt;- lm(Balance ~ ., data = x[ii != j, ])\n    tmp.st &lt;- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n    pr.st[ii == j] &lt;- predict(tmp.st, newdata = x[ii == j, ])\n    pr.f[ii == j] &lt;- predict(full, newdata = x[ii == j, ])\n  }\n  mspe.st[i] &lt;- mean((x$Balance - pr.st)^2)\n  mspe.f[i] &lt;- mean((x$Balance - pr.f)^2)\n}\nboxplot(mspe.st, mspe.f,\n  names = c(\"Stepwise\", \"Full\"),\n  col = c(\"tomato\", \"springgreen\"), cex.axis = 1.5,\n  cex.lab = 1.5, cex.main = 2\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"
  },
  {
    "objectID": "20-ridge-regression.html#selecting-the-level-of-regularization",
    "href": "20-ridge-regression.html#selecting-the-level-of-regularization",
    "title": "5  Ridge regression",
    "section": "5.1 Selecting the level of regularization",
    "text": "5.1 Selecting the level of regularization\nDifferent values of the penalization parameter will typically yield estimators with varying predictive accuracies. To select a good level of regularization we estimate the MSPE of the estimator resulting from each value of the penalization parameter. One way to do this is to run K-fold cross validation for each value of the penalty. The glmnet package provides a built-in function to do this, and a plot method to display the results:\n\n# run 5-fold CV\nset.seed(123)\ntmp &lt;- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\nplot(tmp, lwd = 6, cex.axis = 1.5, cex.lab = 1.2)\n\n\n\n\n\n\n\n\nIn the above plot the red dots are the estimated MSPE’s for each value of the penalty, and the vertical lines mark plus/minus one (estimated) standard deviations (for each of those estimated MSPE’s). The plot method will also mark the optimal value of the regularization parameter, and also the largest one for which the estimated MSPE is within 1-SD of the optimal. The latter is meant to provide a more regularized estimator with estimated MSPE within the error-margin of our estimated minimum.\nNote, however, that the above “analysis” is random (because of the intrinsic randomness of K-fold CV). If we run it again, we will most likely get different results. In many cases, however, the results will be qualitatively similar. If we run 5-fold CV again for this data get the following plot:\n\nset.seed(23)\ntmp &lt;- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\nplot(tmp, lwd = 6, cex.axis = 1.5, cex.lab = 1.2)\n\n\n\n\n\n\n\n\nNote that both plots are similar, but not equal. It would be a good idea to repeat this a few times and explore how much variability is involved. If one were interested in selecting one value of the penalization parameter that was more stable than that obtained from a single 5-fold CV run, one could run it several times and take the average of the estimated optimal values. For example:\n\nset.seed(123)\nop.la &lt;- 0\nfor (j in 1:20) {\n  tmp &lt;- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\n  op.la &lt;- op.la + tmp$lambda.min # tmp$lambda.1se\n}\n(op.la &lt;- op.la / 20)\n#&gt; [1] 11.44547\nlog(op.la)\n#&gt; [1] 2.437594\n\nThis value is reasonably close to the ones we saw in the plots above."
  },
  {
    "objectID": "20-ridge-regression.html#comparing-predictions",
    "href": "20-ridge-regression.html#comparing-predictions",
    "title": "5  Ridge regression",
    "section": "5.2 Comparing predictions",
    "text": "5.2 Comparing predictions\nWe now run a cross-validation experiment to compare the MSPE of 3 models: the full model, the one selected by stepwise and the ridge regression one.\n\nn &lt;- nrow(xm)\nk &lt;- 5\nii &lt;- (1:n) %% k + 1\nset.seed(123)\nN &lt;- 100\nmspe.st &lt;- mspe.ri &lt;- mspe.f &lt;- rep(0, N)\nfor (i in 1:N) {\n  ii &lt;- sample(ii)\n  pr.f &lt;- pr.ri &lt;- pr.st &lt;- rep(0, n)\n  for (j in 1:k) {\n    tmp.ri &lt;- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 0, family = \"gaussian\"\n    )\n    null &lt;- lm(MORT ~ 1, data = airp[ii != j, ])\n    full &lt;- lm(MORT ~ ., data = airp[ii != j, ])\n    tmp.st &lt;- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n    pr.ri[ii == j] &lt;- predict(tmp.ri, s = \"lambda.min\", newx = xm[ii == j, ])\n    pr.st[ii == j] &lt;- predict(tmp.st, newdata = airp[ii == j, ])\n    pr.f[ii == j] &lt;- predict(full, newdata = airp[ii == j, ])\n  }\n  mspe.ri[i] &lt;- mean((airp$MORT - pr.ri)^2)\n  mspe.st[i] &lt;- mean((airp$MORT - pr.st)^2)\n  mspe.f[i] &lt;- mean((airp$MORT - pr.f)^2)\n}\nboxplot(mspe.ri, mspe.st, mspe.f,\n  names = c(\"Ridge\", \"Stepwise\", \"Full\"),\n  col = c(\"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1.5, cex.lab = 1.5,\n  cex.main = 2, ylim = c(1300, 3000)\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"
  },
  {
    "objectID": "20-ridge-regression.html#a-more-stable-ridge-regression",
    "href": "20-ridge-regression.html#a-more-stable-ridge-regression",
    "title": "5  Ridge regression",
    "section": "5.3 A more stable Ridge Regression?",
    "text": "5.3 A more stable Ridge Regression?\nHere we try to obtain a ridge regression estimator with more stable predictions by using the average optimal penalty value using 20 runs. The improvement does not appear to be substantial.\n\nn &lt;- nrow(xm)\nk &lt;- 5\nii &lt;- (1:n) %% k + 1\nset.seed(123)\nN &lt;- 100\nmspe.ri2 &lt;- rep(0, N)\nfor (i in 1:N) {\n  ii &lt;- sample(ii)\n  pr.ri2 &lt;- rep(0, n)\n  for (j in 1:k) {\n    op.la &lt;- 0\n    for (h in 1:20) {\n      tmp &lt;- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\n      op.la &lt;- op.la + tmp$lambda.min # tmp$lambda.1se\n    }\n    op.la &lt;- op.la / 20\n    tmp.ri &lt;- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas, nfolds = 5,\n      alpha = 0, family = \"gaussian\"\n    )\n    pr.ri2[ii == j] &lt;- predict(tmp.ri, s = op.la, newx = xm[ii == j, ])\n  }\n  mspe.ri2[i] &lt;- mean((airp$MORT - pr.ri2)^2)\n}\nboxplot(mspe.ri2, mspe.ri, mspe.st, mspe.f,\n  names = c(\"Stable R\", \"Ridge\", \"Stepwise\", \"Full\"),\n  col = c(\"steelblue\", \"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1.5, cex.lab = 1.5,\n  cex.main = 2, ylim = c(1300, 3000)\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"
  },
  {
    "objectID": "20-ridge-regression.html#an-example-where-one-may-not-need-to-select-variables",
    "href": "20-ridge-regression.html#an-example-where-one-may-not-need-to-select-variables",
    "title": "5  Ridge regression",
    "section": "5.4 An example where one may not need to select variables",
    "text": "5.4 An example where one may not need to select variables\nIn some cases one may not need to select a subset of explanatory variables, and in fact, doing so may affect negatively the accuracy of the resulting predictions. In what follows we discuss such an example. Consider the credit card data set that contains information on credit card users. The interest is in predicting the balance carried by a client. We first load the data, and to simplify the presentation here we consider only the numerical explanatory variables:\n\nx &lt;- read.table(\"data/Credit.csv\", sep = \",\", header = TRUE, row.names = 1)\nx &lt;- x[, c(1:6, 11)]\n\nThere are 6 available covariates, and a stepwise search selects a model with 5 of them (discarding Education):\n\nlibrary(MASS)\nnull &lt;- lm(Balance ~ 1, data = x)\nfull &lt;- lm(Balance ~ ., data = x)\n(tmp.st &lt;- stepAIC(null, scope = list(lower = null, upper = full), trace = 0))\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Balance ~ Rating + Income + Limit + Age + Cards, \n#&gt;     data = x)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)       Rating       Income        Limit          Age        Cards  \n#&gt;   -449.3610       2.0224      -7.5621       0.1286      -0.8883      11.5527\n\nIt is an easy exercise to check that the MSPE of this smaller model is in fact worse than the one for the full one:\n\nn &lt;- nrow(x)\nk &lt;- 5\nii &lt;- (1:n) %% k + 1\nset.seed(123)\nN &lt;- 100\nmspe.st &lt;- mspe.f &lt;- rep(0, N)\nfor (i in 1:N) {\n  ii &lt;- sample(ii)\n  pr.f &lt;- pr.st &lt;- rep(0, n)\n  for (j in 1:k) {\n    null &lt;- lm(Balance ~ 1, data = x[ii != j, ])\n    full &lt;- lm(Balance ~ ., data = x[ii != j, ])\n    tmp.st &lt;- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n    pr.st[ii == j] &lt;- predict(tmp.st, newdata = x[ii == j, ])\n    pr.f[ii == j] &lt;- predict(full, newdata = x[ii == j, ])\n  }\n  mspe.st[i] &lt;- mean((x$Balance - pr.st)^2)\n  mspe.f[i] &lt;- mean((x$Balance - pr.f)^2)\n}\nboxplot(mspe.st, mspe.f,\n  names = c(\"Stepwise\", \"Full\"),\n  col = c(\"tomato\", \"springgreen\"), cex.axis = 1.5,\n  cex.lab = 1.5, cex.main = 2\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n\n\n\n\n\n\n\n\nUsing ridge regression instead of stepwise to prevent the negative effect of possible correlations among the covariates yields a slight improvement (over the full model), but it is not clear the gain is worth the effort.\n\ny &lt;- as.vector(x$Balance)\nxm &lt;- as.matrix(x[, -7])\nlambdas &lt;- exp(seq(-3, 10, length = 50))\nn &lt;- nrow(xm)\nk &lt;- 5\nii &lt;- (1:n) %% k + 1\nset.seed(123)\nN &lt;- 100\nmspe.st &lt;- mspe.ri &lt;- mspe.f &lt;- rep(0, N)\nfor (i in 1:N) {\n  ii &lt;- sample(ii)\n  pr.f &lt;- pr.ri &lt;- pr.st &lt;- rep(0, n)\n  for (j in 1:k) {\n    tmp.ri &lt;- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 0, family = \"gaussian\"\n    )\n    null &lt;- lm(Balance ~ 1, data = x[ii != j, ])\n    full &lt;- lm(Balance ~ ., data = x[ii != j, ])\n    tmp.st &lt;- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n    pr.ri[ii == j] &lt;- predict(tmp.ri, s = \"lambda.min\", newx = xm[ii == j, ])\n    pr.st[ii == j] &lt;- predict(tmp.st, newdata = x[ii == j, ])\n    pr.f[ii == j] &lt;- predict(full, newdata = x[ii == j, ])\n  }\n  mspe.ri[i] &lt;- mean((x$Balance - pr.ri)^2)\n  mspe.st[i] &lt;- mean((x$Balance - pr.st)^2)\n  mspe.f[i] &lt;- mean((x$Balance - pr.f)^2)\n}\nboxplot(mspe.ri, mspe.st, mspe.f,\n  names = c(\n    \"Ridge\", \"Stepwise\",\n    \"Full\"\n  ), col = c(\"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1.5,\n  cex.lab = 1.5, cex.main = 2\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)"
  },
  {
    "objectID": "20-ridge-regression.html#an-important-limitation-of-ridge-regression",
    "href": "20-ridge-regression.html#an-important-limitation-of-ridge-regression",
    "title": "5  Ridge regression",
    "section": "5.5 An important limitation of Ridge Regression",
    "text": "5.5 An important limitation of Ridge Regression\nRidge Regression typically yields estimators with more accurate (less variable) predictions, specially when there is noticeable correlation among covariates. However, it is important to note that Ridge Regression does not select variables, and in that sense it does not “replace” methods like stepwise when the interest is in using a smaller number of explanatory variables. Furthermore, the interpretation of the Ridge Regression coefficient estimates is generally difficult. LASSO regression estimates were proposed to address these two issues (more stable predictions when correlated covariates are present and variable selection) simultaneously."
  },
  {
    "objectID": "20-ridge-regression.html#effective-degrees-of-freedom",
    "href": "20-ridge-regression.html#effective-degrees-of-freedom",
    "title": "5  Ridge regression",
    "section": "5.6 Effective degrees of freedom",
    "text": "5.6 Effective degrees of freedom\nIntuitively, if we interpret “degrees of freedom” as the number of “free” parameters that are available to us for tuning when we fit / train a model or predictor, then we would expect a Ridge Regression estimator to have less “degrees of freedom” than a regular least squares regression estimator, given that it is the solution of a constrained optimization problem. This is, of course, an informal argument, particularly since there is no proper definition of “degrees of freedom”.\nThe more general definition discussed in class, called “effective degrees of freedom” (EDF), reduces to the trace of the “hat” matrix for any linear predictor (including, but not limited to, linear regression models), and is due to Efron (Efron 1986). You may also want to look at some of the more recent papers that cite the one above.\nIt is easy (but worth your time doing it) to see that for a Ridge Regression estimator computed with a penalty / regularization parameter equal to b, the corresponding EDF are the sum of the ratio of each eigenvalue of X’X with respect to itself plus b (see the formula on the lecture slides). We compute the EDF of the Ridge Regression fit to the air pollution data when the penalty parameter is considered to be fixed at the average optimal value over 20 runs of 5-fold CV:\n\nairp &lt;- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\ny &lt;- as.vector(airp$MORT)\nxm &lt;- as.matrix(airp[, -16])\nlibrary(glmnet)\nlambdas &lt;- exp(seq(-3, 10, length = 50))\nset.seed(123)\nop.la &lt;- 0\nfor (j in 1:20) {\n  tmp &lt;- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\n  op.la &lt;- op.la + tmp$lambda.min # tmp$lambda.1se\n}\nop.la &lt;- op.la / 20\nxm &lt;- scale(as.matrix(airp[, -16]), scale = FALSE)\nxm.svd &lt;- svd(xm)\n(est.edf &lt;- sum(xm.svd$d^2 / (xm.svd$d^2 + op.la)))\n#&gt; [1] 12.99595"
  },
  {
    "objectID": "20-ridge-regression.html#important-caveat",
    "href": "20-ridge-regression.html#important-caveat",
    "title": "5  Ridge regression",
    "section": "5.7 Important caveat!",
    "text": "5.7 Important caveat!\nNote that in the above discussion of EDF we have assumed that the matrix defining the linear predictor does not depend on the values of the response variable (that it only depends on the matrix X), as it is the case in linear regression. This is fine for Ridge Regression estimators as long as the penalty parameter was not chosen using the data. This is typically not the case in practice. Although the general definition of EDF still holds, it is not longer true that Ridge Regression yields a linear predictor, and thus the corresponding EDF may not be equal to the trace of the corresponding matrix.\n\n\n\n\nEfron, Bradley. 1986. “How Biased Is the Apparent Error Rate of a Prediction Rule?” Journal of the American Statistical Association 81 (394): 461–70."
  },
  {
    "objectID": "21-lasso-elnet.html#compare-mspes-of-ridge-lasso-on-the-credit-data",
    "href": "21-lasso-elnet.html#compare-mspes-of-ridge-lasso-on-the-credit-data",
    "title": "6  LASSO",
    "section": "6.1 Compare MSPEs of Ridge & LASSO on the credit data",
    "text": "6.1 Compare MSPEs of Ridge & LASSO on the credit data\nWe now use 50 runs of 5-fold cross-validation to estimate (and compare) the MSPEs of the different estimators / predictors:\n\nlibrary(MASS)\nn &lt;- nrow(xm)\nk &lt;- 5\nii &lt;- (1:n)%%k + 1\nset.seed(123)\nN &lt;- 50\nmspe.la &lt;- mspe.st &lt;- mspe.ri &lt;- mspe.f &lt;- rep(0, N)\nfor (i in 1:N) {\n    ii &lt;- sample(ii)\n    pr.la &lt;- pr.f &lt;- pr.ri &lt;- pr.st &lt;- rep(0, n)\n    for (j in 1:k) {\n        tmp.ri &lt;- cv.glmnet(x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n            nfolds = 5, alpha = 0, family = \"gaussian\")\n        tmp.la &lt;- cv.glmnet(x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n            nfolds = 5, alpha = 1, family = \"gaussian\")\n        null &lt;- lm(Balance ~ 1, data = x[ii != j, ])\n        full &lt;- lm(Balance ~ ., data = x[ii != j, ])\n        tmp.st &lt;- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n        pr.ri[ii == j] &lt;- predict(tmp.ri, s = \"lambda.min\", newx = xm[ii == j, ])\n        pr.la[ii == j] &lt;- predict(tmp.la, s = \"lambda.min\", newx = xm[ii == j, ])\n        pr.st[ii == j] &lt;- predict(tmp.st, newdata = x[ii == j, ])\n        pr.f[ii == j] &lt;- predict(full, newdata = x[ii == j, ])\n    }\n    mspe.ri[i] &lt;- mean((x$Balance - pr.ri)^2)\n    mspe.la[i] &lt;- mean((x$Balance - pr.la)^2)\n    mspe.st[i] &lt;- mean((x$Balance - pr.st)^2)\n    mspe.f[i] &lt;- mean((x$Balance - pr.f)^2)\n}\nboxplot(mspe.la, mspe.ri, mspe.st, mspe.f, names = c(\"LASSO\", \"Ridge\", \"Stepwise\",\n    \"Full\"), col = c(\"steelblue\", \"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1,\n    cex.lab = 1, cex.main = 2)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n\n\n\n\n\n\n\n\nWe see that in this example LASSO does not seem to provide better predictions than Ridge Regression. However, it does yield a sequence of explanatory variables that can be interpreted as based on “importance” for the linear regression model (see above)."
  },
  {
    "objectID": "21-lasso-elnet.html#comparing-lasso-with-ridge-regression-on-the-air-pollution-data",
    "href": "21-lasso-elnet.html#comparing-lasso-with-ridge-regression-on-the-air-pollution-data",
    "title": "6  LASSO",
    "section": "6.2 Comparing LASSO with Ridge Regression on the air pollution data",
    "text": "6.2 Comparing LASSO with Ridge Regression on the air pollution data\nLet us compare the Ridge Regression and LASSO fits to the air pollution data. Of course, by the Ridge Regression fit and the LASSO fit we mean the fit obtained with the optimal value of the penalty constant chosen in terms of the corresponding estimated MSPE (which is in general estimated using K-fold cross validation).\nWe first load the data and use cv.glmnet() with alpha = 0 to select an approximately optimal Ridge Regression fit (what makes the calculation below only approximately optimal?).\n\nairp &lt;- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\ny &lt;- as.vector(airp$MORT)\nxm &lt;- as.matrix(airp[, names(airp) != \"MORT\"])\nlambdas &lt;- exp(seq(-3, 10, length = 50))\n# Ridge Regression\nset.seed(23)\nair.l2 &lt;- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0,\n  family = \"gaussian\", intercept = TRUE\n)\nplot(air.l2)\n\n\n\n\n\n\n\n\nThe plot above is included for illustration purposes only. Similarly, we now compute an approximately optimal LASSO fit, and look at the curve of estimated MSPEs:\n\n# LASSO\nset.seed(23)\nair.l1 &lt;- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 1,\n  family = \"gaussian\", intercept = TRUE\n)\nplot(air.l1)\n\n\n\n\n\n\n\n\nIt is interesting to compare the corresponding estimated regression coefficients, so we put them side by side in two columns:\n\ncbind(\n  round(coef(air.l2, s = \"lambda.min\"), 3),\n  round(coef(air.l1, s = \"lambda.min\"), 3)\n)\n#&gt; 16 x 2 sparse Matrix of class \"dgCMatrix\"\n#&gt;                   s1       s1\n#&gt; (Intercept) 1129.267 1070.341\n#&gt; PREC           1.493    1.420\n#&gt; JANT          -0.999   -1.124\n#&gt; JULT          -1.054   -0.877\n#&gt; OVR65         -2.260    .    \n#&gt; POPN          -1.621    .    \n#&gt; EDUC          -8.280  -10.800\n#&gt; HOUS          -1.164   -0.380\n#&gt; DENS           0.005    0.003\n#&gt; NONW           2.895    3.825\n#&gt; WWDRK         -0.464    .    \n#&gt; POOR           0.653    .    \n#&gt; HC            -0.030    .    \n#&gt; NOX            0.056    .    \n#&gt; SO.            0.237    0.226\n#&gt; HUMID          0.388    .\n\nNote how several of them are relatively similar, but LASSO includes fewer of them. A possible explanation for this is the particular correlation structure among the explanatory variables. More specifically, when groups of correlated covariates are present, LASSO tends to choose only one of them, whereas Ridge Regression will tend to keep all of them. For a formal statement see (Zou and Hastie 2005, Lemma 2).\nIt is important to note here that the above observations regarding the Ridge Regression and LASSO fits trained on the air pollution data should be made on a more reliable (more stable, less variable) choice of penalty parameter. For example, we may want to run the above 5-fold CV experiments several times and take the average of the estimated optimal penalty parameters. To simplify the presentation we do not purse this here, but it may be a very good exercise for the reader to do so.\nThe following heatmap of the pairwise correlations among explanatory variables reveals certain patterns that may be used to explain the difference mentioned above. Note that in this visualization method variables were grouped (“clustered”) according to their pairwise correlations in order to improve the interpretability of the plot. We will see later in this course the particular clustering method used here (hierarchical clustering).\n\nlibrary(ggcorrplot)\nggcorrplot(cor(xm), hc.order = TRUE, outline.col = \"white\")"
  },
  {
    "objectID": "21-lasso-elnet.html#compare-mspe-of-ridge-and-lasso-on-air-pollution-data",
    "href": "21-lasso-elnet.html#compare-mspe-of-ridge-and-lasso-on-air-pollution-data",
    "title": "6  LASSO",
    "section": "6.3 Compare MSPE of Ridge and LASSO on air pollution data",
    "text": "6.3 Compare MSPE of Ridge and LASSO on air pollution data\nSince our focus was on the properties of the resulting predictions, it may be interesting to compare the estimated MSPE of the different models / predictors we have considered so far: a full linear model, a model selected via stepwise + AIC, ridge regression and LASSO. As usual, we use 50 runs of 5-fold CV, and obtain the following boxplots:\n\nlibrary(MASS)\nn &lt;- nrow(xm)\nk &lt;- 5\nii &lt;- (1:n) %% k + 1\nset.seed(123)\nN &lt;- 50\nmspe.la &lt;- mspe.st &lt;- mspe.ri &lt;- mspe.f &lt;- rep(0, N)\nfor (i in 1:N) {\n  ii &lt;- sample(ii)\n  pr.la &lt;- pr.f &lt;- pr.ri &lt;- pr.st &lt;- rep(0, n)\n  for (j in 1:k) {\n    tmp.ri &lt;- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 0, family = \"gaussian\"\n    )\n    tmp.la &lt;- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 1, family = \"gaussian\"\n    )\n    null &lt;- lm(MORT ~ 1, data = airp[ii != j, ])\n    full &lt;- lm(MORT ~ ., data = airp[ii != j, ])\n    tmp.st &lt;- stepAIC(null, scope = list(lower = null, upper = full), trace = FALSE)\n    pr.ri[ii == j] &lt;- predict(tmp.ri, s = \"lambda.min\", newx = xm[ii == j, ])\n    pr.la[ii == j] &lt;- predict(tmp.la, s = \"lambda.min\", newx = xm[ii == j, ])\n    pr.st[ii == j] &lt;- predict(tmp.st, newdata = airp[ii == j, ])\n    pr.f[ii == j] &lt;- predict(full, newdata = airp[ii == j, ])\n  }\n  mspe.ri[i] &lt;- mean((airp$MORT - pr.ri)^2)\n  mspe.la[i] &lt;- mean((airp$MORT - pr.la)^2)\n  mspe.st[i] &lt;- mean((airp$MORT - pr.st)^2)\n  mspe.f[i] &lt;- mean((airp$MORT - pr.f)^2)\n}\nboxplot(mspe.la, mspe.ri, mspe.st, mspe.f, names = c(\"LASSO\", \"Ridge\", \"Stepwise\", \"Full\"), col = c(\"steelblue\", \"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1, cex.lab = 1, cex.main = 2)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n\n\n\n\n\n\n\n\nWe see that there is a marginal advantage of LASSO, but it is rather minor, and the three methods we have seen so far improve by similar margins on the predictions obtained by using a full linear regression model."
  },
  {
    "objectID": "21-lasso-elnet.html#less-desirable-properties-of-lasso",
    "href": "21-lasso-elnet.html#less-desirable-properties-of-lasso",
    "title": "6  LASSO",
    "section": "6.4 Less desirable properties of LASSO",
    "text": "6.4 Less desirable properties of LASSO\nAs important as the LASSO estimator has been, its properties may sometimes not be fully satisfactory. In particular:\n\nThe LASSO selects the right variables only under very restrictive conditions (in other words, it is generally not “variable selection”-consistent).\nThe LASSO sampling distribution is not the same as the one we would obtain with the standard least squares estimator if we knew which features to include and which ones to exclude from the model (in orther words, the LASSO does not have an “oracle” property).\nWhen groups of correlated explanatory variables are present the LASSO tends to include only one variable (randomly) from the group, relegate the others to the end of the sequence.\n\nFor precise statements and theoretical results regarding the three points above, see (Zou and Hastie 2005; Zou 2006)."
  },
  {
    "objectID": "21-lasso-elnet.html#elastic-net",
    "href": "21-lasso-elnet.html#elastic-net",
    "title": "6  LASSO",
    "section": "6.5 Elastic net",
    "text": "6.5 Elastic net\nElastic Net estimators were introduced to find an informative compromise between LASSO and Ridge Regression.\nNote that cv.glmnet only considers fits with variying values of one of the penalty constants, while the other one (alpha) is kept fixed. To compare different Elastic Net fits we run cv.glmnet with 4 values of alpha: 0.05, 0.1, 0.5 and 0.75.\n\n# EN\nset.seed(23)\nair.en.75 &lt;- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0.75,\n  family = \"gaussian\", intercept = TRUE\n)\nset.seed(23)\nair.en.05 &lt;- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0.05,\n  family = \"gaussian\", intercept = TRUE\n)\nset.seed(23)\nair.en.1 &lt;- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0.1,\n  family = \"gaussian\", intercept = TRUE\n)\nset.seed(23)\nair.en.5 &lt;- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0.5,\n  family = \"gaussian\", intercept = TRUE\n)\nplot(air.en.05)\n\n\n\n\n\n\n\nplot(air.en.5)\n\n\n\n\n\n\n\nplot(air.en.75)\n\n\n\n\n\n\n\n\n\n6.5.1 Run EN on airpollution data, compare fits\nWe now compare the estimates of the regression coefficients obtained with the different methods discussed so far to alleviate potential problems caused by correlated covariates.\n\na &lt;- cbind(\n  round(coef(air.l2, s = \"lambda.min\"), 3),\n  round(coef(air.l1, s = \"lambda.min\"), 3),\n  round(coef(air.en.05, s = \"lambda.min\"), 3),\n  round(coef(air.en.1, s = \"lambda.min\"), 3),\n  round(coef(air.en.5, s = \"lambda.min\"), 3),\n  round(coef(air.en.75, s = \"lambda.min\"), 3)\n)\ncolnames(a) &lt;- c(\"Ridge\", \"LASSO\", \"EN-05\", \"EN-10\", \"EN-50\", \"EN-75\")\na\n#&gt; 16 x 6 sparse Matrix of class \"dgCMatrix\"\n#&gt;                Ridge    LASSO    EN-05    EN-10    EN-50    EN-75\n#&gt; (Intercept) 1129.267 1070.341 1116.791 1112.228 1101.074 1099.067\n#&gt; PREC           1.493    1.420    1.479    1.481    1.498    1.495\n#&gt; JANT          -0.999   -1.124   -0.968   -0.990   -1.124   -1.153\n#&gt; JULT          -1.054   -0.877   -1.036   -1.041   -1.156   -1.182\n#&gt; OVR65         -2.260    .       -1.099   -0.265    .        .    \n#&gt; POPN          -1.621    .        .        .        .        .    \n#&gt; EDUC          -8.280  -10.800   -8.277   -8.413   -9.585  -10.147\n#&gt; HOUS          -1.164   -0.380   -1.136   -1.102   -0.705   -0.575\n#&gt; DENS           0.005    0.003    0.005    0.005    0.004    0.004\n#&gt; NONW           2.895    3.825    3.187    3.454    3.816    3.895\n#&gt; WWDRK         -0.464    .       -0.422   -0.391   -0.141   -0.052\n#&gt; POOR           0.653    .        0.268    .        .        .    \n#&gt; HC            -0.030    .       -0.006   -0.003    .        .    \n#&gt; NOX            0.056    .        0.000    .        .        .    \n#&gt; SO.            0.237    0.226    0.242    0.241    0.233    0.230\n#&gt; HUMID          0.388    .        0.290    0.241    0.061    0.005\n\nThe same comment made above regarding the need of a more stable choice of “optimal” fits (for each of these methods) applies here. Again, here we limit ourselves to one run of 5-fold CV purely based on simplifying the presentation.\n\n\n6.5.2 Compare MSPE’s of Full, LASSO, Ridge, EN and stepwise\n\nii &lt;- (1:n) %% k + 1\nset.seed(123)\nN &lt;- 50\nmspe.en &lt;- rep(0, N)\nfor (i in 1:N) {\n  ii &lt;- sample(ii)\n  pr.en &lt;- rep(0, n)\n  for (j in 1:k) {\n    tmp.en &lt;- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 0.75, family = \"gaussian\"\n    )\n    pr.en[ii == j] &lt;- predict(tmp.en, s = \"lambda.min\", newx = xm[ii == j, ])\n  }\n  mspe.en[i] &lt;- mean((airp$MORT - pr.en)^2)\n}\nboxplot(mspe.en, mspe.la, mspe.ri, mspe.st, mspe.f,\n  names = c(\"EN\", \"LASSO\", \"Ridge\", \"Stepwise\", \"Full\"),\n  col = c(\"hotpink\", \"steelblue\", \"gray80\", \"tomato\", \"springgreen\"),\n  cex.axis = 1, cex.lab = 1, cex.main = 2\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n\n\n\n\n\n\n\n\nWe see that in this example Elastic Net with alpha = 0.75 (which is not far from the LASSO) provides slightly better estimated MSPEs.\n\n\n\n\nZou, Hui. 2006. “The Adaptive Lasso and Its Oracle Properties.” Journal of the American Statistical Association 101 (476): 1418–29.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67 (2): 301–20."
  },
  {
    "objectID": "22-nonpar-splines-poly.html#polynomial-regression",
    "href": "22-nonpar-splines-poly.html#polynomial-regression",
    "title": "7  Non-parametric regression",
    "section": "7.1 Polynomial regression",
    "text": "7.1 Polynomial regression\nTo illustrate these basis methods, we will consider the lidar data, available in the package SemiPar. More information is available from the corresponding help page: help(lidar, package='SemiPar'). We now load the data and plot it, the response variable is logratio and the explanatory one is range:\n\ndata(lidar, package = \"SemiPar\")\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\n\n\n\n\n\n\n\n\nIn class we discussed the formal motivation to look into a polynomial approximation of the regression function. This argument, however, does not specify which degree of the approximating polynomial to use. Here we first try a 4th degree polynomial and the problem reduces to a linear regression one (see the lecture slides). We can use a command like lm(logratio ~ range + range^2 + range^3 + range^4). However, this call to lm will not work as we intend it (I recommend that you check this and find out the reason why). Instead, we would need to use something like lm(logratio ~ range + I(range^2) + I(range^3) + I(range^4)). To avoid having to type a long formula, we can instead use the function poly() in R to generate the design matrix containing the desired powers of range, and plug that into the call to lm(). The code below fits two such approximations (a 3rd degree and a 4th degree polynomial), plots the data and overlays the estimated regression function:\n\n# Degree 4 polynomials\npm &lt;- lm(logratio ~ poly(range, 4), data = lidar)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(pm)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"blue\")\npm3 &lt;- lm(logratio ~ poly(range, 3), data = lidar)\nlines(predict(pm3)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"hotpink\")\nlegend(\"topright\", legend = c(\"3rd degree\", \"4th degree\"), lwd = 6, col = c(\"hotpink\", \"blue\"))\n\n\n\n\n\n\n\n\nNote that this fit is reasonable, although there is probably room for improvement. Based on the formal motivation discussed in class to use polynomials in the first place, it may seem natural to increase the order of the approximating polynomial in order to improve the quality of the approximation. However, this is easily seen not to be a good idea. Below we compare the 4th degree approximation used above (in blue) with a 10th degree one (in red):\n\n# Degree 10 polynomials\npm2 &lt;- lm(logratio ~ poly(range, 10), data = lidar)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(pm)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"blue\")\nlines(predict(pm2)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"red\")\n\n\n\n\n\n\n\n\nNote that the 10th order fit follows the data much more closely, but it starts to become “too adaptive” and departing quite often from the main (larger scale) trend we associate with the regression (conditional mean) function.\n(Can you explain the discrepancy between what we observe above and the motivation we used in class, that suggests that higher order polynomials provide better approximations?)"
  },
  {
    "objectID": "22-nonpar-splines-poly.html#a-more-stable-basis-splines",
    "href": "22-nonpar-splines-poly.html#a-more-stable-basis-splines",
    "title": "7  Non-parametric regression",
    "section": "7.2 A more stable basis: splines",
    "text": "7.2 A more stable basis: splines\nPart of the problem with global polynomial bases as the ones used above is that they necessarily become more wiggly within the range of the data, and also quickly increase or decrease near the edge of the observations. A more stable but also remarkably flexible basis is given by spline functions, as discussed in class.\nWe first here show how to build a naive linear spline basis with 5 knots (placed at the (1:5)/6 quantiles (i.e. the 0.17, 0.33, 0.5, 0.67, 0.83 percentiles) of the observed values of the explanatory variable), and use it to estimate the regression function. Remember that a linear spline function with knot w is given by f_w(x) = max( x - w, 0 ). Given a fixed set of pre-selected knots w_1, w_2, …, w_k, we consider regression functions that are linear combinations of the corresponding k linear spline functions.\nNote that for higher-order splines (e.g. cubic splines discussed below), the naive spline basis used above is numerically very unstable, and usually works poorly in practice. I include it here simply as an illustration of the methodology and to stress the point that this type of approach (that estimates the regression function as a linear combination of an explicit basis) is in fact nothing more than slightly more complex linear models.\nFirst we find the 5 knots mentioned above that will be used to construct the spline basis:\n\n# select the knots at 5 specific quantiles\n(kn &lt;- as.numeric(quantile(lidar$range, (1:5) / 6)))\n#&gt; [1] 444.6667 499.6667 555.0000 609.6667 664.6667\n\nNow we compute the matrix of “explanatory variables”, that is: the matrix that in its columns has each of the 5 basis functions f_1, f_2, …, f_5 evaluated at the n observed values of the (single) explanatory variable x_1, …, x_n. In other words, the matrix X has in its (i, j) cell the value f_j(x_i), for j=1, …, k, and i=1, …, n. In the code below we use (abuse?) R’s recycling rules when operating over vectors and arrays (can you spot it?)\n\n# prepare the matrix of covariates / explanatory variables\nx &lt;- matrix(0, dim(lidar)[1], length(kn) + 1)\nfor (j in 1:length(kn)) {\n    x[, j] &lt;- pmax(lidar$range - kn[j], 0)\n}\nx[, length(kn) + 1] &lt;- lidar$range\n\nNow that we have the matrix of our “explanatory variables”, we can simply use lm to estimate the coefficients of the linear combination of the functions in the spline basis that will provide our regression function estimator. We then plot the data and overlay the fitted / estimated regression function:\n\n# Fit the regression model\nppm &lt;- lm(lidar$logratio ~ x)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(ppm)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"hotpink\")\n\n\n\n\n\n\n\n\nThere are better (numerically more stable) bases for the same linear space spanned by these spline functions. These bases have different numerical properties and can become cumbersome to describe. Here we use the function bs (in package splines) to build a B-spline basis. For an accessible discussion, see for example (Wood 2017, sec. 4.1).\nGiven the chosen knots and the degree of the splines (linear, quadratic, cubic, etc.) the set (linear space) of functions we are using to construct our regression estimate does not depend on the specific basis we use (in other words: these are different bases that span the same linear space of functions). As a consequence, the estimated regression function should be the same regardless of the basis we use (provided we do not run into serious numerical issues with our naive basis). To illustrate this fact, we will use a B-spline basis with the same 5 knots as above, and compare the estimated regression function with the one we obtained above using our poor people naive basis. The plot below overlays both fits (the naive one with a thick pink line as above, and the one using b-splines with a thinner blue line):\n\nlibrary(splines)\nppm2 &lt;- lm(logratio ~ bs(range, degree = 1, knots = kn), data = lidar)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(ppm)[order(range)] ~ sort(range), data = lidar, lwd = 8, col = \"hotpink\")\nlines(predict(ppm2)[order(range)] ~ sort(range), data = lidar, lwd = 3, col = \"darkblue\")\n\n\n\n\n\n\n\n\nAs expected, both fits provide the same estimated regression function, although its coefficients are naturally different (but their lengths are the same, is this a coincidence, or will it always happen?)\n\nas.vector(coef(ppm))\n#&gt; [1]  0.0269095640  0.0002488169 -0.0003235802 -0.0082773735  0.0063779378\n#&gt; [6]  0.0007385513 -0.0001847752\nas.vector(coef(ppm2))\n#&gt; [1] -0.04515276 -0.01010104 -0.00657875 -0.02093988 -0.48762440 -0.60636798\n#&gt; [7] -0.68496471\n\nNote that, because we are using a set of linear splines, our estimated regression functions will always be piecewise linear (i.e. linear functions between each pair of knots). To obtain smoother (e.g. differentiable, continuously differentiable, or even twice continously differentiable) regression estimators below we will use higher-order splines."
  },
  {
    "objectID": "22-nonpar-splines-poly.html#higher-order-splines-quadratic-cubic-etc.",
    "href": "22-nonpar-splines-poly.html#higher-order-splines-quadratic-cubic-etc.",
    "title": "7  Non-parametric regression",
    "section": "7.3 Higher order splines (quadratic, cubic, etc.)",
    "text": "7.3 Higher order splines (quadratic, cubic, etc.)\nIn what follows (as above) we will use the function bs to evaluate the desired spline basis on the observed values of the explanatory variable (in this case range).\nWe use the arguments degree = 2 and knots = kn to indicate we want a quadratic spline basis with knots located at the elements of the vector kn. As before, we then simply use lm to estimate the coefficients, and overlay the estimated regression function over the data:\n\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nppmq &lt;- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nlines(predict(ppmq)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"steelblue\")\n\n\n\n\n\n\n\n\nA useful consequence of the fact that these regression estimators are in fact just linear regression estimators (but using a richer / more flexible basis than just the straight predictors) is that we can easily compute (pointwise) standard errors for the fitted regression curve, as follows. We first fit and plot a quadratic spline using the same 5 knots as before:\n\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nppmc &lt;- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"gray30\")\n\nTo compute the estimated standard error of the predicted regression curve on a grid of values of the explanatory variable range, we first build a grid of 200 equally spaced points within the observed scope of the variable range:\n\nxx &lt;- seq(min(lidar$range), max(lidar$range), length = 200)\n\nThe predict method for objects of class lm returns estimated standard errors for each fitted value if we set the argument se.fit = TRUE:\n\nppmc &lt;- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nps &lt;- predict(ppmc, newdata = list(range = xx), se.fit = TRUE)\n\nWe now compute upper and lower confidence bands (I used 2 standard errors) around the fitted regression line:\n\nup &lt;- (ps$fit + 2 * ps$se.fit)\nlo &lt;- (ps$fit - 2 * ps$se.fit)\n\nFinally, we display the confidence bands we just constructed (using base R graphics, but also consider using ggplot2):\n\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 4, col = \"gray30\")\nmyrgb &lt;- col2rgb(\"red\") / 256 # , alpha=TRUE)\nmyrgb &lt;- rgb(red = myrgb[1], green = myrgb[2], blue = myrgb[3], alpha = .3)\npolygon(c(xx, rev(xx)), c(up, rev(lo)), density = NA, col = myrgb) #' lightblue')\nlines(ps$fit ~ xx, data = lidar, lwd = 4, col = \"blue\")\n\n\n\n\n\n\n\n\nIt is important to note that the above confidence bands were constructed assuming that the knots were fixed (not random), and similarly for the degree of the spline basis.\nIncreasing the degree of the cubic basis yields smoother fits (having higher order continuous derivatives). For example, using cubic splines yields an even smoother fit:\n\n# cubic splines\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nppmc &lt;- lm(logratio ~ bs(range, degree = 3, knots = kn), data = lidar)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"tomato3\")\n\n\n\n\n\n\n\n\nNote that the estimated regression function seems to have started to “twitch” and wiggle, particularly at the upper end of our observations."
  },
  {
    "objectID": "22-nonpar-splines-poly.html#how-many-knots-should-we-use",
    "href": "22-nonpar-splines-poly.html#how-many-knots-should-we-use",
    "title": "7  Non-parametric regression",
    "section": "7.4 How many knots should we use?",
    "text": "7.4 How many knots should we use?\nSo far we have used 5 knots, but we could have used any other number of knots. If we consider a quadratic spline basis with 10 knots, the fit appears a bit better (at least aesthetically):\n\nk &lt;- 10\nkn &lt;- as.numeric(quantile(lidar$range, (1:k)/(k + 1)))\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nppmc &lt;- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"tomato3\")\n\n\n\n\n\n\n\n\nWhat about using more knots? The following plot used 50 knots:\n\nk &lt;- 50\nkn &lt;- as.numeric(quantile(lidar$range, (1:k) / (k + 1)))\nppmc &lt;- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"hotpink\")\n\n\n\n\n\n\n\n\nClearly not a good idea!"
  },
  {
    "objectID": "22-nonpar-splines-poly.html#smoothing-splines",
    "href": "22-nonpar-splines-poly.html#smoothing-splines",
    "title": "7  Non-parametric regression",
    "section": "7.5 Smoothing splines",
    "text": "7.5 Smoothing splines\nIf we were to follow the approach discussed so far we would need to find an “optimal” of selecting the number of knots and their positions, plus the order of the spline basis. Although one could consider using cross-validation for this, we note that this would require considerable computational effort (we would need to perform an exhaustive search on a 3-dimensional grid).\nWe saw in class that natural cubic splines provide a natural optimal space to look for a good regression estimator. For a formal but surprisingly simple proof of this optimality result, see again Section 4.1 of\n\nWood, S. (2006). Generalized additive models : an introduction with R. Chapman & Hall/CRC, Boca Raton, FL. Library link.\n\nThis result not only justifies using natural cubic splines, but also eliminates many of the unknown “tuning parameters” (the degree of the spline basis, the number of knots, and their locations). In fact, we only need to select one tuning parameter–the penalty term, which can be done using any cross-validation “flavour” (although in this setting leave-one-out CV is particularly appealing, as we discussed in class).\nThe function smooth.spline in R computes a cubic smoothing spline (natural cubic spline). Details on its arguments and different options are available from its help page.\nWhen applied to the lidar data with penalization parameter equal to 0.2 (setting the argument spar = 0.2) we obtain the following estimated regression function:\n\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\ntmp &lt;- smooth.spline(x = lidar$range, y = lidar$logratio, spar = 0.2, cv = FALSE,\n    all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 6, col = \"magenta\")\n\n\n\n\n\n\n\n\nThis fit is clearly too wiggly and unsatisfactory. To obtain a smoother fit we increase the penalty term to 0.5:\n\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\ntmp &lt;- smooth.spline(x = lidar$range, y = lidar$logratio, spar = 0.5, cv = FALSE,\n    all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 6, col = \"red\")\n\n\n\n\n\n\n\n\nThe larger the penalty parameter, the smoother the fit. Setting it to 0.8 yields:\n\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\ntmp &lt;- smooth.spline(x = lidar$range, y = lidar$logratio, spar = 0.8, cv = FALSE,\n    all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 6, col = \"blue\")\n\n\n\n\n\n\n\n\nIt is easy to see that the larger the penalty coefficient the closer the resulting natural cubic spline becomes to a linear function (why?). For example, if we use smooth.spline(spar=2):\n\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\ntmp &lt;- smooth.spline(x = lidar$range, y = lidar$logratio, spar = 2, cv = FALSE, all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 6, col = \"tomato3\")"
  },
  {
    "objectID": "22-nonpar-splines-poly.html#selecting-an-optimal-penalty-parameter",
    "href": "22-nonpar-splines-poly.html#selecting-an-optimal-penalty-parameter",
    "title": "7  Non-parametric regression",
    "section": "7.6 Selecting an “optimal” penalty parameter",
    "text": "7.6 Selecting an “optimal” penalty parameter\nAs discussed in class, an “optimal” natural cubic spline can be found using cross-validation, and for these linear predictors, leave-one-out cross-validation is particularly attractive (in terms of computational cost). The function smooth.spline in R will compute (and use) an optimal value for the penalty term using leave-one-out cross-validation when we set the argument cv = TRUE:\n\ntmp.cv &lt;- smooth.spline(x = lidar$range, y = lidar$logratio, cv = TRUE, all.knots = TRUE)\n# tmp.cv$spar = 0.974\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(tmp.cv$y ~ tmp.cv$x, lwd = 6, col = \"blue\")\n\n\n\n\n\n\n\n\n\n7.6.1 Sanity check (always a good idea)\nNote that the optimal value found for the regularization parameter (spar) is also returned in the element $spar of the object returned by smooth.spline. Just as a sanity check we can now call smooth.spline with cv = FALSE and manually set spar to this optimal value, and verify that we obtain the same fit:\n\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(tmp.cv$y ~ tmp.cv$x, lwd = 8, col = \"blue\")\ntmp &lt;- smooth.spline(x = lidar$range, y = lidar$logratio, spar = tmp.cv$spar, cv = FALSE,\n    all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 3, col = \"red\")"
  },
  {
    "objectID": "22-nonpar-splines-poly.html#the-problem-of-outliers-and-other-model-departures",
    "href": "22-nonpar-splines-poly.html#the-problem-of-outliers-and-other-model-departures",
    "title": "7  Non-parametric regression",
    "section": "7.7 The problem of outliers and other model departures",
    "text": "7.7 The problem of outliers and other model departures\nWhen the data may contain outliers and/or other atypical observations, the estimation methods discussed above may be seriously affected, even if there are only a few such aberrant data points in the training set (possible outliers in the test / validation set are also a concern, but we don’t have time to discuss it here). Some robust estimation methods based on splines exist. See for example (Tharmaratnam et al. 2010) and references therein. Software in R implementing this method is available here.\n\n\n\n\nTharmaratnam, Kukatharmini, Gerda Claeskens, Christophe Croux, and Matias Salibián-Barrera. 2010. “S-Estimation for Penalized Regression Splines.” Journal of Computational and Graphical Statistics 19 (3): 609–25. https://doi.org/10.1198/jcgs.2010.08149.\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction with r. CRC press."
  },
  {
    "objectID": "23-kernel-regression.html#the-problem-of-outliers-and-other-model-departures",
    "href": "23-kernel-regression.html#the-problem-of-outliers-and-other-model-departures",
    "title": "8  Kernel regression / local regression",
    "section": "8.1 The problem of outliers and other model departures",
    "text": "8.1 The problem of outliers and other model departures\nWhen the data may contain outliers and/or other atypical observations, the estimation methods discussed above may be seriously affected, even if there are only a few such aberrant data points in the training set (possible outliers in the test / validation set are also a concern, but we don’t have time to discuss it here). Some robust estimation methods based on kernel smoothers exist. See for example (Boente, Martínez, and Salibián-Barrera 2017) and references therein. This paper deals with a slightly more complex model (additive model), but when only component exists, it is the same model discussed in class. The RBF package implementing this method is available from CRAN and also here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoente, Graciela, Alejandra Martínez, and Matías Salibián-Barrera. 2017. “Robust Estimators for Additive Models Using Backfitting.” Journal of Nonparametric Statistics 29 (4): 744–67. https://doi.org/10.1080/10485252.2017.1369077."
  },
  {
    "objectID": "24-trees.html#curse-of-dimensionality",
    "href": "24-trees.html#curse-of-dimensionality",
    "title": "9  Regression trees",
    "section": "9.1 Curse of dimensionality",
    "text": "9.1 Curse of dimensionality\nSuppose you have a random sample of n = 100 observations, uniformly distributed on the [0, 1] interval. How many do you expect to find within 0.25 of the middle point of the interval (i.e. how many will be between 0.25 and 0.75)? A trivial calculation shows that the expected number of observations falling between 0.25 and 0.75 will be n/2, in this case 50. This is easy verified with a simple numerical experiment:\n\n# X ~ U(0,1)\n# how many points do you expect within 0.25 of 1/2?\nset.seed(1234)\nn &lt;- 100\nx &lt;- runif(n)\n(sum(abs(x - 1 / 2) &lt; 0.25)) # half the width of the dist'n\n#&gt; [1] 50\n\n(wow! what are the chances?)\nConsider now a sample of 100 observations, each with 5 variables (5-dimensional observations), uniformly distributed in the 5-dimensional unit cube ([0,1]^5). How many do you expect to see in the central hypercube with sides [0.25, 0.75] x [0.25, 0.75] … x [0.25, 0.75] = [0.25, 0.75]^5? A simple experiment shows that this number is probably rather small:\n\np &lt;- 5\nx &lt;- matrix(runif(n * p), n, p)\n# how many points in the hypercube (0.25, 0.75)^p ?\ntmp &lt;- apply(x, 1, function(a) all(abs(a - 1 / 2) &lt; 0.25))\n(sum(tmp))\n#&gt; [1] 4\n\nIn fact, the expected number of observations in that central hypercube is exactly n / 2^5, which is approximately 3 when n = 100.\nA relevant question for our local regression estimation problem is: “how large should our sample be if we want to still have about 50 observations in our central hypercube?”. Easy calculations show that this number is 50 / (1/2)^p, which, for p = 5 is 1600. Again, we can verify this with a simple experiment:\n\n# how many obs do we need to have 50 in the hypercube?\nn &lt;- 50 / (0.5^p)\nx &lt;- matrix(runif(n * p), n, p)\n# how many points in the hypercube (0.25, 0.75)^p ?\ntmp &lt;- apply(x, 1, function(a) all(abs(a - 1 / 2) &lt; 0.25))\n(sum(tmp))\n#&gt; [1] 57\n\nSo we see that if the dimension of our problem increases from p = 1 to p = 5, the number of observations we need to maintain an expectation of having about 50 points in our central hypercube increases by a factor of 16 (not 5). However, if we double the dimension of the problem (to p = 10), in order to expect 50 observations in the central [0.25, 0.75] hypercube we need a sample of size n = 51,200. In other words, we doubled the dimension, but need 32 times more data (!) to fill the central hypercube with the same number of points. Moreover, if we doubled the dimension again (to p = 20) we would need over 52 million observations to have (just!) 50 in the central hypercube! Note that now we doubled the dimension again but need 1024 times more data! The number of observations needed to maintain a fixed number of observations in a region of the space grows exponentially with the dimension of the space.\nAnother way to think about this problem is to ask: “given a sample size of n = 1000, say, how wide / large should the central hypercube be to expect about 50 observations in it?”. The answer is easily found to be 1 / (2 (n/50)^(1/p)), which for n = 1000 and p = 5 equals 0.27, with p = 10 is 0.37 and with p = 20 is 0.43, almost the full unit hypercube!\nIn this sense it is fair to say that in moderate to high dimensions local neighbourhoods are either empty or not really local."
  },
  {
    "objectID": "24-trees.html#regression-trees-as-constrained-non-parametric-regression",
    "href": "24-trees.html#regression-trees-as-constrained-non-parametric-regression",
    "title": "9  Regression trees",
    "section": "9.2 Regression trees as constrained non-parametric regression",
    "text": "9.2 Regression trees as constrained non-parametric regression\nRegression trees provide an alternative non-regression estimator that works well, even with many available features. As discussed in class, the basic idea is to approximate the regression function by a linear combination of “simple” functions (i.e. functions \\(h(x) = I( x \\in A )\\) which equal 1 if the argument x belongs to the set A, and 0 otherwise. Each function has its own support set A. Furthermore, this linear combination is not estimated at once, but iteratively, and only considering a specific class of sets A (which ones?) As a result, the regression tree is not the global optimal approximation by simple functions, but a good suboptimal one, that can be computed very rapidly. Details were discussed in class, refer to your notes and the corresponding slides.\nThere are several packages in R implementing trees, in this course we will use rpart. To illustrate their use we will consider the Boston data set, that contains information on housing in the US city of Boston. The corresponding help page contains more information.\nHere, to simplify the comparison of the predictions obtained by trees and other regression estimators, instead of using K-fold CV, we start by randomly splitting the available data into a training and a test set:\n\nlibrary(rpart)\ndata(Boston, package = \"MASS\")\n# split data into a training and\n# a test set\nset.seed(123456)\nn &lt;- nrow(Boston)\nii &lt;- sample(n, floor(n / 4))\ndat.te &lt;- Boston[ii, ]\ndat.tr &lt;- Boston[-ii, ]\n\nWe now build a regression tree using the function rpart and leave most of its arguments to their default values. We specify the response and explanatory variables using a formula, as usual, and set method='anova' to indicate we want to train a regression tree (as opposed to a classification one, for example). Finally, we use the corresponding plot method to display the tree structure:\n\nset.seed(123)\nbos.t &lt;- rpart(medv ~ ., data = dat.tr, method = \"anova\")\nplot(bos.t, uniform = FALSE, margin = 0.05)\ntext(bos.t, pretty = TRUE)\n\n\n\n\n\n\n\n\nA few questions for you:\n\nWhy did we set the pseudo-random generation seed (set.seed(123)) before calling rpart? Is there anything random about building these trees?\nWhat does the uniform argument for plot.rpart do? What does text do here?"
  },
  {
    "objectID": "24-trees.html#compare-predictions",
    "href": "24-trees.html#compare-predictions",
    "title": "9  Regression trees",
    "section": "9.3 Compare predictions",
    "text": "9.3 Compare predictions\nWe now compare the predictions we obtain on the test with the above regression tree, the usual linear model using all explanatory variables, another one constructed using stepwise variable selections methods, and the “optimal” LASSO.\nFirst, we estimate the MSPE of the regression tree using the test set:\n\n# predictions on the test set\npr.t &lt;- predict(bos.t, newdata = dat.te, type = \"vector\")\nwith(dat.te, mean((medv - pr.t)^2))\n#&gt; [1] 16.07227\n\nFor a full linear model, the estimated MSPE using the test set is:\n\n# full linear model\nbos.lm &lt;- lm(medv ~ ., data = dat.tr)\npr.lm &lt;- predict(bos.lm, newdata = dat.te)\nwith(dat.te, mean((medv - pr.lm)^2))\n#&gt; [1] 23.25844\n\nThe estimated MSPE of a linear model constructed via stepwise is:\n\nlibrary(MASS)\nnull &lt;- lm(medv ~ 1, data = dat.tr)\nfull &lt;- lm(medv ~ ., data = dat.tr)\nbos.aic &lt;- stepAIC(null, scope = list(lower = null, upper = full), trace = FALSE)\npr.aic &lt;- predict(bos.aic, newdata = dat.te)\nwith(dat.te, mean((medv - pr.aic)^2))\n#&gt; [1] 22.99864\n\nFinally, the estimated MSPE of the “optimal” LASSO fit is:\n\n# LASSO?\nlibrary(glmnet)\nx.tr &lt;- as.matrix(dat.tr[, -14])\ny.tr &lt;- as.vector(dat.tr$medv)\nset.seed(123)\nbos.la &lt;- cv.glmnet(x = x.tr, y = y.tr, alpha = 1)\nx.te &lt;- as.matrix(dat.te[, -14])\npr.la &lt;- predict(bos.la, s = \"lambda.1se\", newx = x.te)\nwith(dat.te, mean((medv - pr.la)^2))\n#&gt; [1] 26.58914\n\nNote that the regression tree appears to have the best MSPE, although we cannot really assess whether the observed differences are beyond the uncertainty associated with our MSPE estimators. In other words, would these differences still be so if we used a different training / test data split? In fact, a very good exercise for you would be to repeat the above comparison using many different training/test splits, or even better: using all the data for training and K-fold CV to estimate the different MSPEs."
  },
  {
    "objectID": "25-more-trees.html#pruning-regression-trees-with-tree",
    "href": "25-more-trees.html#pruning-regression-trees-with-tree",
    "title": "10  Pruning regression trees with rpart",
    "section": "10.1 Pruning regression trees with tree",
    "text": "10.1 Pruning regression trees with tree\nThe implementation of trees in the R package tree follows the original CV-based pruning strategy, as discussed in Section 3.4 of the book\n\nBreiman, L., Friedman, J.H., Olshen, R.A. and Stone, C.J. (1984). Classification and regression trees. Chapman & Hall.\n\nor Section 7.2 of:\n\nRipley, Brian D. (1996). Pattern recognition and neural networks. Cambridge University Press\n\nBoth books are available in electronic form from the UBC Library: Breiman et al. and Ripley, B.D..\nWe now use the function tree::tree() to fit the same regression tree as above. Note that the default stopping criteria in this implementation of regression trees is different from the one in rpart::rpart(), hence to obtain the same results as above we need to modify the default stopping criteria using the argument control:\n\nlibrary(tree)\nbos.t2 &lt;- tree(medv ~ ., data = dat.tr, control = tree.control(nobs = nrow(dat.tr), mincut = 6, minsize = 20))\n\nWe plot the resulting tree\n\nplot(bos.t2)\ntext(bos.t2)\n\n\n\n\n\n\n\n\nAs discussed before, we now fit a very large tree, which will be pruned later:\n\nset.seed(123)\nbos.to2 &lt;- tree(medv ~ .,\n  data = dat.tr,\n  control = tree.control(nobs = nrow(dat.tr), mincut = 1, minsize = 2, mindev = 1e-5)\n)\nplot(bos.to2)\n\n\n\n\n\n\n\n\nWe now use the function tree:cv.tree() to estimate the MSPE of the subtrees of bos.to2, using 5-fold CV, and plot the estimated MSPE (here labeled as “deviance”) as a function of the complexity parameter (or, equivalently, the size of the tree):\n\nset.seed(123)\ntt &lt;- cv.tree(bos.to2, K = 5)\nplot(tt)\n\n\n\n\n\n\n\n\nFinally, we use the function prune.tree to prune the larger tree at the “optimal” size, as estimated by cv.tree above:\n\nbos.pr2 &lt;- prune.tree(bos.to2, k = tt$k[max(which(tt$dev == min(tt$dev)))])\nplot(bos.pr2)\ntext(bos.pr2)\n\n\n\n\n\n\n\n\nCompare this pruned tree with the one obtained with the regression trees implementation in rpart. In particular, we can compare the predictions of this other pruned tree on the test set:\n\n# predictions are worse than the rpart-pruned tree\npr.tree &lt;- predict(bos.pr2, newdata = dat.te, type = \"vector\")\nwith(dat.te, mean((medv - pr.tree)^2))\n#&gt; [1] 15.7194\n\nNote that the predictions of the tree pruned with the tree package seem to be better than those of the tree pruned with the rpart package. Does this mean that rpart gives trees with worse predictions than tree for data coming from the process than generated our training set? Or could it all be an artifact of the specific test set we used? Can you think of an experiment to check this? Again, it would be a very good exercise for you to check which fit (tree or rpart) gives pruned trees with better prediction properties in this case."
  },
  {
    "objectID": "25-more-trees.html#instability-of-regression-trees",
    "href": "25-more-trees.html#instability-of-regression-trees",
    "title": "10  Pruning regression trees with rpart",
    "section": "10.2 Instability of regression trees",
    "text": "10.2 Instability of regression trees\nTrees can be rather unstable, in the sense that small changes in the training data set may result in relatively large differences in the fitted trees. As a simple illustration we randomly split the Boston data used before into two halves and fit a regression tree to each portion. We then display both trees.\n\n# Instability of trees...\nlibrary(rpart)\ndata(Boston, package = \"MASS\")\nset.seed(123)\nn &lt;- nrow(Boston)\nii &lt;- sample(n, floor(n / 2))\ndat.t1 &lt;- Boston[-ii, ]\nbos.t1 &lt;- rpart(medv ~ ., data = dat.t1, method = \"anova\")\nplot(bos.t1, uniform = FALSE, margin = 0.01)\ntext(bos.t1, pretty = TRUE, cex = .8)\n\n\n\n\n\n\n\n\n\ndat.t2 &lt;- Boston[ii, ]\nbos.t2 &lt;- rpart(medv ~ ., data = dat.t2, method = \"anova\")\nplot(bos.t2, uniform = FALSE, margin = 0.01)\ntext(bos.t2, pretty = TRUE, cex = .8)\n\n\n\n\n\n\n\n\nAlthough we would expect both random halves of the same (moderately large) training set to beat least qualitatively similar, Note that the two trees are rather different. To compare with a more stable predictor, we fit a linear regression model to each half, and look at the two sets of estimated coefficients side by side:\n\n# bos.lmf &lt;- lm(medv ~ ., data=Boston)\nbos.lm1 &lt;- lm(medv ~ ., data = dat.t1)\nbos.lm2 &lt;- lm(medv ~ ., data = dat.t2)\ncbind(\n  round(coef(bos.lm1), 2),\n  round(coef(bos.lm2), 2)\n)\n#&gt;               [,1]   [,2]\n#&gt; (Intercept)  35.47  32.35\n#&gt; crim         -0.12  -0.09\n#&gt; zn            0.04   0.05\n#&gt; indus         0.01   0.03\n#&gt; chas          0.90   3.98\n#&gt; nox         -23.90 -12.33\n#&gt; rm            5.01   3.39\n#&gt; age          -0.01   0.00\n#&gt; dis          -1.59  -1.41\n#&gt; rad           0.33   0.28\n#&gt; tax          -0.01  -0.01\n#&gt; ptratio      -1.12  -0.72\n#&gt; black         0.01   0.01\n#&gt; lstat        -0.31  -0.66\n\nNote that most of the estimated regression coefficients are similar, and all of them are at least qualitatively comparable."
  },
  {
    "objectID": "30-lda-logit.html#linear-discriminant-analysis",
    "href": "30-lda-logit.html#linear-discriminant-analysis",
    "title": "11  Parametric classifiers",
    "section": "11.1 Linear Discriminant Analysis",
    "text": "11.1 Linear Discriminant Analysis\nProbably the “second easiest approach”to estimate the above probability (what would be the easiest one?) is to model the distribution of the explanatory variables within each class (that is, to model the distribution of X | G = g for each possible class g). These conditional distributions will then uniquely determine the probabilities we need to estimate, as discussed above and in class. In particular, one the simplest models we can use for X | G = g is a Normal (Gaussian) multivariate distribution. As we saw in class, if we assume that the distribution of the features for each class is Gaussian with a common covariance matrix across clases, then it easy to show (and I strongly suggest that you do it) that the optimal classifier (using the 0-1 loss function mentioned above) is a linear function of the explanatory variables. The coefficients of this linear function depend on the parameters of the assumed Gaussian distributions, which can be estimated using MLE on the training set. Plugging these parameter estimates in P( G = g | X) provides a natural estimator of each of these conditional probabilities, and thus we can compute an approximation to the optimal classifier.\nThe function lda in the MASS library implements this simple classifier. We illustrate it here on the rather simple and well-known vaso constriction data, available in the robustbase package. More details, as usual, can be found on its help page. The response variable takes two values (represented below as blue and red), and there are only two explanatory variables (which allows us to visualize our methods and results).\n\ndata(vaso, package = \"robustbase\")\nplot(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\n\n\n\n\n\n\n\n\nTo train the LDA classifier we use the function lda as follows (note the model-like syntax to indicate the response and explanatory variables):\n\nlibrary(MASS)\na.lda &lt;- lda(Y ~ Volume + Rate, data = vaso)\n\nNow, given any value of the explanatory variables (Volume, Rate) we can use the method predict on the object returned by lda() to estimate the conditional probabilities of blue and red.\nTo visualize which regions of the feature space will be predicted to contain blue points (and then obviously which areas will be predicted to correspond to red responses) we will construct a relatively fine 2-dimensional grid of posible values of the explanatory variables ((Volume, Rate)):\n\nxvol &lt;- seq(0, 4, length = 200)\nxrat &lt;- seq(0, 4, length = 200)\nthe.grid &lt;- expand.grid(xvol, xrat)\nnames(the.grid) &lt;- c(\"Volume\", \"Rate\")\n\nand estimate the probabilities of the 2 classes for each point in this grid:\n\npr.lda &lt;- predict(a.lda, newdata = the.grid)$posterior\n\nFinally, we plot the corresponding “surface” of predictions for one class (i.e. the conditional probabilites for that class as a function of the explanatory variables):\n\nimage(xrat, xvol, matrix(pr.lda[, 2], 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", main = \"LDA\"\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\n\n\n\n\n\n\n\n\nOn the plot above higher numbers are shown with lighther colors (dark green corresponds to very low conditional probabilities).\n\n11.1.1 Further considerations\nThis model-based approach to classification (LDA) is optimal if the model is correct. The strongest assumption of this model is, of course, the Gaussian conditional distribution of the vector of explanatory variables: X | G = g has a N( mu, Sigma) distribution. The second strongest assumption is that of equal “shape” (in other words, that the covariance matrix Sigma above does not depend on g). This latter assumption can be relaxed slightly if we assume instead that the features have a Gaussian distribution within each class, but that the covariance matrix may be different across classes. In symbols, if we assume that X | G = g has a N( mu, Sigma_g) distribution. The corresponding optimal classifier is now a quadratic function of the predictors (prove it!). The function qda in the MASS library implements this classifier, and it can be used just like lda (as usual, refer to its help page for details).\nThis approach can be used with any number of classes. Can you think of any limitations?"
  },
  {
    "objectID": "30-lda-logit.html#logistic-regression-review",
    "href": "30-lda-logit.html#logistic-regression-review",
    "title": "11  Parametric classifiers",
    "section": "11.2 Logistic regression (Review)",
    "text": "11.2 Logistic regression (Review)\nIf we model the distribution of the features within each class using a multivariate Gaussian distribution, then it is easy to see that the boundaries between classes are linear functions of the features (verify this!) Furthermore, the log of the odds ratio between classes is a linear function. It is interesting to note that one can start with this last assumption (instead of the full Gaussian model) and arrive at a fully parametric model for the conditional distibution of the classes given the features (see the class slides). The parameters can be estimated using maximum likelihood. For two classes this is the logistic regression model, which you may have seen in previous courses.\nWe illustrate this on the vaso data as before. Since this is a 2-class problem, we just need to fit a logistic regression model. The function glm in R does it for us, we specify that we want to fit such a model using the argument family=binomial. Once we obtain parameter estimators (in the glm object a below), we use the predict method to obtain predicted conditional probabilities on the same grid we used before:\n\na &lt;- glm(Y ~ ., data = vaso, family = binomial)\npr &lt;- predict(a, newdata = the.grid, type = \"response\")\n\nWe now plot the data and the surface of predicted probabilities for blue points (higher probabilites are displayed with lighter colors).\n\nimage(xrat, xvol, matrix(pr, 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", main = \"Logistic\"\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)"
  },
  {
    "objectID": "31-qda-knn.html#sensitivity-to-the-gaussian-assumption",
    "href": "31-qda-knn.html#sensitivity-to-the-gaussian-assumption",
    "title": "12  QDA",
    "section": "12.1 Sensitivity to the Gaussian assumption",
    "text": "12.1 Sensitivity to the Gaussian assumption\nWe discussed in class (with the help of a simple example) the sensitivity of QDA to the assumed specific conditional distribution of the features within each class. It is very easy to see that LDA may also be affected by similar problems. This is not at all surprising–in many cases optimal methods obtained under certain conditions are very sensitive to the vailidity of the assumptions used in their derivation.\nIt is interesting to note (as discussed in class) that logistic regression was not affected by the “good outliers” we included in the data. Considering where these “good outliers” are (in terms of their corresponding likelihood values), this is probably not surprising. Note, furthermore, that both QDA (and LDA) and logistic regression are classifiers that require the estimation of parameters (maybe we can call them parametric classifiers?), and in all cases considered so far the parameters were estimated using maximum likelihood. However their sensitivity to this kind of outliers is very different."
  },
  {
    "objectID": "31-qda-knn.html#more-than-2-classes-the-handwritten-digit-recognition-data",
    "href": "31-qda-knn.html#more-than-2-classes-the-handwritten-digit-recognition-data",
    "title": "12  QDA",
    "section": "12.2 More than 2 classes – The handwritten digit recognition data",
    "text": "12.2 More than 2 classes – The handwritten digit recognition data\nAs you may have noted, all the classification methods we have seen so far can be used in applications with an arbitrary number of classes. We will now illustrate them on the well-known Handwritten Digit Recognition Data (as usual, see help(zip.train, package='ElemStatLearn')). We first load the data, and extract the images corresponding to digits 0, 1 and 8. These should be challenging enough to discriminate given their similar shapes.\n\ndata(zip.train, package = \"ElemStatLearn\")\ndata(zip.test, package = \"ElemStatLearn\")\nx.tr &lt;- zip.train[zip.train[, 1] %in% c(0, 1, 8), ]\nx.te &lt;- zip.test[zip.test[, 1] %in% c(0, 1, 8), ]\n\nThe values of the pixes of each image are in the rows of the corresponding matrix (columns 2:256), and the true class of each image is in the first column. Note that there are relatively few 8’s in this training set:\n\ntable(x.tr[, 1])\n#&gt; \n#&gt;    0    1    8 \n#&gt; 1194 1005  542\n\nTo display these 16x16 images we adapt a simple function to plot matrices:\n\n# ----- Define a function for plotting a matrix ----- #\n# modified from: http://www.phaget4.org/R/image_matrix.html\nmyImagePlot &lt;- function(x) {\n  min &lt;- min(x)\n  max &lt;- max(x)\n  ColorRamp &lt;- grey(seq(1, 0, length = 256))\n  ColorLevels &lt;- seq(min, max, length = length(ColorRamp))\n  # Reverse Y axis\n  reverse &lt;- nrow(x):1\n  x &lt;- x[reverse, ]\n  image(1:ncol(x), 1:nrow(x), t(x),\n    col = ColorRamp, xlab = \"\",\n    ylab = \"\", axes = FALSE, zlim = c(min, max)\n  )\n}\n\nNext we choose 9 images at random from the training set, and display them in a 3x3 array of images:\n\na &lt;- x.tr\nset.seed(987)\nsa &lt;- sample(dim(a)[1], 9)\npar(mfrow = c(3, 3))\nfor (j in 1:9) {\n  myImagePlot(t(matrix(unlist(a[sa[j], -1]), 16, 16)))\n}\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nWe can also show the “average 8” in the training set:\n\nmyImagePlot(t(matrix(colMeans(subset(x.tr, subset = (x.tr[, 1] == 8), select = -1)), 16, 16)))\n\n\n\n\n\n\n\n# alternatively: myImagePlot(t(matrix(colMeans(a[a[,1]==8,-1]), 16, 16)))\n\nWe will now use LDA, QDA and a multinomial logistic model. The latter is the natural extension of logistic regression to more than 2 classes. You can easily derive it yourself by assuming that the response variable has a multinomial distribution and modeling each conditional probability as a (different) logistic function of the vector X of features. Note that if there are K classes you only need to model K-1 of these conditional class probabilities. The derivation is left as an easy exercise for you.\nNote that the data is stored in a matrix, but the use of lda(), qda(), etc. is clearer when you have your data in a data frame (as you can then refer to features by their names and use the data argument). So, we first transform our matrix into a data frame, and name the resulting variables V1, V2, …, V257:\n\nx.tr &lt;- data.frame(x.tr)\nx.te &lt;- data.frame(x.te)\nnames(x.te) &lt;- names(x.tr) &lt;- paste(\"V\", 1:257, sep = \"\")\n\nNow we use lda and multinom (this last one from package nnet) to train an LDA and a multinomial classifier to these 3-class data:\n\na &lt;- lda(V1 ~ . - V257, data = x.tr) # x.tr[,1] ~ x[, 2:256])\nlibrary(nnet)\na.log &lt;- multinom(V1 ~ . - V257, data = x.tr, maxit = 5000)\n#&gt; # weights:  771 (512 variable)\n#&gt; initial  value 3011.296283 \n#&gt; iter  10 value 27.327939\n#&gt; iter  20 value 8.491334\n#&gt; iter  30 value 2.640128\n#&gt; iter  40 value 1.228798\n#&gt; iter  50 value 0.663474\n#&gt; iter  60 value 0.391984\n#&gt; iter  70 value 0.212952\n#&gt; iter  80 value 0.114876\n#&gt; iter  90 value 0.053465\n#&gt; iter 100 value 0.026628\n#&gt; iter 110 value 0.014534\n#&gt; iter 120 value 0.009281\n#&gt; iter 130 value 0.006623\n#&gt; iter 140 value 0.004210\n#&gt; iter 150 value 0.002723\n#&gt; iter 160 value 0.001851\n#&gt; iter 170 value 0.001318\n#&gt; iter 180 value 0.001036\n#&gt; iter 190 value 0.000580\n#&gt; iter 200 value 0.000516\n#&gt; iter 210 value 0.000304\n#&gt; iter 220 value 0.000249\n#&gt; iter 230 value 0.000218\n#&gt; final  value 0.000090 \n#&gt; converged\n\n(Question: Why do I remove variable V257 from the models above?)\nAs a side commment: note how slow is the convergence of multinom. This is not unusual, and it has to do with how neural networks are trained. Refer to the corresponding help page for more information. We will probably discuss this further later in the course.\nFor now we obtain the predictions on the test set and build a matrix of classification errors for each classifier. For LDA we have:\n\npr.lda &lt;- predict(a, newdata = x.te)$class\ntable(pr.lda, x.te$V1)\n#&gt;       \n#&gt; pr.lda   0   1   8\n#&gt;      0 353   2   9\n#&gt;      1   0 258   0\n#&gt;      8   6   4 157\n\nFor the logistic multinomial classifier we have:\n\npr.log &lt;- predict(a.log, newdata = x.te)\ntable(pr.log, x.te$V1)\n#&gt;       \n#&gt; pr.log   0   1   8\n#&gt;      0 342   3  13\n#&gt;      1  12 258  10\n#&gt;      8   5   3 143\n\nWe now attempt to train a QDA classifier:\n\na.qda &lt;- try(qda(V1 ~ . - V257, data = x.tr))\n#&gt; Error in qda.default(x, grouping, ...) : rank deficiency in group 0\nclass(a.qda)\n#&gt; [1] \"try-error\"\n\nThis classifier cannot be trained on these data. The problem is that the training set for at least one class is rank deficient (which can be found by looking at the error message stored in the returned object a.qda\n\na.qda\n#&gt; [1] \"Error in qda.default(x, grouping, ...) : rank deficiency in group 0\\n\"\n#&gt; attr(,\"class\")\n#&gt; [1] \"try-error\"\n#&gt; attr(,\"condition\")\n#&gt; &lt;simpleError in qda.default(x, grouping, ...): rank deficiency in group 0&gt;\n\nIndeed, we have:\n\nx1 &lt;- x.tr[x.tr$V1 == 0, ]\ndim(x1)\n#&gt; [1] 1194  257\nqr(x1)$rank\n#&gt; [1] 254\n\nThe questions for you are:\n\nwhy is this rank deficiency a problem for QDA, but not for LDA, or a multinomial model?\ncan we do anything to train a (possibly different) QDA classifier to these data?"
  },
  {
    "objectID": "31-qda-knn.html#k-nearest-neighbours-k-nn",
    "href": "31-qda-knn.html#k-nearest-neighbours-k-nn",
    "title": "12  QDA",
    "section": "12.3 K-Nearest Neighbours (K-NN)",
    "text": "12.3 K-Nearest Neighbours (K-NN)\nPerhaps the intuitively simplest model-free estimator for conditional class probabilities for a given set of feature values X is the one based on nearest neighbours (as discussed in class). It is similar (in spirit) to the kernel regression estimator in the continuous-response regression setting. More specifically, it can be thought of as a variable-bandwidth kernel estimator. For a point X in the feature space we look at the proportion of observations in each class among X’s K-th closest neighbours. That is, of course, equivalent to looking at all points \\((Y_i, \\mathbf{X}_i)\\) in the training set such that \\(\\left\\| \\mathbf{X}_i - \\mathbf{X} \\right\\| \\le h_k\\), where \\(h_k\\) is the distance from X to the K-th closest neighbour in the training set. Refer to the discussion in class for more details.\nHere we will illustrate K-NN classifiers on the toy vaso example (to be able to visualize the results more easily), and also on the hand written digits data. We will use the function knn in package class. This function takes a training set, and also a test set (i.e. a different data set containing the observations to be predicted). In the example below we first create (as we have done before) a 200 x 200 grid of points and display the resulting predicted probabilities (or the corresponding class with highest conditional probability).\nWe first we use a trivial 1-NN classifier: the estimated conditional probabilities for each class at a point X, will simply be 0 or 1 depending on the class of the closest neighbour to X in the training set.\n\nlibrary(class)\ndata(vaso, package = \"robustbase\")\nx1 &lt;- seq(0, 4, length = 200)\nx2 &lt;- seq(0, 4, length = 200)\nxx &lt;- expand.grid(x1, x2)\nu1 &lt;- knn(train = vaso[, c(2, 1)], cl = vaso[, 3], test = xx, k = 1)\nu1 &lt;- as.numeric(u1)\nimage(x1, x2, matrix(u1, 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", main = \"1-NN\"\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\n\n\n\n\n\n\n\n\nWe repeat the analysis with a 5-NN classifier. Now the estimated conditional probabilities for each X in the grid can be 0, 0.20, 0.40, 0.60, 0.80 or 1 (why?) The function knn returns the estimated probabilities in the 'prob' attribute of the returned object, so we need to use the function attr to extract it (as usual, the R help pages are a good source of information if you have any questions about the code below):\n\nu5 &lt;- attr(\n  knn(train = vaso[, c(2, 1)], cl = vaso[, 3], test = xx, k = 5, prob = TRUE),\n  \"prob\"\n)\nimage(x1, x2, matrix(u5, 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", main = \"5-NN\"\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\n\n\n\n\n\n\n\n\nWe now turn to the digits data. We now look at the images for digits 1, 3 and 8 and create the corresponding training and test sets:\n\ndata(zip.train, package = \"ElemStatLearn\")\ndata(zip.test, package = \"ElemStatLearn\")\nx.tr &lt;- data.frame(zip.train[zip.train[, 1] %in% c(1, 3, 8), ])\nx.te &lt;- data.frame(zip.test[zip.test[, 1] %in% c(1, 3, 8), ])\nnames(x.te) &lt;- names(x.tr) &lt;- paste(\"V\", 1:257, sep = \"\")\n\nWe now train 1-, 5-, 10- and 50-NN classifiers and evaluate them on the test set. We report the misclassification rate on the test set, along with the corresponding tables:\n\nu1 &lt;- knn(train = x.tr[, -1], cl = x.tr[, 1], test = x.te[, -1], k = 1)\ntable(u1, x.te$V1)\n#&gt;    \n#&gt; u1    1   3   8\n#&gt;   1 261   0   0\n#&gt;   3   3 162   9\n#&gt;   8   0   4 157\nmean(u1 != x.te$V1)\n#&gt; [1] 0.02684564\n\nu5 &lt;- knn(train = x.tr[, -1], cl = x.tr[, 1], test = x.te[, -1], k = 5)\ntable(u5, x.te$V1)\n#&gt;    \n#&gt; u5    1   3   8\n#&gt;   1 261   1   0\n#&gt;   3   3 161   7\n#&gt;   8   0   4 159\nmean(u5 != x.te$V1)\n#&gt; [1] 0.02516779\n\nu10 &lt;- knn(train = x.tr[, -1], cl = x.tr[, 1], test = x.te[, -1], k = 10)\ntable(u10, x.te$V1)\n#&gt;    \n#&gt; u10   1   3   8\n#&gt;   1 261   1   3\n#&gt;   3   3 163  12\n#&gt;   8   0   2 151\nmean(u10 != x.te$V1)\n#&gt; [1] 0.0352349\n\nu50 &lt;- knn(train = x.tr[, -1], cl = x.tr[, 1], test = x.te[, -1], k = 50)\ntable(u50, x.te$V1)\n#&gt;    \n#&gt; u50   1   3   8\n#&gt;   1 261   2   7\n#&gt;   3   3 159  18\n#&gt;   8   0   5 141\nmean(u50 != x.te$V1)\n#&gt; [1] 0.05872483\n\nNote how the performance of the K-NN classifier in this case stops improving when K is larger than 5. Since the number K of nearest neighbours is in fact a tuning constant that needs to be chosen by the user, how would do it in an objective way? What would you do if you didn’t have a test set available?"
  },
  {
    "objectID": "31-qda-knn.html#challenges-for-k-nn-classifiers",
    "href": "31-qda-knn.html#challenges-for-k-nn-classifiers",
    "title": "12  QDA",
    "section": "12.4 Challenges for K-NN classifiers",
    "text": "12.4 Challenges for K-NN classifiers\n\nIt is easy to see that they suffer from the curse of dimensionality.\nFactor or binary features need to be treated with care.\nEuclidean distances do not reflect shape of features in each class (i.e. the conditional distribution of X in each class). Class-wise pre-standardization (whitening) might be useful.\n\nTo illustrate the last point, consider this toy synthetic example we discussed in class:\n\n# create example\nset.seed(123)\nx &lt;- matrix(runif(250 * 2, min = -1, max = 1), 250, 2)\nnorm2 &lt;- function(a) sqrt(sum(a^2))\nr &lt;- apply(x, 1, norm2)\na &lt;- (r &gt; .4) & (r &lt; .7)\nx &lt;- x[a, ]\n# plot(x, xlim=c(-1,1), ylim=c(-1,1))\nl1 &lt;- (x[, 1] &gt; 0)\nl2 &lt;- (x[, 2] &gt; 0)\na &lt;- l1 & !l2\nb &lt;- l1 & l2\nd &lt;- !l1 & l2\nla &lt;- rep(\"C\", nrow(x))\nla[a] &lt;- \"A\"\nla[b] &lt;- \"B\"\nla[d] &lt;- \"D\"\n# plot(x, pch=la)\nx2 &lt;- x\nx2[, 1] &lt;- x2[, 1] * 1e5\n\n\n# plot(x2, pch=la, cex=1.5)\n#\n# # pick a point\n# points(x2[26,1], x2[26, 2], pch='A', col='red', cex=1.9)\n\n# find closest neighbour\nx0 &lt;- x2[26, ]\nd &lt;- apply(scale(x2, center = x0, scale = FALSE), 1, norm2)\nh &lt;- sort(d)[2]\ne &lt;- (1:nrow(x2))[d == h]\nplot(x2, pch = la, cex = 1.5, xlab = expression(X[1]), ylab = expression(X[2]))\npoints(x2[26, 1], x2[26, 2], pch = \"A\", col = \"red\", cex = 1.9)\npoints(x2[e, 1], x2[e, 2], pch = \"O\", col = \"red\", cex = 1.9)\ntext(-5000, 0, labels = \"Closest neighbour\", cex = 1.5, col = \"red\")\narrows(x2[26, 1], x2[26, 2] + .1, x2[e, 1], x2[e, 2] - .1, lwd = 5, col = \"red\")\n\n\n\n\n\n\n\n\n# pdf('knn-challenge.pdf', bg='transparent')\n# plot(x2, pch=la, cex=1.5, col='gray30', xlab='', ylab='')\n# points(x2[26,1], x2[26, 2], pch='A', col='red', cex=1.9)\n# points(x2[e,1], x2[e, 2], pch=19, col='red', cex=3)\n# arrows(x2[26, 1], x2[26,2] + .15, x2[e,1], x2[e,2]-.15, lwd=7, col='red')\n# text(-5000, 0, labels='Closest neighbour', cex=1.5, col='red')\n# dev.off()"
  },
  {
    "objectID": "32-class-trees.html#pruning",
    "href": "32-class-trees.html#pruning",
    "title": "13  Classification Trees",
    "section": "13.1 Pruning",
    "text": "13.1 Pruning\nJust like regression trees, classification trees generally perform better if they are built by pruning an overfitting one. This is done in the same way as it is done for classification trees. When we do it on the graduate school admissions data we indeed obtain estimated conditional probabilities that appear to be more sensible (less “simple”):\n\nset.seed(123)\na.t &lt;- rpart(V3 ~ V1 + V2,\n  data = mm, method = \"class\", control = rpart.control(minsplit = 3, cp = 1e-8, xval = 10),\n  parms = list(split = \"information\")\n)\nb &lt;- a.t$cptable[which.min(a.t$cptable[, \"xerror\"]), \"CP\"]\na.t &lt;- prune(a.t, cp = b)\np.t &lt;- predict(a.t, newdata = dd, type = \"prob\")\nfilled.contour(aa, bb, matrix(p.t[, 1], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n\n\n\n\n\n\n\nfilled.contour(aa, bb, matrix(p.t[, 2], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n\n\n\n\n\n\n\nfilled.contour(aa, bb, matrix(p.t[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)"
  },
  {
    "objectID": "40-bagging.html#more-efficient-useful-and-elegant-implementation",
    "href": "40-bagging.html#more-efficient-useful-and-elegant-implementation",
    "title": "14  Bagging for regression",
    "section": "14.1 More efficient, useful and elegant implementation",
    "text": "14.1 More efficient, useful and elegant implementation\nI will now illustrate a possibly more efficient way to implement bagging, namely storing the \\(N\\) trees (rather than their predictions on a given data set). In this way one can re-use the ensemble (on any future data set) without having to re-train the elements of the bag. Since the idea is the same, I will just do it for ensemble of \\(N = 100\\) trees. To simplify the comparison between this implementation of bagging and the one used above, we first re-create the original training / test split\n\nset.seed(123456)\nn &lt;- nrow(Boston)\nii &lt;- sample(n, floor(n / 4))\ndat.te &lt;- Boston[ii, ]\ndat.tr &lt;- Boston[-ii, ]\n\nNow, let’s create a list of 100 (empty) elements, each element of this list will store a regression tree:\n\nN &lt;- 100\nmybag &lt;- vector(\"list\", N)\n\nNow, we train the \\(N\\) trees as before, but store them in the list (without computing any predictions):\n\nset.seed(123)\nfor (j in 1:N) {\n  ii &lt;- sample(n.tr, replace = TRUE)\n  mybag[[j]] &lt;- rpart(medv ~ ., data = dat.tr[ii, ], method = \"anova\", control = con)\n}\n\nGiven a new data set, in order to obtain the corresponding predictions for each tree in the ensemble, one could either:\n\nloop over the \\(N\\) trees, averaging the corresponding \\(N\\) vectors of predictions; or\nuse sapply (check the help page if you are not familiar with the apply functions in R).\n\nThe later option results in code that is much more elegant, efficient (allowing for future uses of the ensemble), and compact. Of course both give exactly the same results. Below we illustrate both strategies. If we use the first approach (loop) we obtain the following estimated MSPE using the test set:\n\npr.bagg2 &lt;- rep(0, nrow(dat.te))\nfor (j in 1:N) {\n  pr.bagg2 &lt;- pr.bagg2 + predict(mybag[[j]], newdata = dat.te) / N\n}\nwith(dat.te, mean((medv - pr.bagg2)^2))\n#&gt; [1] 12.10982\n\n(compare it with the results we obtained before). Using the second approach (sapply):\n\npr.bagg3 &lt;- rowMeans(sapply(mybag, predict, newdata = dat.te))\nwith(dat.te, mean((medv - pr.bagg3)^2))\n#&gt; [1] 12.10982\n\nBoth results are of course identical."
  },
  {
    "objectID": "40-bagging.html#bagging-a-regression-spline",
    "href": "40-bagging.html#bagging-a-regression-spline",
    "title": "14  Bagging for regression",
    "section": "14.2 Bagging a regression spline",
    "text": "14.2 Bagging a regression spline\nBagging does not provide much of an advantage when applied to linear predictors (can you explain why?) Nevertheless, let us try it on the lidar data, which, as we did before, we randomly split into a training and test set:\n\ndata(lidar, package = \"SemiPar\")\nset.seed(123456)\nn &lt;- nrow(lidar)\nii &lt;- sample(n, floor(n / 5))\nlid.te &lt;- lidar[ii, ]\nlid.tr &lt;- lidar[-ii, ]\n\nNow fit a cubic spline, and estimate the MSPE using the test set:\n\nlibrary(splines)\na &lt;- lm(logratio ~ bs(x = range, df = 10, degree = 3), data = lid.tr)\noo &lt;- order(lid.tr$range)\npr.of &lt;- predict(a, newdata = lid.te)\nmean((lid.te$logratio - pr.of)^2)\n#&gt; [1] 0.007427559\n\nWe build an ensemble of 10 fits and estimate the corresponding MSPE using the test set:\n\nN &lt;- 10\nmyps &lt;- matrix(NA, nrow(lid.te), N)\nset.seed(123)\nn.tr &lt;- nrow(lid.tr)\nfor (i in 1:N) {\n  ii &lt;- sample(n.tr, replace = TRUE)\n  a.b &lt;- lm(logratio ~ bs(x = range, df = 10, degree = 3), data = lid.tr[ii, ])\n  myps[, i] &lt;- predict(a.b, newdata = lid.te)\n}\npr.ba &lt;- rowMeans(myps) # , na.rm=TRUE)\nmean((lid.te$logratio - pr.ba)^2)\n#&gt; [1] 0.007552562\n\nNote that the estimated MSPE is almost the same as the one of the original single spline. Furthermore, adding more elements to the ensemble does not seem to improve the estimated MSPEs:\n\nN &lt;- 100\nmyps &lt;- matrix(NA, nrow(lid.te), N)\nset.seed(123)\nn.tr &lt;- nrow(lid.tr)\nfor (i in 1:N) {\n  ii &lt;- sample(n.tr, replace = TRUE)\n  a.b &lt;- lm(logratio ~ bs(x = range, df = 10, degree = 3), data = lid.tr[ii, ])\n  myps[, i] &lt;- predict(a.b, newdata = lid.te)\n}\npr.ba &lt;- rowMeans(myps) # , na.rm=TRUE)\nmean((lid.te$logratio - pr.ba)^2)\n#&gt; [1] 0.0075887"
  },
  {
    "objectID": "41-bagging-classifiers.html#instability-of-trees-motivation",
    "href": "41-bagging-classifiers.html#instability-of-trees-motivation",
    "title": "15  Bagging for classification",
    "section": "15.1 Instability of trees (motivation)",
    "text": "15.1 Instability of trees (motivation)\nJust like in the regression case, classification trees can be highly unstable (specifically: relatively small changes in the training set may result in comparably large changes in the corresponding tree). We illustrate the problem on the very simple graduate school admissions example (3-class 2-dimensional covariates) we used in class. First read the data:\n\nmm &lt;- read.table(\"data/T11-6.DAT\", header = FALSE)\n\nWe transform the response variable V3 into a factor (which is how class labels are represented in R, and what rpart() expects as the values in the response variable to build a classifier):\n\nmm$V3 &lt;- as.factor(mm$V3)\n\nTo obtain better looking plots later, we now re-scale one of the features (so both explanatory variables have similar ranges):\n\nmm[, 2] &lt;- mm[, 2] / 150\n\nWe use the function rpart to train a classification tree on these data, using deviance-based (information) splits:\n\nlibrary(rpart)\na.t &lt;- rpart(V3 ~ V1 + V2, data = mm, method = \"class\", parms = list(split = \"information\"))\n\nTo illustrate the instability of this tree (i.e. how the tree changes when the data are perturbed slightly), we create a new training set (mm2) that is identical to the original one (in mm), except for two observations where we change their responses from class 1 to class 2:\n\nmm2 &lt;- mm\nmm2[1, 3] &lt;- 2\nmm2[7, 3] &lt;- 2\n\nThe following plot contains the new training set, with the two changed observations (you can find them around the point (GPA, GMAT) = (3, 4)) highlighted with a blue dot (their new class) and a red ring around them (their old class was “red”):\n\nplot(mm2[, 1:2],\n  pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]],\n  xlab = \"GPA\", \"GMAT\", xlim = c(2, 5), ylim = c(2, 5)\n)\npoints(mm[c(1, 7), -3], pch = \"O\", cex = 1.1, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n\n\n\n\n\n\n\n\nAs we did above, we now train a classification tree on the perturbed data in mm2:\n\na2.t &lt;- rpart(V3 ~ V1 + V2, data = mm2, method = \"class\", parms = list(split = \"information\"))\n\nTo visualize the differences between the two trees we build a fine grid of points and compare the predicted probabilities of each class on each point on the grid. First, construct the grid:\n\naa &lt;- seq(2, 5, length = 200)\nbb &lt;- seq(2, 5, length = 200)\ndd &lt;- expand.grid(aa, bb)\nnames(dd) &lt;- names(mm)[1:2]\n\nNow, compute the estimated conditional probabilities of each of the 3 classes on each of the 40,000 points on the grid dd:\n\np.t &lt;- predict(a.t, newdata = dd, type = \"prob\")\np2.t &lt;- predict(a2.t, newdata = dd, type = \"prob\")\n\nThe next figures show the estimated probabilities of class “red” with each of the two trees:\n\nfilled.contour(aa, bb, matrix(p.t[, 1], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n\n\n\n\n\n\n\n\nfilled.contour(aa, bb, matrix(p2.t[, 1], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm2[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]])\n    points(mm[c(1, 7), -3], pch = \"O\", cex = 1.1, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n  }\n)\n\n\n\n\n\n\n\n\nSimilarly, the estimated conditional probabilities for class “blue” at each point of the grid are:\n\n# blues\nfilled.contour(aa, bb, matrix(p.t[, 2], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n\n\n\n\n\n\n\n\nfilled.contour(aa, bb, matrix(p2.t[, 2], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  pane.last = {\n    points(mm2[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]])\n    points(mm[c(1, 7), -3], pch = \"O\", cex = 1.1, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n  }\n)\n\n\n\n\n\n\n\n\nAnd finally, for class “green”:\n\n# greens\nfilled.contour(aa, bb, matrix(p.t[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  }, panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n\n\n\n\n\n\n\n\nfilled.contour(aa, bb, matrix(p2.t[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  pane.last = {\n    points(mm2[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]])\n    points(mm[c(1, 7), -3], pch = \"O\", cex = 1.1, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n  }\n)\n\n\n\n\n\n\n\n\nNote that, for example, the regions of the feature space (the explanatory variables) that would be classified as “red” or “green” for the trees trained with the original and the slightly changed training sets change quite noticeably, even though the difference in the training sets is relatively small. Below we show how an ensemble of classifiers constructed via bagging can provide a more stable classifier."
  },
  {
    "objectID": "41-bagging-classifiers.html#bagging-trees",
    "href": "41-bagging-classifiers.html#bagging-trees",
    "title": "15  Bagging for classification",
    "section": "15.2 Bagging trees",
    "text": "15.2 Bagging trees\nJust as we did for regression, bagging consists of building an ensemble of predictors (in this case, classifiers) using bootstrap samples. If we using B bootstrap samples, we will construct B classifiers, and given a point x, we now have B estimated conditional probabilities for each of the possible K classes. Unlike what happens with regression problems, we now have a choice to make when deciding how to combine the B outputs for each point. We can take either: a majority vote over the B separate decisions, or we can average the B estimated probabilities for the K classes, to obtain bagged estimated conditional probabilities. As discussed and illustrated in class, the latter approach is usually preferred.\nTo illustrate the increased stability of bagged classification trees, we repeat the experiment above: we build an ensemble of 1000 classification trees trained on the original data, and a second ensemble (also of 1000 trees) using the slightly modified data. Each ensemble is constructed in exactly the same way we did in the regression case. For the first ensemble we train NB = 1000 trees and store them in a list (called ts) for future use:\n\nmy.c &lt;- rpart.control(minsplit = 3, cp = 1e-6, xval = 10)\nNB &lt;- 1000\nts &lt;- vector(\"list\", NB)\nset.seed(123)\nn &lt;- nrow(mm)\nfor (j in 1:NB) {\n  ii &lt;- sample(1:n, replace = TRUE)\n  ts[[j]] &lt;- rpart(V3 ~ V1 + V2, data = mm[ii, ], method = \"class\", parms = list(split = \"information\"), control = my.c)\n}\n\n\n15.2.1 Using the ensemble\nAs discussed in class, there are two possible ways to use this ensemble given a new observation: we can classify it to the class with most votes among the B bagged classifiers, or we can compute the average conditional probabilities over the B classifiers, and use this average as our esimated conditional probability. We illustrate both of these with the point (GPA, GMAT) = (3.3, 3.0).\n\n15.2.1.1 Majority vote\nThe simplest, but less elegant way to compute the votes for each class across the B trees in the ensemble is to loop over them and count:\n\nx0 &lt;- t(c(V1 = 3.3, V2 = 3.0))\nvotes &lt;- vector(\"numeric\", 3)\nnames(votes) &lt;- 1:3\nfor (j in 1:NB) {\n  k &lt;- predict(ts[[j]], newdata = data.frame(x0), type = \"class\")\n  votes[k] &lt;- votes[k] + 1\n}\n(votes)\n#&gt;   1   2   3 \n#&gt; 909   0  91\n\nAnd we see that the class most voted is 1.\nThe above calculation can be made more elegantly with the function sapply (or lapply):\n\nvotes2 &lt;- sapply(ts, FUN = function(a, newx) predict(a, newdata = newx, type = \"class\"), newx = data.frame(x0))\ntable(votes2)\n#&gt; votes2\n#&gt;   1   2   3 \n#&gt; 909   0  91\n\n\n\n15.2.1.2 Average probabilities (over the ensemble)\nIf we wanted to compute the average of the conditional probabilities across the B different estimates, we could do it in a very similar way. Here I show how to do it using sapply. You are strongly encouraged to verify these calculations by computing the average of the conditional probabilities using a for-loop.\n\nvotes2 &lt;- sapply(ts, FUN = function(a, newx) predict(a, newdata = newx, type = \"prob\"), newx = data.frame(x0))\n(rowMeans(votes2))\n#&gt; [1] 0.90881555 0.00000000 0.09118445\n\nAnd again, we see that class 1 has a much higher probability of occuring for this point.\n\n\n\n15.2.2 Increased stability of ensembles\nTo illustrate that ensembles of tree-based classifiers tend to be more stable than a single tree, we construct another example, but this time using the slightly modified data. The ensemble is stored in the list ts2:\n\nmm2 &lt;- mm\nmm2[1, 3] &lt;- 2\nmm2[7, 3] &lt;- 2\nNB &lt;- 1000\nts2 &lt;- vector(\"list\", NB)\nset.seed(123)\nn &lt;- nrow(mm)\nfor (j in 1:NB) {\n  ii &lt;- sample(1:n, replace = TRUE)\n  ts2[[j]] &lt;- rpart(V3 ~ V1 + V2, data = mm2[ii, ], method = \"class\", parms = list(split = \"information\"), control = my.c)\n}\n\nWe use the same fine grid as before to show the estimated conditional probabilities, this time obtained with the two ensembles.\n\naa &lt;- seq(2, 5, length = 200)\nbb &lt;- seq(2, 5, length = 200)\ndd &lt;- expand.grid(aa, bb)\nnames(dd) &lt;- names(mm)[1:2]\n\nTo combine (average) the NB = 1000 estimated probabilities of each of the 3 classes for each of the 40,000 points in the grid dd I use the function vapply and store the result in a 3-dimensional array. The averaged probabilities over the 1000 bagged trees can then obtained by averaging across the 3rd dimension. This approach may not be intuitively very clear at first sight. You are strongly encouraged to ignore my code below and compute the bagged conditional probabilites for the 3 classes for each point in the grid in a way that is clear to you. The main goal is to understand the method and be able to do it on your own. Efficient and / or elegant code can be written later, but it is not the focus of this course. The ensemble of trees trained with the original data:\n\npp0 &lt;- vapply(ts, FUN = predict, FUN.VALUE = matrix(0, 200 * 200, 3), newdata = dd, type = \"prob\")\npp &lt;- apply(pp0, c(1, 2), mean)\n\nAnd the ensemble of trees trained with the slightly modified data:\n\npp02 &lt;- vapply(ts2, FUN = predict, FUN.VALUE = matrix(0, 200 * 200, 3), newdata = dd, type = \"prob\")\npp2 &lt;- apply(pp02, c(1, 2), mean)\n\nThe plots below show the estimated conditional probabilities for class “red” in each point of the grid, with each of the two ensembles. Note how similar they are (and contrast this with the results obtained before without bagging):\n\nfilled.contour(aa, bb, matrix(pp[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n\n\n\n\n\n\n\nfilled.contour(aa, bb, matrix(pp2[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm2[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]])\n    points(mm[c(1, 7), -3], pch = \"O\", cex = 1.2, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n  }\n)\n\n\n\n\n\n\n\n\nYou are strongly encouraged to obtain the corresponding plots comparing the estimated conditional probabilities with both ensembles for each of the other 2 classes (“blue” and “green”)."
  },
  {
    "objectID": "42-random-forests.html#another-example",
    "href": "42-random-forests.html#another-example",
    "title": "16  Random Forests",
    "section": "16.1 Another example",
    "text": "16.1 Another example\nWe will now use a more interesting example. The ISOLET data, available here: http://archive.ics.uci.edu/ml/datasets/ISOLET, contains data on sound recordings of 150 speakers saying each letter of the alphabet (twice). See the original source for more details. Since the full data set is rather large, here we only use the subset corresponding to the observations for the letters C and Z.\nWe first load the training and test data sets, and force the response variable to be categorical, so that the R implementations of the different predictors we will use below will build classifiers and not their regression counterparts:\n\nxtr &lt;- read.table(\"data/isolet-train-c-z.data\", sep = \",\")\nxte &lt;- read.table(\"data/isolet-test-c-z.data\", sep = \",\")\nxtr$V618 &lt;- as.factor(xtr$V618)\nxte$V618 &lt;- as.factor(xte$V618)\n\nTo train a Random Forest we use the function randomForest in the package of the same name. The code underlying this package was originally written by Leo Breiman. We first train a Random Forest, using all the default parameters\n\nlibrary(randomForest)\nset.seed(123)\n(a.rf &lt;- randomForest(V618 ~ ., data = xtr, ntree = 500))\n#&gt; \n#&gt; Call:\n#&gt;  randomForest(formula = V618 ~ ., data = xtr, ntree = 500) \n#&gt;                Type of random forest: classification\n#&gt;                      Number of trees: 500\n#&gt; No. of variables tried at each split: 24\n#&gt; \n#&gt;         OOB estimate of  error rate: 2.29%\n#&gt; Confusion matrix:\n#&gt;      3  26 class.error\n#&gt; 3  234   6  0.02500000\n#&gt; 26   5 235  0.02083333\n\nWe now check its performance on the test set:\n\np.rf &lt;- predict(a.rf, newdata = xte, type = \"response\")\ntable(p.rf, xte$V618)\n#&gt;     \n#&gt; p.rf  3 26\n#&gt;   3  60  1\n#&gt;   26  0 59\n\nNote that the Random Forest only makes one mistake out of 120 (approx 0.8%) observations in the test set. However, the OOB error rate estimate is slightly over 2%. The next plot shows the evolution of the OOB error rate estimate as a function of the number of classifiers in the ensemble (trees in the forest). Note that 500 trees appears to be a reasonable forest size, in the sense thate the OOB error rate estimate is stable.\n\nplot(a.rf, lwd = 3, lty = 1)\n\n\n\n\n\n\n\n\nConsider again the ISOLET data, available here: http://archive.ics.uci.edu/ml/datasets/ISOLET. Here we only use a subset corresponding to the observations for the letters C and Z.\nWe first load the training and test data sets, and force the response variable to be categorical, so that the R implementations of the different predictors we will use below will build classifiers and not their regression counterparts:\n\nxtr &lt;- read.table(\"data/isolet-train-c-z.data\", sep = \",\")\nxte &lt;- read.table(\"data/isolet-test-c-z.data\", sep = \",\")\nxtr$V618 &lt;- as.factor(xtr$V618)\nxte$V618 &lt;- as.factor(xte$V618)\n\nTo train a Random Forest we use the function randomForest in the package of the same name. The code underlying this package was originally written by Leo Breiman. We train a RF leaving all paramaters at their default values, and check its performance on the test set:\n\nlibrary(randomForest)\nset.seed(123)\na.rf &lt;- randomForest(V618 ~ ., data = xtr, ntree = 500)\np.rf &lt;- predict(a.rf, newdata = xte, type = \"response\")\ntable(p.rf, xte$V618)\n#&gt;     \n#&gt; p.rf  3 26\n#&gt;   3  60  1\n#&gt;   26  0 59\n\nNote that the Random Forest only makes one mistake out of 120 observations in the test set. The OOB error rate estimate is slightly over 2%, and we see that 500 trees is a reasonable forest size:\n\nplot(a.rf, lwd = 3, lty = 1)\n\n\n\n\n\n\n\na.rf\n#&gt; \n#&gt; Call:\n#&gt;  randomForest(formula = V618 ~ ., data = xtr, ntree = 500) \n#&gt;                Type of random forest: classification\n#&gt;                      Number of trees: 500\n#&gt; No. of variables tried at each split: 24\n#&gt; \n#&gt;         OOB estimate of  error rate: 2.29%\n#&gt; Confusion matrix:\n#&gt;      3  26 class.error\n#&gt; 3  234   6  0.02500000\n#&gt; 26   5 235  0.02083333"
  },
  {
    "objectID": "42-random-forests.html#using-a-test-set-instead-of-obb",
    "href": "42-random-forests.html#using-a-test-set-instead-of-obb",
    "title": "16  Random Forests",
    "section": "16.2 Using a test set instead of OBB",
    "text": "16.2 Using a test set instead of OBB\nGiven that in this case we do have a test set, we can use it to monitor the error rate (instead of using the OOB error estimates):\n\nx.train &lt;- model.matrix(V618 ~ ., data = xtr)\ny.train &lt;- xtr$V618\nx.test &lt;- model.matrix(V618 ~ ., data = xte)\ny.test &lt;- xte$V618\nset.seed(123)\na.rf &lt;- randomForest(x = x.train, y = y.train, xtest = x.test, ytest = y.test, ntree = 500)\ntest.err &lt;- a.rf$test$err.rate\nma &lt;- max(c(test.err))\nplot(test.err[, 2], lwd = 2, lty = 1, col = \"red\", type = \"l\", ylim = c(0, max(c(0, ma))))\nlines(test.err[, 3], lwd = 2, lty = 1, col = \"green\")\nlines(test.err[, 1], lwd = 2, lty = 1, col = \"black\")\n\n\n\n\n\n\n\n\nAccording to the help page for the plot method for objects of class randomForest, the following plot should show both error rates (OOB plus those on the test set):\n\nplot(a.rf, lwd = 2)"
  },
  {
    "objectID": "42-random-forests.html#feature-sequencing-variable-ranking",
    "href": "42-random-forests.html#feature-sequencing-variable-ranking",
    "title": "16  Random Forests",
    "section": "16.3 Feature sequencing / Variable ranking",
    "text": "16.3 Feature sequencing / Variable ranking\nTo explore which variables were used in the forest, and also, their importance rank as discussed in class, we can use the function varImpPlot:\n\nvarImpPlot(a.rf, n.var = 20)"
  },
  {
    "objectID": "42-random-forests.html#comparing-rf-with-other-classifiers",
    "href": "42-random-forests.html#comparing-rf-with-other-classifiers",
    "title": "16  Random Forests",
    "section": "16.4 Comparing RF with other classifiers",
    "text": "16.4 Comparing RF with other classifiers\nWe now compare the Random Forest with some of the other classifiers we saw in class, using their classification error rate on the test set as our comparison measure. We first start with K-NN:\n\nlibrary(class)\nu1 &lt;- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 1)\ntable(u1, xte$V618)\n#&gt;     \n#&gt; u1    3 26\n#&gt;   3  57  9\n#&gt;   26  3 51\n\nu5 &lt;- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 5)\ntable(u5, xte$V618)\n#&gt;     \n#&gt; u5    3 26\n#&gt;   3  58  5\n#&gt;   26  2 55\n\nu10 &lt;- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 10)\ntable(u10, xte$V618)\n#&gt;     \n#&gt; u10   3 26\n#&gt;   3  58  6\n#&gt;   26  2 54\n\nu20 &lt;- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 20)\ntable(u20, xte$V618)\n#&gt;     \n#&gt; u20   3 26\n#&gt;   3  58  5\n#&gt;   26  2 55\n\nu50 &lt;- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 50)\ntable(u50, xte$V618)\n#&gt;     \n#&gt; u50   3 26\n#&gt;   3  58  7\n#&gt;   26  2 53\n\nTo use logistic regression we first create a new variable that is 1 for the letter C and 0 for the letter Z, and use it as our response variable.\n\nxtr$V619 &lt;- as.numeric(xtr$V618 == 3)\nd.glm &lt;- glm(V619 ~ . - V618, data = xtr, family = binomial)\npr.glm &lt;- as.numeric(predict(d.glm, newdata = xte, type = \"response\") &gt; 0.5)\ntable(pr.glm, xte$V618)\n#&gt;       \n#&gt; pr.glm  3 26\n#&gt;      0 25 33\n#&gt;      1 35 27\n\nQuestion for the reader: why do you think this classifier’s performance is so disappointing?\nIt is interesting to see how a simple LDA classifier does:\n\nlibrary(MASS)\nxtr$V619 &lt;- NULL\nd.lda &lt;- lda(V618 ~ ., data = xtr)\npr.lda &lt;- predict(d.lda, newdata = xte)$class\ntable(pr.lda, xte$V618)\n#&gt;       \n#&gt; pr.lda  3 26\n#&gt;     3  58  3\n#&gt;     26  2 57\n\nFinally, note that a carefully built classification tree performs remarkably well, only using 3 features:\n\nlibrary(rpart)\nmy.c &lt;- rpart.control(minsplit = 5, cp = 1e-8, xval = 10)\nset.seed(987)\na.tree &lt;- rpart(V618 ~ ., data = xtr, method = \"class\", parms = list(split = \"information\"), control = my.c)\ncp &lt;- a.tree$cptable[which.min(a.tree$cptable[, \"xerror\"]), \"CP\"]\na.tp &lt;- prune(a.tree, cp = cp)\np.t &lt;- predict(a.tp, newdata = xte, type = \"vector\")\ntable(p.t, xte$V618)\n#&gt;    \n#&gt; p.t  3 26\n#&gt;   1 59  0\n#&gt;   2  1 60\n\nFinally, note that if you train a single classification tree with the default values for the stopping criterion tuning parameters, the tree also uses only 3 features, but its classification error rate on the test set is larger than that of the pruned one:\n\nset.seed(987)\na2.tree &lt;- rpart(V618 ~ ., data = xtr, method = \"class\", parms = list(split = \"information\"))\np2.t &lt;- predict(a2.tree, newdata = xte, type = \"vector\")\ntable(p2.t, xte$V618)\n#&gt;     \n#&gt; p2.t  3 26\n#&gt;    1 57  2\n#&gt;    2  3 58"
  },
  {
    "objectID": "43-boosting.html#a-different-kind-of-ensembles",
    "href": "43-boosting.html#a-different-kind-of-ensembles",
    "title": "17  Boosting (a Statistical Learning perspective)",
    "section": "17.1 A different kind of ensembles",
    "text": "17.1 A different kind of ensembles\nSo far in this course we have seen ensembles of classifiers (or regression estimators) based on the idea of bagging: combininig the predictions of a number of predictors trained on bootstrap samples taken from the original training set. By construction all the predictors in the ensemble are treated equally (e.g.  their predictions receive the same weight when they are combined). Another characteristic of these ensembles is the predictors in them could be trained in parallel (they are independent from each other).\nBoosting algorithms go back to the late 90s. One of the first ones to appear in the Machine Learning literature is probably Adaboost.M1 introduced in\n\nFreund, Y. and Schapire, R. (1997). A decision-theoretic generalization of online learning and an application to boosting, Journal of Computer and System Sciences, 55:119-139.\n\nWe discussed the specifics of the algorithm in class. An important difference with the other ensembles we discussed in class (can you name them?) is that for Adaboost.M1 the elements of the ensemble are trained sequentially in such a way that to compute the i-th predictor \\(T_i\\) we need to have the previous one \\(T_{i-1}\\) available. Furthemore, the weights in the final combination of predictions are generally different for each member of the ensemble.\nHere we will use the implementation available in the adabag package, specifically the function boosting. This function can be rather slow, but it is a straight implementation of the Adaboost algorithm, and it returns many useful objects (e.g. each of the individual weak lerners, etc.) As usual, I suggest that you invest a few minutes reading the help pages and also exploring the returned objects by hand.\nNote that Adaboost was originally proposed for 2-class problems. To illustrate its use, we look at the zip code digits example. We consider the problem of building a classifier to determine whether an image is a 1 or a 9. We use 1-split classification trees as our weak lerners in the ensemble. Since boosting uses the rpart implementation of classification and regression trees, we use the function rpart.control to specify the type of weak lerners we want.\nWe first load the full training set, and extract the 7’s and 9’s. Since the original data file does not have feature names, we create them as “V1”, “V2”, etc.\n\ndata(zip.train, package = \"ElemStatLearn\")\nx.tr &lt;- data.frame(zip.train)\nnames(x.tr) &lt;- paste(\"V\", 1:257, sep = \"\")\nx.tr &lt;- x.tr[x.tr$V1 %in% c(1, 9), ]\n\nTo force rpart (and thus boosting) to train a classification ensemble (as opposed to a regression one) we force the response variable to be categorical.\n\nx.tr$V1 &lt;- as.factor(x.tr$V1)\n\nNow we load the adabag package, use rpart.control to force it to use 1- or 2-split trees, and train the boosting ensemble:\n\nlibrary(adabag)\nonesplit &lt;- rpart.control(cp = -1, maxdepth = 1, minsplit = 0, xval = 0)\nbo1 &lt;- boosting(V1 ~ ., data = x.tr, boos = FALSE, mfinal = 500, control = onesplit)\n\nWe can explore the evolution of the error rate on the training set (the equivalent of the MSE for classifiers) using the function errorevol:\n\nplot(errorevol(bo1, newdata = x.tr))\n\n\n\n\n\n\n\n\nNote that after approximately 10 iterations the error rate on the training set drops to zero and stays there. A few questions for you:\n\nHas the algorithm converged after approximately 10 iterations?\nAre the predictors trained after the (approximately) 10th iteration irrelevant?\n\nAs we know pretty well by now, a more reliable measure of the expected performance of the ensemble can be obtained using a test set (or cross-validation) (what about OOB?)\nFirst load the full test set, extract the cases corresponding to the digits we are using here, and check the performance of the predictor, including the plot of the error rate as a function of the number of elements in the ensemble:\n\ndata(zip.test, package = \"ElemStatLearn\")\nx.te &lt;- data.frame(zip.test)\nnames(x.te) &lt;- paste(\"V\", 1:257, sep = \"\")\nx.te &lt;- x.te[x.te$V1 %in% c(1, 9), ]\nx.te$V1 &lt;- as.factor(x.te$V1)\ntable(x.te$V1, predict(bo1, newdata = x.te)$class)\n#&gt;    \n#&gt;       1   9\n#&gt;   1 260   4\n#&gt;   9   1 176\nplot(errorevol(bo1, newdata = x.te))\n\n\n\n\n\n\n\n\nJust to make sure boosting is doing a good job, we compare it with another ensemble classifier: a Random Forest. We use the same number of elements in both ensembles (500), even though their complexity is very different – while boosting used stumps (1-split trees), the random forest trees are (purposedly) very large (deep).\nWe first train the random forest and look at the error rates as displayed by the plot method for objects of class randomForest:\n\nset.seed(987)\nlibrary(randomForest)\na &lt;- randomForest(V1 ~ ., data = x.tr) # , ntree=500)\nplot(a)\n\n\n\n\n\n\n\n\nNow we evaluate the performance of the Random Forest on the training set by obtaining fitted values (“predictions” for the observations in the training set) and looking at the corresponding “confusion table”:\n\ntable(x.tr$V1, predict(a, newdata = x.tr, type = \"response\"))\n#&gt;    \n#&gt;        1    9\n#&gt;   1 1005    0\n#&gt;   9    0  644\n\nAn interesting question to ask yourself at this point is: Does this “confusion table” match the information from the error plot above? Can you describe (and explain!) the apparent problem?\nAs we all know too well, of course, the classification error rate on the test set is a better measure of predicition performance:\n\npr.rf &lt;- predict(a, newdata = x.te, type = \"response\")\ntable(x.te$V1, pr.rf)\n#&gt;    pr.rf\n#&gt;       1   9\n#&gt;   1 259   5\n#&gt;   9   2 175\n\nWe see that in this case the random forest does marginally worse than the boosting ensemble, even though the ensemble elements using in boosting are extremely simple trees.\n\n17.1.1 Another example\nConsider the ISOLET data introduced earlier. Here we will consider building a classifier to discriminate between the letters A and H based on the features extracted from their sound recordings. The steps of the analysis are the same as before:\nFirst we load the training set\n\nxtr &lt;- read.table(\"data/isolet-train-a-h.data\", sep = \",\", header = TRUE)\n\nNext, we force the response to be a categorical variable:\n\nxtr$V618 &lt;- as.factor(xtr$V618)\n\nNow train a boosting ensamble and evaluate it on the test set (which needs to be loaded as well):\n\nonesplit &lt;- rpart.control(cp = -1, maxdepth = 1, minsplit = 0, xval = 0)\nbo1 &lt;- boosting(V618 ~ ., data = xtr, boos = FALSE, mfinal = 200, control = onesplit)\nxte &lt;- read.table(\"data/isolet-test-a-h.data\", sep = \",\", header = TRUE)\nxte$V618 &lt;- as.factor(xte$V618)\ntable(xte$V618, predict(bo1, newdata = xte)$class)\n#&gt;    \n#&gt;      1  8\n#&gt;   1 59  1\n#&gt;   8  0 60\n\nWe can also look at the error evolution on the test set to decide whether a smaller ensemble would be satisfactory:\n\nplot(errorevol(bo1, newdata = xte))\n\n\n\n\n\n\n\n\nFinally, we compare these results with those obtained with a Random Forest:\n\nset.seed(123)\na.rf &lt;- randomForest(V618 ~ ., data = xtr, ntree = 200)\nplot(a.rf)\n\n\n\n\n\n\n\np.rf &lt;- predict(a.rf, newdata = xte, type = \"response\")\ntable(xte$V618, p.rf)\n#&gt;    p.rf\n#&gt;      1  8\n#&gt;   1 58  2\n#&gt;   8  0 60"
  },
  {
    "objectID": "44-adaboost.html",
    "href": "44-adaboost.html",
    "title": "18  What is Adaboost doing, really?",
    "section": "",
    "text": "Following the work of (Friedman, Hastie, and Tibshirani 2000) (see also Chapter 10 of [ESL]), we saw in class that Adaboost can be interpreted as fitting an additive model in a stepwise (greedy) way, using an exponential loss. It is then easy to prove that Adaboost.M1 is computing an approximation to the optimal classifier G( x ) = log[ P( Y = 1 | X = x ) / P( Y = -1 | X = x ) ] / 2, where optimal here is taken with respect to the exponential loss function. More specifically, Adaboost.M1 is using an additive model to approximate that function. In other words, Boosting is attempting to find functions \\(f_1\\), \\(f_2\\), …, \\(f_N\\) such that \\(G(x) = \\sum_i f_i( x^{(i)} )\\), where \\(x^{(i)}\\) is a sub-vector of \\(x\\) (i.e. the function \\(f_i\\) only depends on some of the available features, typically a few of them: 1 or 2, say). Note that each \\(f_i\\) generally depends on a different subset of features than the other \\(f_j\\)’s.\nKnowing the function the boosting algorithm is approximating (even if it does it in a greedy and suboptimal way), allows us to understand when the algorithm is expected to work well, and also when it may not work well. In particular, it provides one way to choose the complexity of the weak lerners used to construct the ensemble. For an example you can refer to the corresponding lab activity.\n\n18.0.1 A more challenging example, the email spam data\nThe email spam data set is a relatively classic data set containing 57 features (potentially explanatory variables) measured on 4601 email messages. The goal is to predict whether an email is spam or not. The 57 features are a mix of continuous and discrete variables. More information can be found at https://archive.ics.uci.edu/ml/datasets/spambase.\nWe first load the data and randomly separate it into a training and a test set. A more thorough analysis would be to use full K-fold cross-validation, but given the computational complexity, I decided to leave the rest of this 3-fold CV exercise to the reader.\n\ndata(spam, package = \"ElemStatLearn\")\nn &lt;- nrow(spam)\nset.seed(987)\nii &lt;- sample(n, floor(n / 3))\nspam.te &lt;- spam[ii, ]\nspam.tr &lt;- spam[-ii, ]\n\nWe now use Adaboost with 500 iterations, using stumps (1-split trees) as our weak learners / classifiers, and check the performance on the test set:\n\nlibrary(adabag)\nonesplit &lt;- rpart.control(cp = -1, maxdepth = 1, minsplit = 0, xval = 0)\nbo1 &lt;- boosting(spam ~ ., data = spam.tr, boos = FALSE, mfinal = 500, control = onesplit)\npr1 &lt;- predict(bo1, newdata = spam.te)\ntable(spam.te$spam, pr1$class) # (pr1$confusion)\n#&gt;        \n#&gt;         email spam\n#&gt;   email   883   39\n#&gt;   spam     45  566\n\nThe classification error rate on the test set is 0.055. We now compare it with that of a Random Forest and look at the fit:\n\nlibrary(randomForest)\nset.seed(123)\n(a &lt;- randomForest(spam ~ ., data = spam.tr, ntree = 500))\n#&gt; \n#&gt; Call:\n#&gt;  randomForest(formula = spam ~ ., data = spam.tr, ntree = 500) \n#&gt;                Type of random forest: classification\n#&gt;                      Number of trees: 500\n#&gt; No. of variables tried at each split: 7\n#&gt; \n#&gt;         OOB estimate of  error rate: 5.05%\n#&gt; Confusion matrix:\n#&gt;       email spam class.error\n#&gt; email  1813   53  0.02840300\n#&gt; spam    102 1100  0.08485857\n\nNote that the OOB estimate of the classification error rate is 0.051. The number of trees used seems to be appropriate in terms of the stability of the OOB error rate estimate:\n\nplot(a)\n\n\n\n\n\n\n\n\nNow use the test set to estimate the error rate of the Random Forest (for a fair comparison with the one computed with boosting) and obtain\n\npr.rf &lt;- predict(a, newdata = spam.te, type = \"response\")\ntable(spam.te$spam, pr.rf)\n#&gt;        pr.rf\n#&gt;         email spam\n#&gt;   email   886   36\n#&gt;   spam     36  575\n\nThe performance of Random Forests on this test set is better than that of boosting (recall that the estimated classification error rate for 1-split trees-based Adaboost was 0.055, while for the Random Forest is 0.047 on the test set and 0.051 using OOB).\nIs there any room for improvement for Adaboost? As we discussed in class, depending on the interactions that may be present in the true classification function, we might be able to improve our boosting classifier by slightly increasing the complexity of our base ensemble members. Here we try to use 3-split classification trees, instead of the 1-split ones used above:\n\nthreesplits &lt;- rpart.control(cp = -1, maxdepth = 3, minsplit = 0, xval = 0)\nbo3 &lt;- boosting(spam ~ ., data = spam.tr, boos = FALSE, mfinal = 500, control = threesplits)\npr3 &lt;- predict(bo3, newdata = spam.te)\n(pr3$confusion)\n#&gt;                Observed Class\n#&gt; Predicted Class email spam\n#&gt;           email   881   34\n#&gt;           spam     41  577\n\nThe number of elements on the boosting ensemble (500) appears to be appropriate when we look at the error rate on the test set as a function of the number of boosting iterations:\n\nplot(errorevol(bo3, newdata = spam.te))\n\n\n\n\n\n\n\n\nThere is, in fact, a noticeable improvement in performance on this test set compared to the AdaBoost using stumps. The estimated classification error rate of AdaBoost using 3-split trees on this test set is 0.049. Recall that the estimated classification error rate for the Random Forest was 0.047 (or 0.051 using OOB).\nAs mentioned above you are strongly encouraged to finish this analysis by doing a complete K-fold CV analysis in order to compare boosting with random forests on these data.\n\n\n18.0.2 An example on improving Adaboost’s performance including interactions\nSome error I can’t track happens below\nConsider the data set in the file boost.sim.csv. This is a synthetic data inspired by the well-known Boston Housing data. The response variable is class and the two predictors are lon and lat. We read the data set\n\nsim &lt;- read.table(\"data/boost.sim.csv\", header = TRUE, sep = \",\", row.names = 1)\n\nWe split the data randomly into a training and a test set:\n\nset.seed(123)\nii &lt;- sample(nrow(sim), nrow(sim) / 3)\nsim.tr &lt;- sim[-ii, ]\nsim.te &lt;- sim[ii, ]\n\nAs before, we use stumps as our base classifiers\n\nlibrary(rpart)\nstump &lt;- rpart.control(cp = -1, maxdepth = 1, minsplit = 0, xval = 0)\n\nand run 300 iterations of the boosting algorithm:\n\nset.seed(17)\nsim1 &lt;- boosting(class ~ ., data = sim.tr, boos = FALSE, mfinal = 300, control = stump)\n\nWe examine the evolution of our ensemble on the test set:\n\nplot(errorevol(sim1, newdata = sim.te))\n\nand note that the peformance is both disappointing and does not improve with the number of iterations. The error rate on the test set is . Based on the discussion in class about the effect of the complexity of the base classifiers, we now increase slightly their complexity: from stumps to trees with up to 2 splits:\n\ntwosplit &lt;- rpart.control(cp = -1, maxdepth = 2, minsplit = 0, xval = 0)\nset.seed(17)\nsim2 &lt;- boosting(class ~ ., data = sim.tr, boos = FALSE, mfinal = 300, control = twosplit)\nplot(errorevol(sim2, newdata = sim.te))\n\nNote that the error rate improves noticeably to . Interestingly, note as well that increasing the number of splits of the base classifiers does not seem to help much. With 3-split trees:\n\nthreesplit &lt;- rpart.control(cp = -1, maxdepth = 3, minsplit = 0, xval = 0)\nset.seed(17)\nsim3 &lt;- boosting(class ~ ., data = sim.tr, boos = FALSE, mfinal = 300, control = threesplit)\nplot(errorevol(sim3, newdata = sim.te))\n\n\nfoursplit &lt;- rpart.control(cp = -1, maxdepth = 4, minsplit = 0, xval = 0)\nset.seed(17)\nsim4 &lt;- boosting(class ~ ., data = sim.tr, boos = FALSE, mfinal = 300, control = foursplit)\n\nthe error rate on the test set is\n\nround(predict(sim3, newdata = sim.te)$error, 4)\n\nwhile with 4-split trees the error rate is\n\nround(predict(sim4, newdata = sim.te)$error, 4)\n\nThe explanation for this is that the response variables in the data set were in fact generated through the following relationship:\nlog [ P ( Y = 1 | X = x ) / P ( Y = -1 | X = x ) ] / 2\n = [ max( x2 - 2, 0) - max( x1 + 1, 0) ] ( 1- x1 + x2 )\nwhere \\(x = (x_1, x_2)^\\top\\). Since stumps (1-split trees) are by definition functions of a single variable, boosting will not be able to approximate the above function using a linear combination of them, regardless of how many terms you use. Two-split trees, on the other hand, are able to model interactions between the two explanatory variables \\(X_1\\) (lon) and \\(X_2\\) (lat), and thus, with sufficient terms in the sum, we are able to approximate the above function relatively well.\nAs before, note that the analysis above may depend on the specific training / test split we used, so it is strongly suggested that you re-do it using a proper cross-validation setup.\n\n\n\n\nFriedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2000. “Additive logistic regression: a statistical view of boosting (With discussion and a rejoinder by the authors).” The Annals of Statistics 28 (2): 337–407. https://doi.org/10.1214/aos/1016218223."
  },
  {
    "objectID": "45-single-layer-nn.html#c-and-z",
    "href": "45-single-layer-nn.html#c-and-z",
    "title": "19  Single layer neural network",
    "section": "19.1 “C” and “Z”",
    "text": "19.1 “C” and “Z”\nFirst we look at building a classifier to identify the letters C and Z. This is the simplest scenario and it will help us fix ideas. We now read the full data set, and extract the training and test rows corresponding to those two letters:\n\nlibrary(nnet)\nxx.tr &lt;- readRDS(\"data/isolet-train.RDS\")\nxx.te &lt;- readRDS(\"data/isolet-test.RDS\")\nlets &lt;- c(3, 26)\nLETTERS[lets]\n#&gt; [1] \"C\" \"Z\"\n# Training set\nx.tr &lt;- xx.tr[xx.tr$V618 %in% lets, ]\nx.tr$V618 &lt;- as.factor(x.tr$V618)\n# Test set\nx.te &lt;- xx.te[xx.te$V618 %in% lets, ]\ntruth &lt;- x.te$V618 &lt;- as.factor(x.te$V618)\n\nWe train a NN with a single hidden layer, and a single unit in the hidden layer.\n\nset.seed(123)\na1 &lt;- nnet(V618 ~ ., data = x.tr, size = 1, decay = 0, maxit = 1500, MaxNWts = 2000)\n#&gt; # weights:  620\n#&gt; initial  value 350.425020 \n#&gt; iter  10 value 41.176789\n#&gt; iter  20 value 18.095256\n#&gt; iter  30 value 18.052107\n#&gt; iter  40 value 18.050646\n#&gt; iter  50 value 18.050036\n#&gt; iter  60 value 18.048042\n#&gt; iter  70 value 12.957465\n#&gt; iter  80 value 6.912100\n#&gt; iter  90 value 6.483391\n#&gt; iter 100 value 6.482796\n#&gt; iter 110 value 6.482767\n#&gt; iter 120 value 6.482733\n#&gt; iter 120 value 6.482733\n#&gt; final  value 6.482722 \n#&gt; converged\n\nNote the slow convergence. The final value of the objective value was:\n\na1$value\n#&gt; [1] 6.482722\n\nThe error rate on the training set (“goodness of fit”) is\n\nb1 &lt;- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#&gt; [1] 0.002083333\n\nWe see that this NN fits the training set perfectly. Is this desirable?\nWe now run the algorithm again, with a different starting point.\n\nset.seed(456)\na2 &lt;- nnet(V618 ~ ., data = x.tr, size = 1, decay = 0, maxit = 1500, MaxNWts = 2000)\n#&gt; # weights:  620\n#&gt; initial  value 336.934868 \n#&gt; iter  10 value 157.630462\n#&gt; iter  20 value 61.525474\n#&gt; iter  30 value 48.367799\n#&gt; iter  40 value 42.896353\n#&gt; iter  50 value 37.039697\n#&gt; iter  60 value 36.481582\n#&gt; iter  70 value 27.239536\n#&gt; iter  80 value 20.422772\n#&gt; iter  90 value 20.410547\n#&gt; final  value 20.410540 \n#&gt; converged\n\nCompare the attained value of the objective and the error rate on the training set with those above (6.482722 and 0, respectively):\n\na2$value\n#&gt; [1] 20.41054\nb2 &lt;- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#&gt; [1] 0.008333333\n\nSo, we see that the second run of NN produces a much worse solution. How are their performances on the test set?\n\nb1 &lt;- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#&gt; [1] 0.03333333\nb2 &lt;- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#&gt; [1] 0.03333333\n\nThe second (worse) solution performs better on the test set.\nWhat if we add more units to the hidden layer? We increase the number of units on the hidden layer from 3 to 6.\n\nset.seed(123)\na1 &lt;- nnet(V618 ~ ., data = x.tr, size = 3, decay = 0, maxit = 1500, MaxNWts = 2000, trace = FALSE)\nset.seed(456)\na2 &lt;- nnet(V618 ~ ., data = x.tr, size = 3, decay = 0, maxit = 1500, MaxNWts = 2000, trace = FALSE)\n\nThe objective functions are\n\na1$value\n#&gt; [1] 6.482738\na2$value\n#&gt; [1] 9.052402e-05\n\nrespectively, and their performance on the training and test sets are:\n\nb1 &lt;- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#&gt; [1] 0.002083333\nb2 &lt;- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#&gt; [1] 0\n\nb1 &lt;- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#&gt; [1] 0.03333333\nb2 &lt;- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#&gt; [1] 0.04166667\n\nAgain we note that the (seemingly much) worse solution (in terms of the objective function whose optimization defines the NN) performs better on the test set.\nWhat if we add a decaying factor as a form of regularization?\n\nset.seed(123)\na1 &lt;- nnet(V618 ~ ., data = x.tr, size = 3, decay = 0.05, maxit = 500, MaxNWts = 2000, trace = FALSE)\na1$value\n#&gt; [1] 5.345279\nset.seed(456)\na2 &lt;- nnet(V618 ~ ., data = x.tr, size = 3, decay = 0.05, maxit = 500, MaxNWts = 2000, trace = FALSE)\na2$value\n#&gt; [1] 5.345279\n\nNow the two solutions starting from these random initial values are the same (the reader is encouraged to try more random starts). How does this NN do on the training and test sets?\n\nb1 &lt;- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#&gt; [1] 0\nb1 &lt;- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#&gt; [1] 0.008333333\n\nNote that this “regularized” solution which corresponds to a slightly better solution than the worse one above in terms of objective function (but still much worse than the best ones) performs noticeably better on the test set. This seem to suggest that it is not easy to select which of the many local extrema to used based on the objective function values they attain.\nAnother tuning parameter we can vary is the number of units in the hidden layer, which will also increase significantly the number of possible weight parameters in our model. The above solution uses 1858 weights. We now add more units to the hidden layer (6 instead of 3) and increase the limit on the number of allowable weights to 4000:\n\nset.seed(123)\na1 &lt;- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.05, maxit = 500, MaxNWts = 4000, trace = FALSE)\na1$value\n#&gt; [1] 4.172022\nset.seed(456)\na2 &lt;- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.05, maxit = 500, MaxNWts = 4000, trace = FALSE)\na2$value\n#&gt; [1] 4.172023\nb1 &lt;- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#&gt; [1] 0\n\nb2 &lt;- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#&gt; [1] 0\n\nb1 &lt;- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#&gt; [1] 0.008333333\n\nb2 &lt;- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#&gt; [1] 0.008333333\n\nNote that both of these two distinct solutions fit the training set exactly (0 apparent error rate), and have the same performance on the test set. We leave it to the reader to perform a more exhaustive study of the prediction properties of these solutions using an appropriate CV experiment."
  },
  {
    "objectID": "45-single-layer-nn.html#more-letters",
    "href": "45-single-layer-nn.html#more-letters",
    "title": "19  Single layer neural network",
    "section": "19.2 More letters",
    "text": "19.2 More letters\nWe now repeat the same exercise above but on a 4-class setting.\n\nlets &lt;- c(3, 7, 9, 26)\nx.tr &lt;- xx.tr[xx.tr$V618 %in% lets, ]\nx.tr$V618 &lt;- as.factor(x.tr$V618)\n# testing set\nx.te &lt;- xx.te[xx.te$V618 %in% lets, ]\ntruth &lt;- x.te$V618 &lt;- as.factor(x.te$V618)\n\nThe following tries show that a NN with only one unit in the hidden layer does not perform well. As before, we compare two local minima of the NN training algorithm. First we show the values of the corresponding local minima of the objective function, and then their error rates on the training and test sets.\n\nset.seed(123)\na1 &lt;- nnet(V618 ~ ., data = x.tr, size = 1, decay = 0, maxit = 1500, MaxNWts = 2000, trace = FALSE)\na1$value\n#&gt; [1] 6.482735\nset.seed(456)\na2 &lt;- nnet(V618 ~ ., data = x.tr, size = 1, decay = 0, maxit = 1500, MaxNWts = 2000, trace = FALSE)\na2$value\n#&gt; [1] 789.9009\nb1 &lt;- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#&gt; [1] 0.001041667\nb2 &lt;- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#&gt; [1] 0.5010417\nb1 &lt;- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#&gt; [1] 0.4708333\nb2 &lt;- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#&gt; [1] 0.4791667\n\nNote that the error rates on the test set are 0.471 and 0.479, which are very high. Better results are obtained with 6 units on the hidden layer and a slightly regularized solution. As before, use two runs of the training algorithm and look at the corresponding values of the objective function, and the error rates of both NNs on the training and test sets.\n\nset.seed(123)\na1 &lt;- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.05, maxit = 500, MaxNWts = 4000, trace = FALSE)\na1$value\n#&gt; [1] 9.037809\nset.seed(456)\na2 &lt;- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.05, maxit = 500, MaxNWts = 4000, trace = FALSE)\na2$value\n#&gt; [1] 9.171046\nb1 &lt;- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#&gt; [1] 0\nb2 &lt;- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#&gt; [1] 0\nb1 &lt;- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#&gt; [1] 0.0125\nb2 &lt;- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#&gt; [1] 0.0125\n\nThe error rates on the test set are now 0.013 and 0.013, which are much better than before."
  },
  {
    "objectID": "45-single-layer-nn.html#even-more-letters",
    "href": "45-single-layer-nn.html#even-more-letters",
    "title": "19  Single layer neural network",
    "section": "19.3 Even more letters",
    "text": "19.3 Even more letters\nWe now consider building a classifier with 7 classes, which is a more challenging problem.\n\nlets &lt;- c(3, 5, 7, 9, 12, 13, 26)\nLETTERS[lets]\n#&gt; [1] \"C\" \"E\" \"G\" \"I\" \"L\" \"M\" \"Z\"\nx.tr &lt;- xx.tr[xx.tr$V618 %in% lets, ]\nx.tr$V618 &lt;- as.factor(x.tr$V618)\n# testing set\nx.te &lt;- xx.te[xx.te$V618 %in% lets, ]\ntruth &lt;- x.te$V618 &lt;- as.factor(x.te$V618)\n\nThe following code trains a NN with 6 units on the hidden layer and moderate regularization (via a decaying factor of 0.3 and an upper limit of 4000 weights).\n\nset.seed(123)\na1 &lt;- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.3, maxit = 1500, MaxNWts = 4000, trace = FALSE)\na1$value\n#&gt; [1] 102.1805\nset.seed(456)\na2 &lt;- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.3, maxit = 1500, MaxNWts = 4000, trace = FALSE)\na2$value\n#&gt; [1] 100.5938\nb1 &lt;- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#&gt; [1] 0\nb2 &lt;- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#&gt; [1] 0\nb1 &lt;- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#&gt; [1] 0.01909308\nb2 &lt;- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#&gt; [1] 0.01193317\n\nNote that in this case the NN with a better objective function (100.5938029 versus 102.1805373) achieves a better performance on the test set (0.012 versus 0.019), although the difference is rather small. Conclusions based on a proper CV study would be much more reliable.\nYou are strongly encouraged to study what happens with other combinations of decay, number of weights and number of units on the hidden layer, using a proper CV setting to evaluate the results."
  },
  {
    "objectID": "51-pca.html#principal-components-analysis",
    "href": "51-pca.html#principal-components-analysis",
    "title": "20  Introduction",
    "section": "20.1 Principal Components Analysis",
    "text": "20.1 Principal Components Analysis\nAlthough principal components can be easily computed with the spectral decomposition of the covariance matrix of the data (using the function svd in R, for example), there are a few dedicated implementations in R, among them prcomp and princomp). The main difference between these two is which internal function is used to compute eigenvalues and eigenvectors: prcomp uses svd and princomp uses the less preferred function eigen. Both princomp and prcomp return the matrix of loadings (eigenvectors), the scores (projections of the data on the basis of eigenvectors), and other auxiliary objects. They also include plot and summary methods.\nInstead of reviewing those (which can be easily done individually), in these notes I will reproduce two of the examples used in class (the simple 2-dimensional one used to motivate the topic, and the more interesting 256-dimensional one using the digits data).\nFinally, I will also show that principal components can be computed using an iterative algorithm (alternate regression), which may be faster than factorizing the covariance matrix, particularly when one is only interested in a few principal components and the dimension of the data is large (but also look at the arguments nu and nv for the function svd in R)."
  },
  {
    "objectID": "51-pca.html#simple-2-dimensional-example",
    "href": "51-pca.html#simple-2-dimensional-example",
    "title": "20  Introduction",
    "section": "20.2 Simple 2-dimensional example",
    "text": "20.2 Simple 2-dimensional example\nWe first read the data for the simple illustration of PC’s as best lower dimensional approximations.\n\nx &lt;- read.table(\"data/t8-5.dat\", header = FALSE)\n\nNote that the data has 5 explanatory variables. Here we only use two of them in order to be able to visualize the analysis more easily:\n\nxx &lt;- x[, c(2, 5)]\ncolnames(xx) &lt;- c(\"Prof degree\", \"Median home value\")\n\nAs discussed in class, we standardize the data to avoid a large difference in scales “hijacking” the principal components:\n\nxx &lt;- scale(xx, center = colMeans(xx), scale = TRUE)\n\nWe now define two auxiliary functions to compute Euclidean norms and squared Euclidean norms (less general by probably faster than R’s base::norm):\n\nnorm2 &lt;- function(a) sum(a^2)\nnorm &lt;- function(a) sqrt(norm2(a))\n\nWe start by looking at the data with a scatter plot:\n\nplot(xx, pch = 19, col = \"black\", cex = 2, xlim = c(-1.5, 4.5), ylim = c(-1.5, 4.5))\nabline(h = 0, lwd = 2, col = \"grey\", lty = 2)\nabline(v = 0, lwd = 2, col = \"grey\", lty = 2)\n\n\n\n\n\n\n\n\nWe now compute projections along the direction of the vector \\(\\mathbf{v} \\propto (1, 0.05)^\\top\\). Recall from your linear algebra courses that the orthogonal projection of a point \\(\\mathbf{x}\\) on the linear subspace spanned by \\(\\mathbf{v}\\) (where \\(\\| \\mathbf{v} \\| = 1\\)) is given by \\(\\pi_{\\mathbf{v}} ( \\mathbf{x} ) = \\langle \\mathbf{x}, \\mathbf{v} \\rangle \\, \\mathbf{v}\\) which can also be written as \\(\\pi_{\\mathbf{v}} ( \\mathbf{x} ) = ( \\mathbf{v} \\, \\mathbf{v}^\\top) \\mathbf{x}\\). We first find the coordinates of the orthogonal projects of each observation along the subspace generated by \\(\\mathbf{v} = (1, 0.05)^\\top\\) (these are the scalars \\(\\langle \\mathbf{x}_i, \\mathbf{v} \\rangle = \\mathbf{x}_i^\\top \\mathbf{v}\\) for each point \\(\\mathbf{x}_i\\):\n\na &lt;- c(1, 0.05)\na &lt;- a / norm(a)\n# Find the projections (coordinates of the\n# observations on this basis of size 1)\nprs &lt;- (xx %*% a)\n\nWe now compute the projections \\(\\pi_{\\mathbf{v}} ( \\mathbf{x}_i ) = \\langle \\mathbf{x}_i, \\mathbf{v} \\rangle \\, \\mathbf{v}\\):\n\npr &lt;- prs %*% a\n\nand add them to the plot, with a few observations highlighted. The subspace is shown in red, and the orthogonal projections as solid red dots on that line:\n\n# Plot the data\nplot(xx, pch = 19, col = \"black\", cex = 2, xlim = c(-1.5, 4.5), ylim = c(-1.5, 4.5))\nabline(h = 0, lwd = 2, col = \"grey\", lty = 2)\nabline(v = 0, lwd = 2, col = \"grey\", lty = 2)\n# Show the subspace on which we are projecting\nabline(0, a[2] / a[1], lwd = 2, col = \"red\")\n# Add the projections of the data on this subspace\npoints(pr[, 1], pr[, 2], pch = 19, cex = 1.5, col = \"red\")\n# Highlight a few of them\nind &lt;- c(26, 25, 48, 36)\npr2 &lt;- pr[ind, ]\nfor (j in 1:length(ind)) {\n  lines(c(xx[ind[j], 1], pr2[j, 1]), c(xx[ind[j], 2], pr2[j, 2]),\n    col = \"blue\", lwd = 3.5, lty = 2\n  )\n}\n\n\n\n\n\n\n\n\nWe repeat the above but projecting on a different direction \\(\\mathbf{v} \\propto (-1, 3)^\\top\\):\n\na &lt;- c(-1, 3)\na &lt;- a / norm(a)\n# Find the projections (coordinates of the\n# observations on this basis of size 1)\nprs &lt;- (xx %*% a)\n# Find the orthogonal projections of each\n# observation on this subspace of dimension 1\npr &lt;- prs %*% a\n# Plot the data\nplot(xx, pch = 19, col = \"black\", cex = 2, xlim = c(-1.5, 4.5), ylim = c(-1.5, 4.5))\nabline(h = 0, lwd = 2, col = \"grey\", lty = 2)\nabline(v = 0, lwd = 2, col = \"grey\", lty = 2)\n\n# Show the subspace on which we are projecting\nabline(0, a[2] / a[1], lwd = 2, col = \"red\")\n\n# Add the projections of the data on this subspace\npoints(pr[, 1], pr[, 2], pch = 19, cex = 1.5, col = \"red\")\n\n# Highlight a few of them\nind &lt;- c(26, 25, 48, 36)\npr2 &lt;- pr[ind, ]\nfor (j in 1:length(ind)) {\n  lines(c(xx[ind[j], 1], pr2[j, 1]), c(xx[ind[j], 2], pr2[j, 2]),\n    col = \"blue\", lwd = 3.5, lty = 2\n  )\n}\n\n\n\n\n\n\n\n\nWe saw in class that the direction \\(\\mathbf{v}\\) that results in orthogonal projections closest to the original data (in the sense of minimizing the mean (or sum) of the residuals Euclidean norm squared) is given by the “first” eigenvector of the covariance matrix of the data. This is the first principal component. Refer to the class slides and discussion for more details and the definition and properties of the other principal components."
  },
  {
    "objectID": "51-pca.html#digits-example",
    "href": "51-pca.html#digits-example",
    "title": "20  Introduction",
    "section": "20.3 Digits example",
    "text": "20.3 Digits example\nIn this example we use principal components to explore the zip code data. In particular, we focus on images from a single digit (we use 3, but the reader is strongly encouraged to re-do this analysis for other digits to explore whether similar conclusions hold for them). We load the training data from the ElemStatLearn package in R, and extract the images that correspond to the digit 3. For more information use help(zip.train, package='ElemStatLearn').\n\ndata(zip.train, package = \"ElemStatLearn\")\na &lt;- zip.train[zip.train[, 1] == 3, -1]\n\nDefine an auxiliary function to compute the squared Euclidean distance between two vectors (recall that we have already defined the function norm2 above):\n\ndist &lt;- function(a, b) norm2(a - b)\n\nTo display the images we adapt the following function for plotting a matrix, which was originally available at http://www.phaget4.org/R/image_matrix.html:\n\nmyImagePlot &lt;- function(x) {\n  min &lt;- min(x)\n  max &lt;- max(x)\n  ColorRamp &lt;- grey(seq(1, 0, length = 256))\n  ColorLevels &lt;- seq(min, max, length = length(ColorRamp))\n  # Reverse Y axis\n  reverse &lt;- nrow(x):1\n  x &lt;- x[reverse, ]\n  image(1:ncol(x), 1:nrow(x), t(x),\n    col = ColorRamp, xlab = \"\",\n    ylab = \"\", axes = FALSE, zlim = c(min, max)\n  )\n}\n\nUsing this function, we plot 9 randomly chosen images from our data set:\n\nset.seed(31)\nsa &lt;- sample(nrow(a), 9)\npar(mai = c(1, 1, 1, 1) / 5, xaxs = \"i\", yaxs = \"i\")\npar(mfrow = c(3, 3))\nfor (j in 1:9) myImagePlot(t(matrix(unlist(a[sa[j], ]), 16, 16)))\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we centre the observations in order to compute the eigenvectors and eigenvalues of the covariance matrix more efficiently. In fact, note that we do not need to even compute the covariance matrix and can use the SVD of the centred data.\n\nac &lt;- scale(a, center = TRUE, scale = FALSE)\nsi.svd &lt;- svd(ac)\n\nUsing the relationship between the eigenvectors of the covariance matrix and the SVD of the \\(n \\times p\\) data matrix, we compute the coordinates of the centered data on their orthogonal projections along each of the first and 2nd and 3rd principal directions (eigenvectors of the covariance matrix). Recall that the data are stored as rows of the matrix a:\n\nv1 &lt;- as.vector(ac %*% si.svd$v[, 1])\nv2 &lt;- as.vector(ac %*% si.svd$v[, 2])\n\nAs discussed in class, we identify 5 quantiles of each of these coordinates to use as our 2-dimensional grid:\n\nqv1 &lt;- quantile(v1, c(.05, .25, .5, .75, .95))\nqv2 &lt;- quantile(v2, c(.05, .25, .5, .75, .95))\n\nWe can visualize the grid of these 5 x 5 = 25 points over the scatter plot of all the 2-dimensional projections of the data (their coordinates on the principal components basis):\n\nqv &lt;- expand.grid(qv1, qv2)\nplot(v1, v2, pch = 19, cex = 1, col = \"grey\")\npoints(qv[, 1], qv[, 2], pch = 19, cex = 1.5, col = \"red\")\n\n\n\n\n\n\n\n\nWe now find the points in our data set (images) with projections closest to each of the 5 x 5 = 25 points in the grid (note that these distances between points in the principal-subspace, which is in the 256 dimensional space) can be computed in terms of their coordinates on the principal-basis only (which are 2-dimensional points):\n\nvs &lt;- cbind(v1, v2)\ncvs &lt;- array(0, dim = dim(qv))\nfor (j in 1:dim(qv)[1]) cvs[j, ] &lt;- vs[which.min(apply(vs, 1, dist, b = qv[j, ])), ]\n\nWe now add these points to our plot (we use color blue for them):\n\nplot(v1, v2, pch = 19, cex = 1, col = \"grey\")\npoints(qv[, 1], qv[, 2], pch = 19, cex = 1.5, col = \"red\")\nfor (j in 1:dim(qv)[1]) points(cvs[j, 1], cvs[j, 2], pch = 19, col = \"blue\")\n\n\n\n\n\n\n\n\nUsing these “blue” coordinates, we construct the corresponding points in the 256-dimensional space:\n\napp &lt;- t(si.svd$v[, 1:2] %*% t(cvs))\n\nand identify the images in our data set that are closest to these points\n\nrepre &lt;- matrix(0, dim(qv)[1], dim(app)[2])\nfor (j in 1:dim(qv)[1]) repre[j, ] &lt;- ac[which.min(apply(ac, 1, dist, b = app[j, ])), ]\n\nThese are the actual images that are closest to the points in the array app above. Now add the column means and display these 25 images according to the points they represent in the red grid:\n\nrepre &lt;- scale(repre, center = -colMeans(a), scale = FALSE)\npar(mai = c(1, 1, 1, 1) / 5, xaxs = \"i\", yaxs = \"i\")\npar(mfrow = c(5, 5))\nfor (j in 1:dim(repre)[1]) {\n  myImagePlot(t(matrix(unlist(repre[j, ]), 16, 16)))\n}\n\n\n\n\n\n\n\n\nNote how these images change when we “traverse” the 256-dimensional space along each of these 2 principal directions."
  },
  {
    "objectID": "51-pca.html#alternating-regression-to-compute-principal-components",
    "href": "51-pca.html#alternating-regression-to-compute-principal-components",
    "title": "20  Introduction",
    "section": "20.4 Alternating regression to compute principal components",
    "text": "20.4 Alternating regression to compute principal components\nFor details see Appendix @ref(alt-pca).\nA function implementing this method to compute the first principal component is:\n\nalter.pca.k1 &lt;- function(x, max.it = 500, eps = 1e-10) {\n  n2 &lt;- function(a) sum(a^2)\n  p &lt;- dim(x)[2]\n  x &lt;- scale(x, scale = FALSE)\n  it &lt;- 0\n  old.a &lt;- c(1, rep(0, p - 1))\n  err &lt;- 10 * eps\n  while (((it &lt;- it + 1) &lt; max.it) & (abs(err) &gt; eps)) {\n    b &lt;- as.vector(x %*% old.a) / n2(old.a)\n    a &lt;- as.vector(t(x) %*% b) / n2(b)\n    a &lt;- a / sqrt(n2(a))\n    err &lt;- sqrt(n2(a - old.a))\n    old.a &lt;- a\n  }\n  conv &lt;- (it &lt; max.it)\n  return(list(a = a, b = b, conv = conv))\n}\n\nWe use it on the digits data above to compute the first principal component (we also time it):\n\nsystem.time(tmp &lt;- alter.pca.k1(ac)$a)\n#&gt;    user  system elapsed \n#&gt;   0.058   0.007   0.065\n\nand compare it with the one given by svd, which we also time. Note that the sign of the eigenvectors is arbitrary, so we adjust these vectors in order to have first elements with the same sign.\n\nsystem.time(tmp2 &lt;- svd(ac)$v[, 1])\n#&gt;    user  system elapsed \n#&gt;   0.183   0.002   0.183\ntmp &lt;- tmp * sign(tmp2[1] * tmp[1])\nsummary(abs(tmp - tmp2))\n#&gt;      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#&gt; 4.200e-16 1.195e-12 4.012e-12 7.272e-12 1.169e-11 3.524e-11\n\nNote that both eigenvectors are essentially identical, and that the alternating regression method is typically faster than a full SVD decomposition of the covariance matrix.\nThis difference in speed is more striking for problems in higer dimensions.\nTo further illustrate the potential gain in speed for larger dimensions, consider the following synthetic data set with n = 2000 observation and p = 1000, and compare the timing and the results (even when forcing svd to only compute a single component).\nFirst generate the data set\n\nn &lt;- 2000\np &lt;- 1000\nx &lt;- matrix(rt(n * p, df = 2), n, p)\n\nCompute the first eigenvector using alternating regression, and time it:\n\nsystem.time(tmp &lt;- alter.pca.k1(x))\n#&gt;    user  system elapsed \n#&gt;   0.143   0.013   0.157\na1 &lt;- tmp$a\n\nCompute the first eigenvector using svd, and time it:\n\nsystem.time(e1 &lt;- svd(cov(x))$u[, 1])\n#&gt;    user  system elapsed \n#&gt;   5.011   0.038   5.053\n\nAsking svd to only compute one component does not seem to make the algorithm faster (the results are identical):\n\nsystem.time(e1.1 &lt;- svd(cov(x), nu = 1, nv = 1)$u[, 1])\n#&gt;    user  system elapsed \n#&gt;   4.974   0.024   4.999\nsummary(abs(e1 - e1.1))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;       0       0       0       0       0       0\n\nFinally, check that the first eigenvector computed with svd and with the alternating regression approach are practially identical:\n\na1 &lt;- a1 * sign(e1[1] * a1[1])\nsummary(abs(e1 - a1))\n#&gt;      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#&gt; 0.000e+00 9.376e-18 2.076e-17 5.745e-17 4.107e-17 2.232e-14"
  },
  {
    "objectID": "52-kmeans.html#k-means-k-means-k-medoids",
    "href": "52-kmeans.html#k-means-k-means-k-medoids",
    "title": "21  Clustering",
    "section": "21.1 K-means, K-means++, K-medoids",
    "text": "21.1 K-means, K-means++, K-medoids\nProbably the most intuitive and easier to explain unsupervised clustering algorithm is K-means (and its variants K-means++ and K-medoids, a.k.a. pam, partition around medoids). The specifics of the K-means algorithm were discussed in class. Here we will illustrate its use on a few examples.\n\n21.1.1 UN votes example.\nThese data contain the historical voting patterns of United Nations members. More details can be found here at (Voeten, Strezhnev, and Bailey 2009). The UN was founded in 1946 and it contains 193 member states. The data include only “important” votes, as classified by the U.S. State Department. The votes for each country were coded as follows: Yes (1), Abstain (2), No (3), Absent (8), Not a Member (9). There were 368 important votes, and 77 countries voted in at least 95% of these. We focus on these UN members. Our goal is to explore whether voting patterns reflect political alignments, and also whether countries vote along known political blocks. Our data consists of 77 observations with 368 variables each. More information on these data can be found here.\nThe dataset is organized by vote (resolution), one per row, and its columns contain the corresponding vote of each country (one country per column). We first read the data, and limit ourselves to resolutions where every country voted without missing votes:\n\nX &lt;- read.table(file = \"data/unvotes.csv\", sep = \",\", row.names = 1, header = TRUE)\nX2 &lt;- X[complete.cases(X), ]\n\nWe now compute a K-means partition using the function kmeans with K = 5, and look at the resulting groups:\n\nset.seed(123)\nb &lt;- kmeans(t(X2), centers = 5, iter.max = 20, nstart = 1)\ntable(b$cluster)\n#&gt; \n#&gt;  1  2  3  4  5 \n#&gt; 26  2 15  9 25\n\nIf we run kmeans again, we might get a different partition:\n\nb &lt;- kmeans(t(X2), centers = 5, iter.max = 20, nstart = 1)\ntable(b$cluster)\n#&gt; \n#&gt;  1  2  3  4  5 \n#&gt; 27 13 12  5 20\n\nIt is better to consider a large number of random starts and take the best found solution (what does best mean in this context? in other words, how does the algorithm decide which one is the solution it should return?)\n\n# Take the best solution out of 1000 random starts\nb &lt;- kmeans(t(X2), centers = 5, iter.max = 20, nstart = 1000)\nsplit(colnames(X2), b$cluster)\n#&gt; $`1`\n#&gt;  [1] \"Algeria\"              \"Bangladesh\"           \"Belarus\"             \n#&gt;  [4] \"Brunei.Darussalam\"    \"China\"                \"Cuba\"                \n#&gt;  [7] \"Egypt\"                \"India\"                \"Indonesia\"           \n#&gt; [10] \"Kuwait\"               \"Libya\"                \"Malaysia\"            \n#&gt; [13] \"Pakistan\"             \"Russian.Federation\"   \"Sudan\"               \n#&gt; [16] \"Syrian.Arab.Republic\" \"Venezuela\"           \n#&gt; \n#&gt; $`2`\n#&gt;  [1] \"Bolivia\"             \"Botswana\"            \"Burkina.Faso\"       \n#&gt;  [4] \"Ecuador\"             \"Ethiopia\"            \"Ghana\"              \n#&gt;  [7] \"Guyana\"              \"Jamaica\"             \"Jordan\"             \n#&gt; [10] \"Kenya\"               \"Mali\"                \"Nepal\"              \n#&gt; [13] \"Nigeria\"             \"Philippines\"         \"Singapore\"          \n#&gt; [16] \"Sri.Lanka\"           \"Tanzania\"            \"Thailand\"           \n#&gt; [19] \"Togo\"                \"Trinidad.and.Tobago\" \"Zambia\"             \n#&gt; \n#&gt; $`3`\n#&gt;  [1] \"Argentina\"  \"Bahamas\"    \"Brazil\"     \"Chile\"      \"Colombia\"  \n#&gt;  [6] \"Costa.Rica\" \"Mexico\"     \"Panama\"     \"Paraguay\"   \"Peru\"      \n#&gt; [11] \"Uruguay\"   \n#&gt; \n#&gt; $`4`\n#&gt;  [1] \"Australia\"   \"Austria\"     \"Belgium\"     \"Bulgaria\"    \"Canada\"     \n#&gt;  [6] \"Cyprus\"      \"Denmark\"     \"Finland\"     \"France\"      \"Greece\"     \n#&gt; [11] \"Hungary\"     \"Iceland\"     \"Ireland\"     \"Italy\"       \"Japan\"      \n#&gt; [16] \"Luxembourg\"  \"Malta\"       \"Netherlands\" \"New.Zealand\" \"Norway\"     \n#&gt; [21] \"Poland\"      \"Portugal\"    \"Spain\"       \"Sweden\"      \"UK\"         \n#&gt; [26] \"Ukraine\"    \n#&gt; \n#&gt; $`5`\n#&gt; [1] \"Israel\" \"USA\"\n\nIt may be better to look at the groups on a map:\n\nlibrary(rworldmap)\nlibrary(countrycode)\nthese &lt;- countrycode(colnames(X2), \"country.name\", \"iso3c\")\nmalDF &lt;- data.frame(country = these, cluster = b$cluster)\n# malDF is a data.frame with the ISO3 country names plus a variable to merge to\n# the map data\n\n# This line will join your malDF data.frame to the country map data\nmalMap &lt;- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\n#&gt; 77 codes from your data successfully matched countries in the map\n#&gt; 0 codes from your data failed to match with a country code in the map\n#&gt; 166 codes from the map weren't represented in your data\n# colors()[grep('blue', colors())] fill the space on the graphical device\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", missingCountryCol = \"white\",\n    addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\",\n        \"blueviolet\", \"yellow\"), oceanCol = \"dodgerblue\")\n\n\n\n\n\n\n\n\nWe can compare this partition with the one we obtain using PAM (K-medoids), which is implemented in the function pam of the package cluster. Recall from the discussion in class that pam does not need to manipulate the actual observations, only its pairwise distances (or dissimilarities). In this case we use Euclidean distances, but it may be interesting to explore other distances, particulary in light of the categorical nature of the data. Furthermore, to obtain clusters that may be easier to interpret we use K = 3:\n\nlibrary(cluster)\n# Use Euclidean distances\nd &lt;- dist(t(X))\n# what happens with missing values?\nset.seed(123)\na &lt;- pam(d, k = 3)\n\nCompare the resulting groups with those of K-means:\n\nb &lt;- kmeans(t(X2), centers = 3, iter.max = 20, nstart = 1000)\ntable(a$clustering)\n#&gt; \n#&gt;  1  2  3 \n#&gt; 26 24 27\ntable(b$cluster)\n#&gt; \n#&gt;  1  2  3 \n#&gt; 16 28 33\n\nAn better visualization is done using the map. interesting We plot the 3 groups found by pam on the map, followed by those found by K-means:\n\nthese &lt;- countrycode(colnames(X), \"country.name\", \"iso3c\")\nmalDF &lt;- data.frame(country = these, cluster = a$clustering)\nmalMap &lt;- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\n#&gt; 77 codes from your data successfully matched countries in the map\n#&gt; 0 codes from your data failed to match with a country code in the map\n#&gt; 166 codes from the map weren't represented in your data\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", missingCountryCol = \"white\",\n    addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\",\n        \"blueviolet\", \"yellow\"), oceanCol = \"dodgerblue\")\n\n\n\n\n\n\n\nthese &lt;- countrycode(colnames(X2), \"country.name\", \"iso3c\")\nmalDF &lt;- data.frame(country = these, cluster = b$cluster)\nmalMap &lt;- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\n#&gt; 77 codes from your data successfully matched countries in the map\n#&gt; 0 codes from your data failed to match with a country code in the map\n#&gt; 166 codes from the map weren't represented in your data\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", missingCountryCol = \"white\",\n    addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"yellow\", \"tomato\", \"blueviolet\"),\n    oceanCol = \"dodgerblue\")\n\n\n\n\n\n\n\n\nWhat if we use the L_1 norm instead?\n\nd &lt;- dist(t(X), method = \"manhattan\")\nset.seed(123)\na &lt;- pam(d, k = 3)\nthese &lt;- countrycode(colnames(X), \"country.name\", \"iso3c\")\nmalDF &lt;- data.frame(country = these, cluster = a$clustering)\nmalMap &lt;- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\n#&gt; 77 codes from your data successfully matched countries in the map\n#&gt; 0 codes from your data failed to match with a country code in the map\n#&gt; 166 codes from the map weren't represented in your data\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", missingCountryCol = \"white\",\n    addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\",\n        \"blueviolet\", \"yellow\"), oceanCol = \"dodgerblue\")\n\n\n\n\n\n\n\n\nAs mentioned before, since the data set does not include a true label, the comparison between the different results is somewhat subjective, and it often relies on the knowledge of the subject matter experts. In our example above, this would mean asking the opinion of a political scientist as to whether these groupings correspond to known international political blocks or alignments.\n\n\n21.1.2 Breweries\nIn this example beer drinkers were asked to rate 9 breweries one 26 attributes, e.g. whether this brewery has a rich tradition; or whether it makes very good pilsner beer, etc. For each of these questions, the judges reported a score on a 6-point scale ranging from 1: “not true at all” to 6: “very true”. The data are in the file breweries.dat:\n\nx &lt;- read.table(\"data/breweries.dat\", header = FALSE)\nx &lt;- t(x)\n\nFor illustration purposes we use the \\(L_1\\) distance and the PAM clustering method.\n\nd &lt;- dist(x, method = \"manhattan\")\nset.seed(123)\na &lt;- pam(d, k = 3)\ntable(a$clustering)\n#&gt; \n#&gt; 1 2 3 \n#&gt; 3 3 3\n\nTo visualize the strength of these cluster partition we use the silhouette plot discussed in class:\n\nplot(a)\n\n\n\n\n\n\n\n\nSince other distances may produce different partitions, an interesting exercise would be to compare the above clusters with those found using the Euclidean or \\(L_\\infty\\) norms, for example.\n\n\n21.1.3 Cancer example\nThis data contains gene expression levels for 6830 genes (rows) for 64 cell samples (columns). More information can be found here: http://genome-www.stanford.edu/nci60/. The data are included in the ElemStatLearn package, and also available on-line: https://web.stanford.edu/~hastie/ElemStatLearn/.\nWe will use K-means to identify 8 possible clusters among the 64 cell samples. As discussed in class this exercise can (perhaps more interestingly) be formulated in terms of feature selection. We load the data and use K-means to find 8 clusters:\n\ndata(nci, package = \"ElemStatLearn\")\nncit &lt;- t(nci)\nset.seed(31)\na &lt;- kmeans(ncit, centers = 8, iter.max = 5000, nstart = 100)\ntable(a$cluster)\n#&gt; \n#&gt;  1  2  3  4  5  6  7  8 \n#&gt;  3  4  5  8 14  6  9 15\n\nNote that in this application we do know the group to which each observation belongs (its cancer type). We can look at the cancer types that have been grouped together in each of the 8 clusters:\n\nsapply(split(colnames(nci), a$cluster), table)\n#&gt; $`1`\n#&gt; \n#&gt; K562A-repro K562B-repro    LEUKEMIA \n#&gt;           1           1           1 \n#&gt; \n#&gt; $`2`\n#&gt; \n#&gt;      BREAST MCF7A-repro MCF7D-repro \n#&gt;           2           1           1 \n#&gt; \n#&gt; $`3`\n#&gt; \n#&gt; LEUKEMIA \n#&gt;        5 \n#&gt; \n#&gt; $`4`\n#&gt; \n#&gt; BREAST    CNS  RENAL \n#&gt;      2      5      1 \n#&gt; \n#&gt; $`5`\n#&gt; \n#&gt;    COLON    NSCLC  OVARIAN PROSTATE \n#&gt;        1        6        5        2 \n#&gt; \n#&gt; $`6`\n#&gt; \n#&gt; COLON \n#&gt;     6 \n#&gt; \n#&gt; $`7`\n#&gt; \n#&gt;   BREAST MELANOMA \n#&gt;        2        7 \n#&gt; \n#&gt; $`8`\n#&gt; \n#&gt;   BREAST MELANOMA    NSCLC  OVARIAN    RENAL  UNKNOWN \n#&gt;        1        1        3        1        8        1\n\nNote that clusters 3, 4, 6 and 7 are dominated by one type of cancer. Similarly, almost all melanoma and renal samples are in clusters 7 and 8, respectively, while all CNS samples are in cluster 4. Cluster 5 is harder to interpret. Although all but one ovarian cancer samples are in this cluster, it also contains 2/3 of the NSCLC samples. It may be of interest to compare these results with those using different numbers of clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVoeten, Erik, Anton Strezhnev, and Michael Bailey. 2009. “United Nations General Assembly Voting Data.” Harvard Dataverse. https://doi.org/10.7910/DVN/LEJUQZ."
  },
  {
    "objectID": "53-model-based-clustering.html#em-algorithm",
    "href": "53-model-based-clustering.html#em-algorithm",
    "title": "22  Model based clustering",
    "section": "22.1 EM algorithm",
    "text": "22.1 EM algorithm\nThe specifics of the EM algorithm were introduced and discussed in class. Although the algorithm may seem clear at first sight, it is fairly subtle, and mistakes and misunderstandings are very (very) common. Many applications of the EM algorithm found on-line are either wrong, or wrongly derived. For a more detailed discussion and a different (and also very useful) application of the algorithm, see the Section Imputation via EM below."
  },
  {
    "objectID": "53-model-based-clustering.html#bivariate-gaussian-mixture-model-via-em-by-hand",
    "href": "53-model-based-clustering.html#bivariate-gaussian-mixture-model-via-em-by-hand",
    "title": "22  Model based clustering",
    "section": "22.2 Bivariate Gaussian mixture model via EM “by hand”",
    "text": "22.2 Bivariate Gaussian mixture model via EM “by hand”\nWe will use a 2-dimensional representation of the UN votes data. This lower-dimensional representation is obtained using multidimensional scaling, a topic we will cover later in the course. For formulas and specific steps of the algorithm please refer to your class notes. We first load the data and reduce it to a 2-dimensional problem, in order to be able to plot the results. It will be a very nice exercise for the reader to re-do this analysis on the original data set.\n\nX &lt;- read.table(file = \"data/unvotes.csv\", sep = \",\", row.names = 1, header = TRUE)\n# Compute pairwise distances and use MDS\ndd &lt;- dist(t(X))\ntmp &lt;- cmdscale(dd, k = 2)\n\nThis is the data with which we will work:\n\nplot(tmp, pch = 19, col = \"gray50\", cex = 2, xlab = \"\", ylab = \"\")\n\n\n\n\n\n\n\n\nWe will now use the EM algorithm to find (Gaussian-ly distributed) clusters in the data. First we find initial maximum likelihood estimators (i.e. initial values for the EM algorithm), using a random partition of the data:\n\nk &lt;- 3\nn &lt;- nrow(tmp)\nset.seed(123456)\nb &lt;- sample((1:n) %% k + 1)\ngammas &lt;- matrix(0, n, k)\nfor (j in 1:k) gammas[b == j, j] &lt;- 1\npis &lt;- colSums(gammas) / n\nmus &lt;- sigmas &lt;- vector(\"list\", k)\nfor (j in 1:k) {\n  mus[[j]] &lt;- colSums(tmp * gammas[, j]) / sum(gammas[, j])\n  sigmas[[j]] &lt;- t(tmp * gammas[, j]) %*% tmp / sum(gammas[, j])\n}\n\nNote that the above loop could have been computed more efficiently using the fact that at the initial step the gamma coefficients are either 0’s or 1’s. However, in the following steps of the EM algorithm we will need to use such weighted averages computations, since in general the weights are between 0 and 1.\nThis is the initial configuration (pure noise):\n\nplot(tmp[, 1], tmp[, 2],\n  pch = 19, cex = 2,\n  col = c(\"black\", \"red\", \"darkblue\")[b], xlab = \"\", ylab = \"\"\n)\n\n\n\n\n\n\n\n\nWe now launch our iterations. Here I run 120 iterations. Can you think of an appropriate convergence criterion? Should we look at the parameter estimates, the gammas (posterior class probabilities), the likelihood function?\n\nlibrary(mvtnorm)\nniter &lt;- 120\nfor (i in 1:niter) {\n  # E step\n  # compute posterior probabilites f(x_i, \\theta^k)\n  for (j in 1:k) {\n    gammas[, j] &lt;- apply(tmp, 1, dmvnorm,\n      mean = mus[[j]],\n      sigma = sigmas[[j]]\n    )\n  }\n  # multiply by probs of each class\n  # f(x_i, \\theta^k) * pi_k\n  gammas &lt;- gammas %*% diag(pis)\n  # standardize: f(x_i, \\theta^k) * pi_k / [ sum_s { f(x_i, \\theta^s) * pi_s } ]\n  gammas &lt;- gammas / rowSums(gammas)\n  # M step\n  # the maximizers of the expected likelihood have\n  # a closed form in the Gaussian case, they are\n  # just weighted means and covariance matrices\n  for (j in 1:k) {\n    mus[[j]] &lt;- colSums(tmp * gammas[, j]) / sum(gammas[, j])\n    tmp2 &lt;- scale(tmp, scale = FALSE, center = mus[[j]])\n    sigmas[[j]] &lt;- t(tmp2 * gammas[, j]) %*% tmp2 / sum(gammas[, j])\n  }\n  # update pi's\n  pis &lt;- colSums(gammas) / n # n = sum(colSums(gammas))\n}\n\nWe now plot the estimated density for X, which is a combination of 3 gaussian densities. We do this by evaluating the estimated densities on a relatively fine grid of points and displaying them. We will color the points according to the estimated group labels (the largest estimated posterior probability for each point). We first compute those\n\n# estimated groups\nemlab &lt;- apply(gammas, 1, which.max)\n# build a 100 x 100 grid\nngr &lt;- 100\nx1 &lt;- seq(-15, 15, length = ngr)\nx2 &lt;- seq(-10, 7, length = ngr)\nxx &lt;- expand.grid(x1, x2)\n# evaluate each density component on each grid point\nm &lt;- matrix(NA, ngr * ngr, k)\nfor (j in 1:k) {\n  m[, j] &lt;- apply(xx, 1, dmvnorm, mean = mus[[j]], sigma = sigmas[[j]])\n}\n# apply weights\nmm &lt;- m %*% pis # apply(m, 1, max)\nfilled.contour(x1, x2, matrix(mm, ngr, ngr),\n  col = terrain.colors(35),\n  xlab = \"\", ylab = \"\",\n  panel.last = {\n    points(tmp[, 1], tmp[, 2], pch = 19, cex = 1, col = c(\"black\", \"red\", \"darkblue\")[emlab])\n  }\n)\n\n\n\n\n\n\n\n\nWe can also show each separate estimated component:\n\nm2 &lt;- m %*% diag(pis)\nfor (j in 1:k) {\n  filled.contour(x1, x2, matrix(m2[, j], ngr, ngr),\n    col = terrain.colors(35), xlab = \"\", ylab = \"\",\n    panel.last = {\n      points(tmp[, 1], tmp[, 2], pch = 19, cex = 1, col = c(\"black\", \"red\", \"darkblue\")[emlab])\n    }\n  )\n}"
  },
  {
    "objectID": "53-model-based-clustering.html#model-assumptions-may-be-important",
    "href": "53-model-based-clustering.html#model-assumptions-may-be-important",
    "title": "22  Model based clustering",
    "section": "22.3 Model assumptions may be important",
    "text": "22.3 Model assumptions may be important\nWe will illustrate the problem with a synthetic data set. There are 3 groups with 300 observations in each, and 3 variables / features.\n\n# sample size\nn &lt;- 300\n\n# covariance matrices for two of the groups\ns1 &lt;- matrix(c(2, -1, -1, -1, 2, 1, -1, 1, 1), ncol = 3, byrow = TRUE)\ns2 &lt;- matrix(c(4, 0, -1, 0, 4, 3, -1, 3, 5), ncol = 3, byrow = TRUE)\ns1.sqrt &lt;- chol(s1)\ns2.sqrt &lt;- chol(s2)\n\n# easy case, well separated groups\nset.seed(31)\nx1 &lt;- matrix(rnorm(n * 3), n, 3) %*% s1.sqrt\nmu2 &lt;- c(8, 8, 3)\nx2 &lt;- scale(matrix(rnorm(n * 3), n, 3) %*% s2.sqrt, center = -mu2, scale = FALSE)\nmu3 &lt;- c(-5, -5, -10)\nx3 &lt;- scale(matrix(rnorm(n * 3), n, 3), center = -mu3, scale = FALSE)\nx &lt;- rbind(x1, x2, x3)\n\nThis is how the data look\n\npairs(x, col = \"gray\", pch = 19)\n\n\n\n\n\n\n\n\nIt is not a surprise that model-based clustering works very well in this case:\n\nlibrary(mclust)\n# select the number of clusters using likelihood-base criterion\nm &lt;- Mclust(x)\n# show the data, color-coded according to the groups found\npairs(x, col = m$class)\n\n\n\n\n\n\n\n\nWe now create a data set that does not satisfy the model:\n\nset.seed(31)\nx1 &lt;- matrix(rexp(n * 3, rate = .2), n, 3)\nmu2 &lt;- c(10, 20, 20)\nx2 &lt;- scale(matrix(runif(n * 3, min = -6, max = 6), n, 3), center = -mu2, scale = FALSE)\nmu3 &lt;- c(-5, -5, -5)\nx3 &lt;- scale(matrix(rnorm(n * 3, sd = 3), n, 3), center = -mu3, scale = FALSE)\nx.3 &lt;- rbind(x1, x2, x3)\n\n# run model-based clustering,\n# select the number of clusters using likelihood-base criterion\nm3 &lt;- Mclust(x.3)\n\n# show the data, colors according to groups found\npairs(x.3, col = m3$class)\n\n\n\n\n\n\n\n\nThe problem is with the likelihood-based criterion used by mclust() to select the number of clusters. Note that the function increases until k = 3, and it almost stops growing after k = 4. The the maximum is nonetheless attained at k = 8.\n\nplot(m3$BIC[, 6], type = \"b\", xlab = \"K\", ylab = \"BIC\", lwd = 2, pch = 19)\n\n\n\n\n\n\n\n\nIt is interesting to note that K-means would have found the right number of clusters and cluster memberships rather easily. Here is the sum-of-squares plot based on K-means, which indicates that K = 3 is a sensible choice:\n\n# run k-means with k = 2, 2, ..., 10\n# to try to identify how many clusters are present\nm3.l &lt;- vector(\"list\", 10)\nss &lt;- rep(0, 10)\nfor (i in 2:10) {\n  ss[i] &lt;- sum((m3.l[[i]] &lt;- kmeans(x.3, centers = i, nstart = 500))$within)\n}\nplot(2:10, ss[-1], xlab = \"K\", ylab = \"W_k\", type = \"b\", lwd = 2, pch = 19)\n\n\n\n\n\n\n\n\nThe clusters found when K-means was run with kK = 3 were:\n\npairs(x.3, col = m3.l[[3]]$cluster)\n\n\n\n\n\n\n\n\nFurthermore, if you force mclust() to use 3 classes it works fairly well, even thought the model is wrong. The main problem here is that BIC depends heavily on the assumed likelihood / probabilistic model:\n\nm3.3 &lt;- Mclust(x.3, G = 3)\npairs(x.3, col = m3.3$class)"
  },
  {
    "objectID": "53-model-based-clustering.html#behaviour-when-there-are-noise-variables",
    "href": "53-model-based-clustering.html#behaviour-when-there-are-noise-variables",
    "title": "22  Model based clustering",
    "section": "22.4 Behaviour when there are noise variables",
    "text": "22.4 Behaviour when there are noise variables\nThe presence of noise variables (i.e. features that are non-informative about clusters that may be present in the data) can be quite damaging to these methods (both K-means and mclust) We will create two data sets with “noise” features: one with Gaussian noise, and one with uniformly distributed noise.\n\nset.seed(31)\nx1 &lt;- matrix(rnorm(n * 3, mean = 3), n, 3) %*% s1.sqrt\nmu2 &lt;- c(9, 9, 3)\nx2 &lt;- scale(matrix(rnorm(n * 3), n, 3) %*% s2.sqrt, center = -mu2, scale = FALSE)\nmu3 &lt;- c(5, 5, -10)\nx3 &lt;- scale(matrix(rnorm(n * 3), n, 3), center = -mu3, scale = FALSE)\nx &lt;- rbind(x1, x2, x3)\n# non-normal \"noise\" features\nx.4 &lt;- cbind(x, matrix(rexp(n * 3 * 3, rate = 1 / 10), n * 3, 3))\n# normal \"noise\" features\nx.5 &lt;- cbind(x, matrix(rnorm(n * 3 * 3, mean = 0, sd = 150), n * 3, 3))\n\nWe now find clusters using a Gaussian model, and select the number of clusters using likelihood-base criterion:\n\nm4 &lt;- Mclust(x.4)\nm5 &lt;- Mclust(x.5)\n\nIf we use the first 3 features (which are the ones that determine the cluster structure) to show the clusters found by mclust when the noise was not Gaussian, we get:\n\npairs(x.4[, 1:3], col = m4$class, pch = 19)\n\n\n\n\n\n\n\n\nAnd even when the noise had a Gaussian distribution, we do not identify the ``right’’ clusters:\n\n# pairs(x.5[,1:3], col=m5$class, pch=19)\ntable(m5$class, rep(1:3, each = n))\n#&gt;    \n#&gt;       1   2   3\n#&gt;   1 300   1   0\n#&gt;   2   0 299   0\n#&gt;   3   0   0 300\n\nIf we force mclust() to identify 3 clusters, things look much better both for Gaussian and non-Gaussian noise:\n\nm4.3 &lt;- Mclust(x.4, G = 3)\nm5.3 &lt;- Mclust(x.5, G = 3)\n# it works well\npairs(x.4[, 1:3], col = m4.3$class, pch = 19)\n\n\n\n\n\n\n\npairs(x.5[, 1:3], col = m5.3$class, pch = 19)\n\n\n\n\n\n\n\n\n\ntable(m4.3$class, rep(1:3, each = n))\n#&gt;    \n#&gt;       1   2   3\n#&gt;   1 300   5   0\n#&gt;   2   0 295   0\n#&gt;   3   0   0 300\ntable(m5.3$class, rep(1:3, each = n))\n#&gt;    \n#&gt;       1   2   3\n#&gt;   1 300   1   0\n#&gt;   2   0 299   0\n#&gt;   3   0   0 300\n\nNote that noise also affects K-means seriously. I refer you to the robust and sparse K-means method (links on the module’s main page).\nWithin sum-of-squares plot for K-means with non-Gaussian noise:\n\nm4.l &lt;- vector(\"list\", 10)\nss &lt;- rep(0, 10)\nfor (i in 2:10) {\n  ss[i] &lt;- sum((m4.l[[i]] &lt;- kmeans(x.4, centers = i, nstart = 100, iter.max = 20))$within)\n}\nplot(2:10, ss[-1], xlab = \"K\", ylab = \"W_k\", type = \"b\", lwd = 2, pch = 19)\n\n\n\n\n\n\n\n\nWithin sum-of-squares plot for K-means with Gaussian noise:\n\nm5.l &lt;- vector(\"list\", 10)\nss &lt;- rep(0, 10)\nfor (i in 2:10) {\n  ss[i] &lt;- sum((m5.l[[i]] &lt;- kmeans(x.5, centers = i, nstart = 100, iter.max = 20))$within)\n}\nplot(2:10, ss[-1], xlab = \"K\", ylab = \"W_k\", type = \"b\", lwd = 2, pch = 19)\n\n\n\n\n\n\n\n\nNot even forcing k-means to identify 3 clusters helps when there are noise features:\n\npairs(x.4[, 1:3], col = m4.l[[3]]$cluster, pch = 19)\n\n\n\n\n\n\n\npairs(x.5[, 1:3], col = m5.l[[3]]$cluster, pch = 19)"
  },
  {
    "objectID": "53-model-based-clustering.html#imputation-via-em-a-detailed-example-by-hand",
    "href": "53-model-based-clustering.html#imputation-via-em-a-detailed-example-by-hand",
    "title": "22  Model based clustering",
    "section": "22.5 Imputation via EM (a detailed example “by hand”)",
    "text": "22.5 Imputation via EM (a detailed example “by hand”)\nMissing data is a rather prevalent problem, and different strategies to replace them by sensible “predictions” exit. They are collectively called “imputation methods”. In these notes we will follow the missing data example discussed in class and use the EM algorithm to impute partially unobserved data points in a synthetic bivariate Gaussian data set. Furthemore, the scripts below are designed for the case where only one entry may be missing in each observation. It is not difficult to extend this to data with more coordinates and more than one entry missing. Please refer to your class notes for formulas and details.\n\n22.5.1 A synthetic example\nTo illustrate the method in a simple setting where we can visualize the ideas on a 2-dimensional scatter plot, we will work with a toy example. We first create a simple synthetic data set with 50 observations in 2 dimensions, normally distributed with center at the point (3,7), and a fairly strong correlation between its two coordinates:\n\nlibrary(mvtnorm)\n# mean vector\nmu &lt;- c(3, 7)\n# variance/covariance matrix\nsi &lt;- matrix(c(1, 1.2, 1.2, 2), 2, 2)\n# generate data\nset.seed(123)\nx &lt;- rmvnorm(50, mean = mu, sigma = si)\n\nThis is the data. The larger red point indicates the sample mean (3.13, 7.15):\n\nplot(x, pch = 19, col = \"gray30\", xlim = c(0, 8), ylim = c(0, 13), xlab = \"X1\", ylab = \"X2\", cex = 1.5)\nxbar &lt;- colMeans(x)\npoints(xbar[1], xbar[2], pch = 19, col = \"tomato\", cex = 2)\n\n\n\n\n\n\n\n\nAssume we have an observation (5, NA) where the second coordinate is missing, and another one (NA, 5.5) with the first coordinate missing. We indicate them with grey lines to indicate the uncertainty about their missing entries:\n\nplot(x, pch = 19, col = \"gray30\", xlim = c(0, 8), ylim = c(0, 13), xlab = \"X1\", ylab = \"X2\", cex = 1.5)\nabline(v = 5, lwd = 6, col = \"gray80\")\nabline(h = 5.5, lwd = 6, col = \"gray80\")\npoints(x, pch = 19, col = \"gray30\", cex = 1.5)\npoints(xbar[1], xbar[2], pch = 19, col = \"tomato\", cex = 2)\ntext(1, 6, \"(NA, 5.5)\")\ntext(6, 2, \"(5, NA)\")\n\n\n\n\n\n\n\n\nA simple method to impute the missing coordinates would be to replace them by the mean of the missing variable over the rest of the data. Hence (5, NA) becomes (5, 7.15) and (NA, 5.5) becomes (3.13, 5.5). The imputed points are shown below as blue dots:\n\nplot(x, pch = 19, col = \"gray30\", xlim = c(0, 8), ylim = c(0, 13), xlab = \"X1\", ylab = \"X2\", cex = 1.5)\nabline(h = 5.5, lwd = 6, col = \"gray80\")\nabline(v = 5, lwd = 6, col = \"gray80\")\npoints(x, pch = 19, col = \"gray30\", cex = 1.5)\npoints(xbar[1], xbar[2], pch = 19, col = \"tomato\", cex = 2)\npoints(5, xbar[2], pch = 19, col = \"steelblue\", cex = 2)\npoints(xbar[1], 5.5, pch = 19, col = \"steelblue\", cex = 2)\n\n\n\n\n\n\n\n\nNote that the imputed points are in fact away from the bulk of the data, even though this is not apparent if you look at each coordinate separately. A better imputation method uses the EM algorithm.\nWe assume that the points in our data can be modelled as occurences of a bivariate random vector with a normal / Gaussian distribution. The unknown parameters are its mean vector and 2x2 variance/covariance matrix. The EM algorithm will alternate between computing the expected value of the log-likelihood for the full (non-missing) data set conditional on the actually observed points (even incompletely observed ones), and finding the parameters (mean vector and covariance matrix) that maximize this conditional expected log-likelihood.\nIt is not trivial to see that the conditional expected log-likelihood equals a constant (that depends only on the parameters from the previous iteration) plus the log-likelihood of a data set where the missing coordinates of each observation are replaced by their conditional expectation (given the observed entries in the same unit). Refer to the discussion in class for more details.\nWe now implement this imputation method in R. First add the two incomplete observations to the data set above, we append them at the “bottom” of the matrix x:\n\nset.seed(123)\ndat &lt;- rbind(x, c(5, NA), c(NA, 5.5))\n\nNext, we compute initial values for the estimates of the parameters of the model. These can be, for example, the sample mean and sample covariance matrix using only the fully observed data points:\n\nmu &lt;- colMeans(dat, na.rm = TRUE)\nsi &lt;- var(dat, na.rm = TRUE)\n\nBefore we start the EM iterations it will be helpful to keep track of wich observations are missing a coordinate (we store their indices in the vector mi):\n\nn &lt;- nrow(dat)\np &lt;- 2\n# find observations with a missing coordinate\nmi &lt;- (1:n)[!complete.cases(dat)]\n\nOut of the n (52) rows in x, the ones with some missing coordinates are: 51, 52.\nNow we run 100 iterations of the EM algorithm, although convergence is achieved much sooner:\n\n# For this data we don't need many iterations\nniter &lt;- 100\n# how many observations with missing entries:\nlen.mi &lt;- length(mi)\n# Start the EM iterations\nfor (i in 1:niter) {\n  # E step\n  # impute the data points with missing entries\n  for (h in 1:len.mi) {\n    # which entries are not missing?\n    nm &lt;- !is.na(dat[mi[h], ])\n    dat[mi[h], !nm] &lt;- mu[!nm] + si[!nm, nm] * solve(si[nm, nm], dat[mi[h], nm] - mu[nm])\n  }\n  # M step, luckily we have a closed form for the maximizers of the\n  # conditional expected likelihood\n  mu &lt;- colMeans(dat)\n  si &lt;- var(dat)\n}\n\nThe imputed data are now much more in line with the shape and distribution of the other points in the data set:\n\nplot(x, pch = 19, col = \"gray30\", xlim = c(0, 8), ylim = c(0, 13), xlab = \"X1\", ylab = \"X2\", cex = 1.5)\nabline(h = 5.5, lwd = 6, col = \"gray80\")\nabline(v = 5, lwd = 6, col = \"gray80\")\npoints(x, pch = 19, col = \"gray30\", cex = 1.5)\npoints(xbar[1], xbar[2], pch = 19, col = \"tomato\", cex = 2)\nfor (h in 1:length(mi)) points(dat[mi[h], 1], dat[mi[h], 2], pch = 19, col = \"steelblue\", cex = 2)"
  },
  {
    "objectID": "54-hclust.html#breweries-example",
    "href": "54-hclust.html#breweries-example",
    "title": "23  Hierarchical clustering",
    "section": "23.1 Breweries example",
    "text": "23.1 Breweries example\nBeer drinkers were asked to rate 9 breweries on 26 attributes. The attributes were, e.g., Brewery has rich tradition; or Brewery makes very good Pils beer. Relative to each attribute, the informant had to assign each brewery a score on a 6-point scale ranging from 1=not true at all to 6=very true. We read the data, and use the function dist to compute the pairwise L_1 distances between the 9 breweries. Note that the data are available columnwise (\\(p \\times x\\)) so we first transpose it before we compute the distances. We also change the misleading column names assigned by read.table, which are not features but rather observation numbers:\n\nx &lt;- read.table(\"data/breweries.dat\", header = FALSE)\ncolnames(x) &lt;- paste0(\"Brew-\", 1:ncol(x))\nx &lt;- t(x)\nd &lt;- dist(x, method = \"manhattan\")\n\nOne implementation of hierarchical clustering methods in R is in the function hclust in package cluster. We first use Ward’s information criterion (corrected to appropriately use squared distances). The plot method for objects of class hclust produces the associated dendogram. The function rect.hclust computes the height at which one shuld cut the dendogram to obtain a desired number k of clusters. Below we show the result for K = 3 clusters:\n\n# hierarchical\nlibrary(cluster)\n# show the dendogram\nplot(cl &lt;- hclust(d, method = \"ward.D2\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\n# identify 3 clusters\nrect.hclust(cl, k = 3, border = \"red\")\n\n\n\n\n\n\n\n\nNow we repeat the analysis but using Euclidean distances and single linkage, and show K = 3 clusters:\n\nbr.dis &lt;- dist(x) # L2\nbr.hc &lt;- hclust(br.dis, method = \"single\")\nplot(br.hc)\nbr.hc.3 &lt;- rect.hclust(br.hc, k = 3)\n\n\n\n\n\n\n\n\nNote how these 3 clusters are somewhat different from the ones found before. However, the (V1, V4, V7) cluster is present in both partitions, and also the triplet (V3, V6, V8) stays together as well. It is interesting to compare these clusters with those found by K-means (see previous notes), in particular, these dendograms resemble the information on the silhouette plots to some extent."
  },
  {
    "objectID": "54-hclust.html#languages-example",
    "href": "54-hclust.html#languages-example",
    "title": "23  Hierarchical clustering",
    "section": "23.2 Languages example",
    "text": "23.2 Languages example\nThe details of this example were discussed in class. Here we present the results of three commonly used merging criteria: single linkage, complete linkage, average linkage, and Ward’s criterion. As usual, we start by reading the data, which in this case are the specific dissimilarities between languages discussed in class, and we arrange them in a matrix that can be used by hclust:\n\ndd &lt;- read.table(\"data/languages.dat\", header = FALSE)\nnames(dd) &lt;- c(\"E\", \"N\", \"Da\", \"Du\", \"G\", \"Fr\", \"S\", \"I\", \"P\", \"H\", \"Fi\")\ndd &lt;- (dd + t(dd) / 2)\nd &lt;- as.dist(dd)\n\nNow we compute a hierarchical clustering sequence using single linkage, plot the corresponding dendogram and identify 4 clusters:\n\nplot(cl &lt;- hclust(d, method = \"single\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\nrect.hclust(cl, k = 4, border = \"red\")\n\n\n\n\n\n\n\n\nCompare the above with the results obtained with complete linkage:\n\nplot(cl &lt;- hclust(d, method = \"complete\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\nrect.hclust(cl, k = 4, border = \"red\")\n\n\n\n\n\n\n\n\nWith average linkage we obtain:\n\nplot(cl &lt;- hclust(d, method = \"average\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\nrect.hclust(cl, k = 4, border = \"red\")\n\n\n\n\n\n\n\n\nAnd finally, using Ward’s criterion results in the following dendogram and 4 clusters:\n\nplot(cl &lt;- hclust(d, method = \"ward.D2\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\nrect.hclust(cl, k = 4, border = \"red\")"
  },
  {
    "objectID": "54-hclust.html#cancer-example",
    "href": "54-hclust.html#cancer-example",
    "title": "23  Hierarchical clustering",
    "section": "23.3 Cancer example",
    "text": "23.3 Cancer example\nHere we revisit the Cancer example discussed before. We use Euclidean distances and Ward’s information criterion. Below we show the clusters identified when we stop the algorithm at K = 8, which based on the dendogram seems to be a reasonable choice:\n\ndata(nci, package = \"ElemStatLearn\")\nnci.dis &lt;- dist(t(nci), method = \"euclidean\")\nplot(nci.hc.w &lt;- hclust(nci.dis, method = \"ward.D2\"),\n  main = \"\",\n  xlab = \"\", sub = \"\", hang = -1, labels = rownames(nci)\n)\nrect.hclust(nci.hc.w, k = 8, border = \"red\")\n\n\n\n\n\n\n\n\nFor completeness, below we show the results obtained with the other linkage criteria, including Ward’s:\n\nnci.hc.s &lt;- hclust(nci.dis, method = \"single\")\nnci.hc.c &lt;- hclust(nci.dis, method = \"complete\")\nnci.hc.a &lt;- hclust(nci.dis, method = \"average\")\n\n# plot them\nplot(nci.hc.s, labels = colnames(nci), cex = .5)\n\n\n\n\n\n\n\n\nplot(nci.hc.c, labels = colnames(nci), cex = .5)\n\n\n\n\n\n\n\n\nplot(nci.hc.a, labels = colnames(nci), cex = .5)\n\n\n\n\n\n\n\n\nNote that with these 3 other criteria no clear structure seems apparent in the data."
  },
  {
    "objectID": "54-hclust.html#nations-example",
    "href": "54-hclust.html#nations-example",
    "title": "23  Hierarchical clustering",
    "section": "23.4 Nations example",
    "text": "23.4 Nations example\nThis is a smaller Political Science dataset. Twelve countries were assessed on their perceived “likeness” by Political Science students. Note that (as in the Languages example above) in this example we do not have raw observations (features), we only have access to the already determined parwise dissimilarities. Below we show the results of using hierarchical clustering with complete and average linkage merging criteria, which produce identical clusters. You are encouraged to investigate what can be found with other merging criteria.\n\n# read the pairwise dissimilarities\na2 &lt;- read.table(\"data/nations2.dat\", header = FALSE)\n\n# since only the lower triangular matrix is available\n# we need to copy it on the upper half\na2 &lt;- a2 + t(a2)\n\n# create a vector of country names, to be used later\nnams2 &lt;- c(\n  \"BEL\", \"BRA\", \"CHI\", \"CUB\", \"EGY\", \"FRA\",\n  \"IND\", \"ISR\", \"USA\", \"USS\", \"YUG\", \"ZAI\"\n)\n\n# compute hierarchical clustering using complete linkage\nna.hc &lt;- hclust(as.dist(a2), method = \"complete\")\nplot(na.hc, labels = nams2)\n\n\n\n\n\n\n\n\n# compute hierarchical clustering using average linkage\nna.hc &lt;- hclust(as.dist(a2), method = \"average\")\nplot(na.hc, labels = nams2)"
  },
  {
    "objectID": "54-hclust.html#un-votes",
    "href": "54-hclust.html#un-votes",
    "title": "23  Hierarchical clustering",
    "section": "23.5 UN Votes",
    "text": "23.5 UN Votes\nWe revisit here the UN votes example (see Lecture 19). Using Euclidean distances and Ward’s criterion we obtain the following 3 clusters:\n\nX &lt;- read.table(file = \"data/unvotes.csv\", sep = \",\", row.names = 1, header = TRUE)\nun.dis &lt;- dist(t(X), method = \"euclidean\")\nun.hc &lt;- hclust(un.dis, method = \"ward.D2\")\nplot(un.hc, cex = .5)\nun.hc.3 &lt;- rect.hclust(un.hc, k = 3)\n\n\n\n\n\n\n\nlapply(un.hc.3, names)\n#&gt; [[1]]\n#&gt;  [1] \"Australia\"   \"Austria\"     \"Belgium\"     \"Bulgaria\"    \"Canada\"     \n#&gt;  [6] \"Denmark\"     \"Finland\"     \"France\"      \"Greece\"      \"Hungary\"    \n#&gt; [11] \"Iceland\"     \"Ireland\"     \"Israel\"      \"Italy\"       \"Japan\"      \n#&gt; [16] \"Luxembourg\"  \"Netherlands\" \"New.Zealand\" \"Norway\"      \"Poland\"     \n#&gt; [21] \"Portugal\"    \"Spain\"       \"Sweden\"      \"UK\"          \"Ukraine\"    \n#&gt; [26] \"USA\"        \n#&gt; \n#&gt; [[2]]\n#&gt;  [1] \"Argentina\"  \"Bahamas\"    \"Chile\"      \"Colombia\"   \"Costa.Rica\"\n#&gt;  [6] \"Cyprus\"     \"Malta\"      \"Mexico\"     \"Panama\"     \"Paraguay\"  \n#&gt; [11] \"Peru\"       \"Uruguay\"   \n#&gt; \n#&gt; [[3]]\n#&gt;  [1] \"Algeria\"              \"Bangladesh\"           \"Belarus\"             \n#&gt;  [4] \"Bolivia\"              \"Botswana\"             \"Brazil\"              \n#&gt;  [7] \"Brunei.Darussalam\"    \"Burkina.Faso\"         \"China\"               \n#&gt; [10] \"Cuba\"                 \"Ecuador\"              \"Egypt\"               \n#&gt; [13] \"Ethiopia\"             \"Ghana\"                \"Guyana\"              \n#&gt; [16] \"India\"                \"Indonesia\"            \"Jamaica\"             \n#&gt; [19] \"Jordan\"               \"Kenya\"                \"Kuwait\"              \n#&gt; [22] \"Libya\"                \"Malaysia\"             \"Mali\"                \n#&gt; [25] \"Nepal\"                \"Nigeria\"              \"Pakistan\"            \n#&gt; [28] \"Philippines\"          \"Russian.Federation\"   \"Singapore\"           \n#&gt; [31] \"Sri.Lanka\"            \"Sudan\"                \"Syrian.Arab.Republic\"\n#&gt; [34] \"Tanzania\"             \"Thailand\"             \"Togo\"                \n#&gt; [37] \"Trinidad.and.Tobago\"  \"Venezuela\"            \"Zambia\"\n\nIf we repeat the same exercise but using \\(L_1\\) distances we obtain different clusters.\n\nun.dis.l1 &lt;- dist(t(X), method = \"manhattan\")\nun.hc.l1 &lt;- hclust(un.dis.l1, method = \"ward.D2\")\nplot(un.hc.l1, cex = .5)\nun.hc.l1.3 &lt;- rect.hclust(un.hc.l1, k = 3)\n\n\n\n\n\n\n\nlapply(un.hc.l1.3, names)\n#&gt; [[1]]\n#&gt;  [1] \"Australia\"   \"Austria\"     \"Belgium\"     \"Bulgaria\"    \"Canada\"     \n#&gt;  [6] \"Cyprus\"      \"Denmark\"     \"Finland\"     \"France\"      \"Greece\"     \n#&gt; [11] \"Hungary\"     \"Iceland\"     \"Ireland\"     \"Israel\"      \"Italy\"      \n#&gt; [16] \"Japan\"       \"Luxembourg\"  \"Malta\"       \"Netherlands\" \"New.Zealand\"\n#&gt; [21] \"Norway\"      \"Poland\"      \"Portugal\"    \"Spain\"       \"Sweden\"     \n#&gt; [26] \"UK\"          \"Ukraine\"     \"USA\"        \n#&gt; \n#&gt; [[2]]\n#&gt;  [1] \"Algeria\"              \"Bangladesh\"           \"Belarus\"             \n#&gt;  [4] \"Brunei.Darussalam\"    \"China\"                \"Cuba\"                \n#&gt;  [7] \"Egypt\"                \"India\"                \"Indonesia\"           \n#&gt; [10] \"Jordan\"               \"Kuwait\"               \"Libya\"               \n#&gt; [13] \"Malaysia\"             \"Pakistan\"             \"Russian.Federation\"  \n#&gt; [16] \"Sri.Lanka\"            \"Sudan\"                \"Syrian.Arab.Republic\"\n#&gt; [19] \"Venezuela\"           \n#&gt; \n#&gt; [[3]]\n#&gt;  [1] \"Argentina\"           \"Bahamas\"             \"Bolivia\"            \n#&gt;  [4] \"Botswana\"            \"Brazil\"              \"Burkina.Faso\"       \n#&gt;  [7] \"Chile\"               \"Colombia\"            \"Costa.Rica\"         \n#&gt; [10] \"Ecuador\"             \"Ethiopia\"            \"Ghana\"              \n#&gt; [13] \"Guyana\"              \"Jamaica\"             \"Kenya\"              \n#&gt; [16] \"Mali\"                \"Mexico\"              \"Nepal\"              \n#&gt; [19] \"Nigeria\"             \"Panama\"              \"Paraguay\"           \n#&gt; [22] \"Peru\"                \"Philippines\"         \"Singapore\"          \n#&gt; [25] \"Tanzania\"            \"Thailand\"            \"Togo\"               \n#&gt; [28] \"Trinidad.and.Tobago\" \"Uruguay\"             \"Zambia\"\n\nIt is easier to compare these 2 sets of clusters if we show them on a map. We first find the cluster labels corresponding to 3 clusters using Euclidean and \\(L_1\\) distances:\n\nlabs &lt;- cutree(un.hc, k = 3)\nlabs.l1 &lt;- cutree(un.hc.l1, k = 3)\n\nWe can now use these labels to color a map, as we did previously. For the Euclidean distances we obtain:\n\nlibrary(rworldmap)\nlibrary(countrycode)\nthese &lt;- countrycode(colnames(X), \"country.name\", \"iso3c\")\nmalDF &lt;- data.frame(country = these, cluster = labs)\nmalMap &lt;- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap,\n  nameColumnToPlot = \"cluster\", catMethod = \"categorical\",\n  missingCountryCol = \"white\", addLegend = FALSE, mapTitle = \"\",\n  colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\", \"blueviolet\", \"yellow\"),\n  oceanCol = \"dodgerblue\"\n)\n\n\n\n\n\n\n\n\nWhile with the \\(L_1\\) distances we get:\n\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap,\n  nameColumnToPlot = \"cluster\", catMethod = \"categorical\",\n  missingCountryCol = \"white\", addLegend = FALSE, mapTitle = \"\",\n  colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\", \"blueviolet\", \"yellow\"),\n  oceanCol = \"dodgerblue\"\n)\n\n\n\n\n\n\n\n\nRecall that, as discussed in class, the analyses above may be questionable, because these distance measures do not take into account the actual nature of the available features."
  },
  {
    "objectID": "80-references.html",
    "href": "80-references.html",
    "title": "References",
    "section": "",
    "text": "Alfaro, Esteban, Matías Gámez, and Noelia García. 2013. “adabag: An R Package for\nClassification with Boosting and Bagging.” Journal of\nStatistical Software 54 (2): 1–35. https://doi.org/10.18637/jss.v054.i02.\n\n\nAlfaro, Esteban; Gamez, Matias, Garcia, Noelia; with contributions from\nL. Guo, A. Albano, M. Sciandra, and A. Plaia. 2023. Adabag: Applies\nMulticlass AdaBoost.M1, SAMME and Bagging. https://CRAN.R-project.org/package=adabag.\n\n\nAllaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier\nLuraschi, Kevin Ushey, Aron Atkins, et al. 2023. Rmarkdown: Dynamic\nDocuments for r. https://CRAN.R-project.org/package=rmarkdown.\n\n\nAmbroise, Christophe, and Geoffrey J. McLachlan. 2002. “Selection\nBias in Gene Extraction on the Basis of Microarray Gene-Expression\nData.” Proceedings of the National Academy of Sciences\n99 (10): 6562–66. https://doi.org/10.1073/pnas.102102699.\n\n\nArel-Bundock, Vincent. 2023. Countrycode: Convert Country Names and\nCountry Codes. https://vincentarelbundock.github.io/countrycode/.\n\n\nArel-Bundock, Vincent, Nils Enevoldsen, and CJ Yetman. 2018.\n“Countrycode: An r Package to Convert Country Names and Country\nCodes.” Journal of Open Source Software 3 (28): 848. https://doi.org/10.21105/joss.00848.\n\n\nBoente, Graciela, Alejandra Martínez, and Matías Salibián-Barrera. 2017.\n“Robust Estimators for Additive Models Using Backfitting.”\nJournal of Nonparametric Statistics 29 (4): 744–67. https://doi.org/10.1080/10485252.2017.1369077.\n\n\nBreiman, Leo, Adele Cutler, Andy Liaw, and Matthew Wiener. 2022.\nrandomForest: Breiman and Cutler’s Random Forests for Classification\nand Regression. https://www.stat.berkeley.edu/~breiman/RandomForests/.\n\n\nCsárdi, Gábor, Kirill Müller, and Jim Hester. 2022. Desc: Manipulate\nDESCRIPTION Files. https://CRAN.R-project.org/package=desc.\n\n\nDolnicar, Sara, Bettina Gruen, and Friedrich Leisch. 2018. Market\nSegmentation Analysis — Understanding It, Doing It, and Making It\nUseful. Singapore: Springer. https://doi.org/10.1007/978-981-10-8818-6.\n\n\nEfron, Bradley. 1986. “How Biased Is the Apparent Error Rate of a\nPrediction Rule?” Journal of the American Statistical\nAssociation 81 (394): 461–70.\n\n\nFraley, Chris, Adrian E. Raftery, and Luca Scrucca. 2022. Mclust:\nGaussian Mixture Modelling for Model-Based Clustering, Classification,\nand Density Estimation. https://mclust-org.github.io/mclust/.\n\n\nFriedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2000.\n“Additive logistic regression: a statistical\nview of boosting (With discussion and a rejoinder by the\nauthors).” The Annals of Statistics 28 (2):\n337–407. https://doi.org/10.1214/aos/1016218223.\n\n\nFriedman, Jerome, Trevor Hastie, Rob Tibshirani, Balasubramanian\nNarasimhan, Kenneth Tay, Noah Simon, and James Yang. 2023. Glmnet:\nLasso and Elastic-Net Regularized Generalized Linear Models. https://glmnet.stanford.edu.\n\n\nFriedman, Jerome, Robert Tibshirani, and Trevor Hastie. 2010.\n“Regularization Paths for Generalized Linear Models via Coordinate\nDescent.” Journal of Statistical Software 33 (1): 1–22.\nhttps://doi.org/10.18637/jss.v033.i01.\n\n\nGenz, Alan, and Frank Bretz. 2009. Computation of Multivariate\nNormal and t Probabilities. Lecture Notes in Statistics.\nHeidelberg: Springer-Verlag.\n\n\nGenz, Alan, Frank Bretz, Tetsuhisa Miwa, Xuefei Mi, and Torsten Hothorn.\n2023. Mvtnorm: Multivariate Normal and t Distributions. http://mvtnorm.R-forge.R-project.org.\n\n\nHalvorsen, Kjetil. 2019. ElemStatLearn: Data Sets, Functions and\nExamples from the Book: \"The Elements of Statistical Learning, Data\nMining, Inference, and Prediction\" by Trevor Hastie, Robert Tibshirani\nand Jerome Friedman. http://www-stat.stanford.edu/~tibs/ElemStatLearn/.\n\n\nHastie, Trevor, and Brad Efron. 2022. Lars: Least Angle Regression,\nLasso and Forward Stagewise. https://doi.org/10.1214/009053604000000067.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2022.\nISLR2: Introduction to Statistical Learning, Second Edition. https://www.statlearning.com.\n\n\nKassambara, Alboukadel. 2022. Ggcorrplot: Visualization of a\nCorrelation Matrix Using Ggplot2. http://www.sthda.com/english/wiki/ggcorrplot-visualization-of-a-correlation-matrix-using-ggplot2.\n\n\nLeisch, Friedrich. 2006. “A Toolbox for k-Centroids Cluster\nAnalysis.” Computational Statistics and Data Analysis 51\n(2): 526–44.\n\n\n———. 2010. “Neighborhood Graphs, Stripes and Shadow Plots for\nCluster Visualization.” Statistics and Computing 20:\n457–69. https://doi.org/10.1007/s11222-009-9137-8.\n\n\n———. 2022. Flexclust: Flexible Cluster Algorithms. https://CRAN.R-project.org/package=flexclust.\n\n\nLeisch, Friedrich, and Bettina Gruen. 2006. “Extending Standard\nCluster Algorithms to Allow for Group Constraints.” In\nCompstat 2006—Proceedings in Computational Statistics, edited\nby Alfredo Rizzi and Maurizio Vichi, 885–92. Physica Verlag, Heidelberg,\nGermany.\n\n\nLiaw, Andy, and Matthew Wiener. 2002. “Classification and\nRegression by randomForest.” R News 2 (3): 18–22. https://CRAN.R-project.org/doc/Rnews/.\n\n\nLumley, Thomas. 2020. Leaps: Regression Subset Selection. https://CRAN.R-project.org/package=leaps.\n\n\nMaechler, Martin, Peter Rousseeuw, Anja Struyf, and Mia Hubert. 2022.\nCluster: \"Finding Groups in Data\": Cluster Analysis Extended\nRousseeuw Et Al. https://svn.r-project.org/R-packages/trunk/cluster/.\n\n\nMüller, Kirill, and Hadley Wickham. 2023. Tibble: Simple Data\nFrames. https://CRAN.R-project.org/package=tibble.\n\n\nRipley, Brian. 2023a. Class: Functions for Classification. http://www.stats.ox.ac.uk/pub/MASS4/.\n\n\n———. 2023b. MASS: Support Functions and Datasets for Venables and\nRipley’s MASS. http://www.stats.ox.ac.uk/pub/MASS4/.\n\n\n———. 2023c. Nnet: Feed-Forward Neural Networks and Multinomial\nLog-Linear Models. http://www.stats.ox.ac.uk/pub/MASS4/.\n\n\n———. 2023d. Tree: Classification and Regression Trees. https://CRAN.R-project.org/package=tree.\n\n\nScharl, Theresa, and Friedrich Leisch. 2006. “The Stochastic\nQT–Clust Algorithm: Evaluation of Stability and Variance on Time–Course\nMicroarray Data.” In Compstat 2006—Proceedings in\nComputational Statistics, edited by Alfredo Rizzi and Maurizio\nVichi, 1015–22. Physica Verlag, Heidelberg, Germany.\n\n\nScrucca, Luca, Michael Fop, T. Brendan Murphy, and Adrian E. Raftery.\n2016. “mclust 5: Clustering,\nClassification and Density Estimation Using Gaussian Finite\nMixture Models.” The R Journal 8 (1):\n289–317. https://doi.org/10.32614/RJ-2016-021.\n\n\nSievert, Carson, Joe Cheng, and Garrick Aden-Buie. 2023. Bslib:\nCustom Bootstrap ’Sass’ Themes for Shiny and Rmarkdown. https://CRAN.R-project.org/package=bslib.\n\n\nSimon, Noah, Jerome Friedman, Robert Tibshirani, and Trevor Hastie.\n2011. “Regularization Paths for Cox’s Proportional Hazards Model\nvia Coordinate Descent.” Journal of Statistical Software\n39 (5): 1–13. https://doi.org/10.18637/jss.v039.i05.\n\n\nSouth, Andy. 2011. “Rworldmap: A New r Package for Mapping Global\nData.” The R Journal 3 (1): 35–43. http://journal.r-project.org/archive/2011-1/RJournal_2011-1_South.pdf.\n\n\n———. 2016. Rworldmap: Mapping Global Data. https://CRAN.R-project.org/package=rworldmap.\n\n\nTay, J. Kenneth, Balasubramanian Narasimhan, and Trevor Hastie. 2023.\n“Elastic Net Regularization Paths for All Generalized Linear\nModels.” Journal of Statistical Software 106 (1): 1–31.\nhttps://doi.org/10.18637/jss.v106.i01.\n\n\nTharmaratnam, Kukatharmini, Gerda Claeskens, Christophe Croux, and\nMatias Salibián-Barrera. 2010. “S-Estimation for Penalized\nRegression Splines.” Journal of Computational and Graphical\nStatistics 19 (3): 609–25. https://doi.org/10.1198/jcgs.2010.08149.\n\n\nTherneau, Terry, and Beth Atkinson. 2022. Rpart: Recursive\nPartitioning and Regression Trees. https://CRAN.R-project.org/package=rpart.\n\n\nTodorov, Valentin, and Peter Filzmoser. 2009. “An Object-Oriented\nFramework for Robust Multivariate Analysis.” Journal of\nStatistical Software 32 (3): 1–47. https://www.jstatsoft.org/article/view/v032i03/.\n\n\nTodorov, Valentin, Andreas Ruckstuhl, Matias Salibian-Barrera, Tobias\nVerbeke, Manuel Koller, and Martin Maechler. 2023. Robustbase: Basic\nRobust Statistics. https://robustbase.R-forge.R-project.org/.\n\n\nVenables, W. N., and B. D. Ripley. 2002b. Modern Applied Statistics\nwith s. Fourth. New York: Springer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\n———. 2002a. Modern Applied Statistics with s. Fourth. New York:\nSpringer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\n———. 2002c. Modern Applied Statistics with s. Fourth. New York:\nSpringer. https://www.stats.ox.ac.uk/pub/MASS4/.\n\n\nVoeten, Erik, Anton Strezhnev, and Michael Bailey. 2009.\n“United Nations General Assembly Voting Data.”\nHarvard Dataverse. https://doi.org/10.7910/DVN/LEJUQZ.\n\n\nWand, Matt. 2018. SemiPar: Semiparametic Regression. http://matt-wand.utsacademics.info/SPmanu.pdf.\n\n\n———. 2023. KernSmooth: Functions for Kernel Smoothing Supporting\nWand & Jones (1995). https://CRAN.R-project.org/package=KernSmooth.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2023a. Downlit: Syntax Highlighting and Automatic Linking.\nhttps://CRAN.R-project.org/package=downlit.\n\n\n———. 2023b. Tidyverse: Easily Install and Load the Tidyverse.\nhttps://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Winston Chang, Robert Flight, Kirill Müller, and Jim\nHester. 2021. Sessioninfo: R Session Information. https://CRAN.R-project.org/package=sessioninfo.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,\nKohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey\nDunnington. 2023. Ggplot2: Create Elegant Data Visualisations Using\nthe Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, and Dana Seidel. 2022. Scales: Scale Functions for\nVisualization. https://CRAN.R-project.org/package=scales.\n\n\nWood, Simon N. 2017. Generalized Additive Models: An Introduction\nwith r. CRC press.\n\n\nXie, Yihui. 2014. “Knitr: A Comprehensive Tool for Reproducible\nResearch in R.” In Implementing Reproducible\nComputational Research, edited by Victoria Stodden, Friedrich\nLeisch, and Roger D. Peng. Chapman; Hall/CRC.\n\n\n———. 2015. Dynamic Documents with R and Knitr. 2nd\ned. Boca Raton, Florida: Chapman; Hall/CRC. https://yihui.org/knitr/.\n\n\n———. 2016. Bookdown: Authoring Books and Technical Documents with\nR Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/bookdown.\n\n\n———. 2023a. Bookdown: Authoring Books and Technical Documents with r\nMarkdown. https://CRAN.R-project.org/package=bookdown.\n\n\n———. 2023b. formatR: Format r Code Automatically. https://github.com/yihui/formatR.\n\n\n———. 2023c. Knitr: A General-Purpose Package for Dynamic Report\nGeneration in r. https://yihui.org/knitr/.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown:\nThe Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R\nMarkdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.\n\n\nZou, Hui. 2006. “The Adaptive Lasso and Its Oracle\nProperties.” Journal of the American Statistical\nAssociation 101 (476): 1418–29.\n\n\nZou, Hui, and Trevor Hastie. 2005. “Regularization and Variable\nSelection via the Elastic Net.” Journal of the Royal\nStatistical Society: Series B (Statistical Methodology) 67 (2):\n301–20."
  },
  {
    "objectID": "90-pca-alternating-regression.html",
    "href": "90-pca-alternating-regression.html",
    "title": "Appendix A — PCA and alternating regression",
    "section": "",
    "text": "Let \\(X_1, \\ldots, X_n \\in \\mathbb{R}^p\\) be the observations for which we want to compute the corresponding PCA. Without loss of generality we can always assume that \\[\n\\frac{1}{n} \\sum_{i=1}^n X_i \\ = (0,\\ldots,0)^\\top \\, ,\n\\] so that the sample covariance matrix \\(S_n\\) is \\[\nS_n \\ = \\ \\frac{1}{n-1} \\, \\sum_{i=1}^n X_i \\, X_i^\\top \\, .\n\\] We saw in class that if \\(B \\in \\mathbb{R}^{p \\times k}\\) has in its columns the eigenvectors of \\(S_n\\) associated with its \\(k\\) largest eigenvalues, then \\[\n\\frac{1}{n} \\, \\sum_{i=1}^n \\left\\| X_i - P( L_{B}, X_i\n) \\right\\|^2 \\ \\le \\\n\\frac{1}{n} \\, \\sum_{i=1}^n \\left\\| X_i - P( L, X_i\n) \\right\\|^2 \\, ,\n\\] for any \\(k\\)-dimensional linear subspace \\(L \\subset \\mathbb{R}^p\\) where \\(P( L, X)\\) denotes the orthogonal projection of \\(X\\) onto the subspace \\(L\\), \\(P( L_{B}, X) = {B} {B}^\\top X\\) (whenever \\({B}\\) is chosen so that \\({B}^\\top {B} = I\\)) and \\(L_{B}\\) denotes the subspace spanned by the columns of \\(B\\).\nWe will show now that, instead of finding the spectral decomposition of \\(S_n\\), principal components can also be computed via a sequence of “alternating least squares” problems. To fix ideas we will consider the case \\(k=1\\), but the method is trivially extended to arbitrary values of \\(k\\).\nWhen \\(k=1\\) we need to solve the following problem \\[\\begin{equation}\n\\min_{\\left\\| a \\right\\|=1, v \\in \\mathbb{R}^n} \\\n\\sum_{i=1}^n \\left\\| X_i - a \\, v_i \\right\\|^2,\n(\\#eq:pca1)\n\\end{equation}\\]\\end{equation} where \\(v = (v_1, \\ldots v_n)^\\top\\) (in general, for any \\(k\\) we have \\[\n\\min_{ A^\\top A = I, v_1, \\ldots, v_n \\in \\mathbb{R}^k} \\\n\\sum_{i=1}^n \\left\\| X_i - A \\, v_i \\right\\|^2 \\, ).\n\\] The objective function in Equation @ref(eq:pca1) can also be written as \\[\\begin{equation}\n\\sum_{i=1}^n \\sum_{j=1}^p \\left( X_{i,j} - a_j \\, v_i \\right)^2 \\, , (\\#eq:pca2)\n\\end{equation}\\] and hence, for a given vector \\(a\\), the minimizing values of \\(v_1, \\ldots, v_n\\) in Equation @ref(eq:pca2) can be found solving \\(n\\) separate least squares problems: \\[\nv_\\ell \\, = \\, \\arg \\,  \\min_{d \\in \\mathbb{R}}\n\\sum_{j=1}^p \\left( X_{\\ell,j} - a_j \\, d \\right)^2 \\, , \\qquad\n\\ell = 1, \\ldots, n \\, .\n\\] Similarly, for a given set \\(v_1, \\ldots, v_n\\) the entries of \\(a\\) can be found solving \\(p\\) separate least squares problems: \\[\na_r \\, = \\, \\arg \\,  \\min_{d \\in \\mathbb{R}}\n\\sum_{i=1}^n \\left( X_{i, r} - d \\, v_i \\right)^2 \\, , \\qquad\nr = 1, \\ldots, p \\, .\n\\] We can then set \\(a \\leftarrow a / \\| a \\|\\) and iterate to find new \\(v\\)’s, then a new \\(a\\), etc."
  }
]