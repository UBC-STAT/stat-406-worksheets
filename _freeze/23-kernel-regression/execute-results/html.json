{
  "hash": "fb5593ecb395e2717c766474db3b564c",
  "result": {
    "markdown": "# Kernel regression / local regression \n\n\n\n\n\n\nA different approach to estimating a regression function is\nbased on recalling that the  true regression function \nis *f(a) = E(Y | X = a)*, the mean of the response variable *Y* **conditional**\nto the event that the explanatory variable(s) **X** equal(s) **a**. If we\nhad lots of data, we could, in principle, think of the following \nintuitively simple regression estimator: given **c**, consider all \nobservations (Y, **X**) in your training set that have **X = c**, and take\nour estimated *f(c)* as \nthe average of the corresponding observed values of the response variable\nY. This would be a resonable estimator for E(Y | **X** = **c** ) (if \nwe had sufficient cases in our training data pairs for which **X** = **c**).\n\nAlthough the simple approach above does not usually work in practice (because\nwe do not have enough training points with **X** = **c** for many values of **c**), \nthe idea can still be used to construct a regression estimator that works\n**locally**, i.e. that given **c** uses the points in the training set\nthat have **X close to c** (you can think of this as *working in a neighbourhood* of\n**c**). This family of regression estimators is called\n*local regression*, or *kernel regression*. The latter name is based \non the fact that we will use a specific\nfamily of functions (called kernels) to define which points \nare *neighbours* and \nhow they will be used to estimate the regression function.\nNote that these *kernel functions* are different from those used in\nSupport Vector Machines and other reproducible kernel Hilbert spaces methods.\n\nProbably the simplest kernel regression estimator is to simply take\nthe average of the responses of the training points where the explanatory\nvariables are within *h* of the point of interest. This \"window width\" *h*\nis called the *bandwidth*. We can use the function\n`ksmooth` in package `KernSmooth` in `R` to do this (**but it would be \na great exercise to write your own `R` function to do it**). The code below\nconsiders one specific explanatory variable for the air pollution data \n(just for illustration purposes) and fits a local averages regression\nestimator, with bandwidth equal to 50:\n\n\n::: {.cell layout-align=\"center\" hash='23-kernel-regression_cache/html/kernel0_6df9145b8862e3c8305cfd42d8122e9b'}\n\n```{.r .cell-code}\ndat <- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\nlibrary(KernSmooth)\nx <- dat$SO.\ny <- dat$MORT\nh <- 50\na <- ksmooth(x = x, y = y, kernel = \"box\", bandwidth = h, n.points = 1000)\nplot(y ~ x, pch = 19, col = \"gray\", cex = 1.3, xlab = \"SO.\", ylab = \"MORT\")\nlines(a$x, a$y, lwd = 4, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](23-kernel-regression_files/figure-html/kernel0-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote the gap in the estimated regression function. Why do you think\nthis happened? \n\nIf we increase the bandwidth from 50 to 60 we obtain the following estimated\nregression function:\n\n\n::: {.cell layout-align=\"center\" hash='23-kernel-regression_cache/html/kernel0.1_7b8cc53e269f64620a6000fada07160a'}\n\n```{.r .cell-code}\nh <- 60\na <- ksmooth(x = x, y = y, kernel = \"box\", bandwidth = h, n.points = 1000)\nplot(y ~ x, pch = 19, col = \"gray\", cex = 1.3, xlab = \"SO.\", ylab = \"MORT\")\nlines(a$x, a$y, lwd = 4, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](23-kernel-regression_files/figure-html/kernel0.1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThis fit is still rather unsatisfactory. For example, note \nhow it looks like a *staircase*. The estimated regression curve\nis fairly jagged, which is usually not how we expect the true regression\nfunction to be. Can you explain why the above regression estimator looks like this? \n\nAs discussed in class, using a smoother kernel function \nresults in a smoother estimated regression function. The plot\nbelow uses the same bandwidth as before, but the kernel function is\nthe standard gaussian density:\n\n\n::: {.cell layout-align=\"center\" hash='23-kernel-regression_cache/html/kernel0.2_cb150f2a41b89a7e4b77f58f9e0d0a9b'}\n\n```{.r .cell-code}\nh <- 60\na <- ksmooth(x = x, y = y, kernel = \"normal\", bandwidth = h, n.points = 1000)\nplot(y ~ x, pch = 19, col = \"gray\", cex = 1.3, xlab = \"SO.\", ylab = \"MORT\")\nlines(a$x, a$y, lwd = 4, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](23-kernel-regression_files/figure-html/kernel0.2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nBetter properties for the estimated regression function are obtained \nwhen one uses a smooth kernel with *compact support* (the support of\nthe gaussian density function is the whole real line and thus not\ncompact). The reasons for this (better kernel regression estimators\nwhen the kernel has compact support) are rather technical and will\nnot be discussed here. A good technical reference for these topics is\nthe following, which is available on-line via the Library:\n\n> Nonparametric and Semiparametric Models. (2004). \n> Hardle, W., Werwatz, A., Muller, M. and  Sperlich, S. \n> Springer-Verlag Berlin Heidelberg. \n> DOI: [10.1007/978-3-642-17146-8](http://doi.org/10.1007/978-3-642-17146-8)\n\nIn what follows we will use the `R` function `loess`\nthat implements this approach with a tri-cubic \nkernel given by *k(a) = ( 1 - (|a|)^3 )^3* if *|a| < 1*, and 0 otherwise.\nThe following plot compares this kernel with the gaussian one. \nSince the important characteristics of a kernel are its shape and support set, \nbelow I standardized both of them to reach the same maximum value (1): \n\n\n::: {.cell layout-align=\"center\" hash='23-kernel-regression_cache/html/kernel.comp_0d714a6108b6db8f9749389ea785fef4'}\n\n```{.r .cell-code}\ntt <- seq(-2, 2, length = 100)\ntmp <- dnorm(tt)\nplot(tt, tmp / max(tmp), ylab = \"Kernel\", xlab = \"\", lwd = 6, type = \"l\", col = \"gray40\", ylim = c(0, 1))\ntmp <- (1 - abs(tt)^3)^3\ntmp[abs(tt) > 1] <- 0\nlines(tt, tmp / max(tmp), lwd = 6, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](23-kernel-regression_files/figure-html/kernel.comp-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n### Fixed versus variable bandwidths\n\nAs we discussed in class, fixed bandwidths may present problems in practice\nwhen the density of the observed explanatory variables is not uniform \n(i.e. there are *dense* regions where we have more observations and \n*sparse* regions where there are fewer observations). A solution \nto this is to use *variable bandwidths*, where at each point *c* we \ntake a bandwidth large enough to contain a pre-specified proportion\n*alpha* of the data. The function `loess` implements this approach, \nthe desired proportion of observations in each neighbourhood is given\nby the argument `span`. \n\nWhen we apply this method (with `span = 0.5`, and `degree = 0` to indicate\nwe are using *local averages*) to the example above, we get the following\nfit:\n\n\n::: {.cell layout-align=\"center\" hash='23-kernel-regression_cache/html/loess.air_68c42571149dff5d745ee6cc28c5bc84'}\n\n```{.r .cell-code}\na <- loess(MORT ~ SO., data = dat, span = 0.5, degree = 0, family = \"gaussian\")\nplot(MORT ~ SO., data = dat, pch = 19, col = \"gray\", cex = 1.3, xlab = \"SO.\", ylab = \"MORT\")\ntmp <- order(a$x)\nlines(a$x[tmp], a$fitted[tmp], lwd = 4, col = \"steelblue\")\n```\n\n::: {.cell-output-display}\n![](23-kernel-regression_files/figure-html/loess.air-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote, in particular, how the upper end of the estimated regression function\nlooks better than the approach discussed before using fixed bandwidths. \n\nAlthough we have not yet discussed how to choose a bandwidth (either fixed\nor variable) among the infinitely many possible ones, I expect you\nto already know how this may be done. \n\n### Local regression versus local means\n\nAs discussed in more detail in class, a better way to exploit the\napproximating properties of a Taylor expansion, is to use it locally.\nIn particular, using kernels as above, we can estimate the regression\nfunction using a linear function *locally* (corresponding to a \nTaylor expansion of order 1), or a quadratic function (expansion of\norder 2), etc. We will illustrate these points on  the `ethanol`\ndata in package `SemiPar`. As usual, information about the data\ncan be found on its help page. \n\nBelow we load the data and compute a *local constant* (`degree = 0`) *regression \nestimator*, where the response variable is `NOx` and the explanatory variable is \n`E`. The span was arbitrarily set to 0.40 (but this will discussed in \nmore detail below). \n\n\n::: {.cell layout-align=\"center\" hash='23-kernel-regression_cache/html/kernel0.3_86d011b2d62ebb5e9f1c82c905a2bbfa'}\n\n```{.r .cell-code}\ndata(ethanol, package = \"SemiPar\")\n# local constant\nspan <- .4\nb0 <- loess(NOx ~ E, data = ethanol, span = span, degree = 0, family = \"gaussian\")\nplot(NOx ~ E, data = ethanol, pch = 19, col = \"gray\", cex = 1.3, xlab = \"E\", ylab = \"NOx\")\ntmp <- order(b0$x)\nlines(b0$x[tmp], b0$fitted[tmp], lwd = 4, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](23-kernel-regression_files/figure-html/kernel0.3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote how this regression estimator tends to not fit well the *tails* of the data\n(i.e. the smallest and largest observed values of `E`). A better fit\nis obtained with a *locally linear* estimator (`degree = 1`), shown below in red, over the\n*locally constant* one (in blue):\n\n\n::: {.cell layout-align=\"center\" hash='23-kernel-regression_cache/html/kernel0.4_041e88e2408c33b2b2acc9876cc7dd7a'}\n\n```{.r .cell-code}\n# local linear\nspan <- .4\nb1 <- loess(NOx ~ E, data = ethanol, span = span, degree = 1, family = \"gaussian\")\nplot(NOx ~ E, data = ethanol, pch = 19, col = \"gray\", cex = 1.3, xlab = \"E\", ylab = \"NOx\")\ntmp <- order(b1$x)\nlines(b1$x[tmp], b1$fitted[tmp], lwd = 4, col = \"red\")\nlines(b0$x[tmp], b0$fitted[tmp], lwd = 4, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](23-kernel-regression_files/figure-html/kernel0.4-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThis fit is an improvement from the previous one, but it does not\ncapture well the *peak* of the data (around `E` = 0.90). It is easy to see that\na quadratic local fit might be able to do this, without affecting the\nquality of the fit elsewhere. Below we compare the locally linear (red) and\nlocally quadratic (dark green) fits:\n\n\n::: {.cell layout-align=\"center\" hash='23-kernel-regression_cache/html/kernel0.5_2035f940f9564a14d8dbf9bf413e091e'}\n\n```{.r .cell-code}\n# local quad\nspan <- .4\nb2 <- loess(NOx ~ E, data = ethanol, span = span, degree = 2, family = \"gaussian\")\nplot(NOx ~ E, data = ethanol, pch = 19, col = \"gray\", cex = 1.3, xlab = \"E\", ylab = \"NOx\")\ntmp <- order(b2$x)\nlines(b1$x[tmp], b1$fitted[tmp], lwd = 4, col = \"red\")\nlines(b2$x[tmp], b2$fitted[tmp], lwd = 4, col = \"darkgreen\")\n```\n\n::: {.cell-output-display}\n![](23-kernel-regression_files/figure-html/kernel0.5-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n### Choosing the bandwidth\n\nIt is easy to see that the bandwidths plays a similar role to\nthe one played by the penalty term in smoothers based on\nsplines or other bases. A very small bandwidth results in\nan estimator that is too adaptive to local quirks of the\ntraining set. Similarly, a bandwidth that is too large\nwill result in an estimator that essentially fit a single\nglobal model to the whole data set. \n\nWe illustrate the effect of different choices of\nbandwidths below. We take a local quadratic (2nd degree\npolynomial) fit, with a very small span (0.05):\n\n\n::: {.cell layout-align=\"center\" hash='23-kernel-regression_cache/html/kernel1_ce00a5d4fb18aa9b278145bd8660db1c'}\n\n```{.r .cell-code}\ntmp <- loess(NOx ~ E, data = ethanol, span = .05, degree = 2, family = \"gaussian\")\nplot(NOx ~ E, data = ethanol, pch = 19, col = \"gray\", cex = 1.5)\n# artificial grid of values to show predictions for the plot\nprs <- with(ethanol, seq(min(E), max(E), length = 1000))\nlines(predict(tmp, newdata = prs) ~ prs, data = ethanol, lwd = 4, col = \"steelblue\")\n```\n\n::: {.cell-output-display}\n![](23-kernel-regression_files/figure-html/kernel1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nLarger spans result in \"better\" fits, at least in the sense of \nbeing more pleasant to the eye:\n\n\n::: {.cell layout-align=\"center\" hash='23-kernel-regression_cache/html/kernel2_025deabe29fa4e7d466e6e7e2bcd6344'}\n\n```{.r .cell-code}\ntmp <- loess(NOx ~ E, data = ethanol, span = .25, degree = 2, family = \"gaussian\")\nplot(NOx ~ E, data = ethanol, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(tmp, newdata = prs) ~ prs, data = ethanol, lwd = 4, col = \"hotpink\")\ntmp <- loess(NOx ~ E, data = ethanol, span = .5, degree = 2, family = \"gaussian\")\nlines(predict(tmp, newdata = prs) ~ prs, data = ethanol, lwd = 4, col = \"darkgreen\")\nlegend(\"topleft\", legend = c(\"span: 0.25\", \"span: 0.50\"), col = c(\"hotpink\", \"darkgreen\"), lwd = 4)\n```\n\n::: {.cell-output-display}\n![](23-kernel-regression_files/figure-html/kernel2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThe range of sensible or acceptable values of the argument `span` in `loess`\nis determined, of course, by the exact definition of this parameter. Information\ncan be found in the corresponding help page, as usual. As you probably know, \nan \"optimal\" value of span could be chosen using cross-validation. A \ncouple of **very good** questions for you are the following: \n\n* are kernel regression estimators *linear* in the sense of there being a \nmatrix **S** such that the fitted values equal **S y**, where **y** is the\nvector of responses in the training set, and **S** does not depend on **y**? \n* use K-fold cross-validation to choose an \"optimal\" value of `span`.\n\n\n## The problem of outliers and other model departures\n\nWhen the data may contain outliers and/or other atypical observations,\nthe estimation methods discussed above may be seriously affected, even\nif there are only a few such aberrant data points in the training set \n(possible outliers in the test / validation set are also a concern, but\nwe don't have time to discuss it here). Some robust estimation \nmethods based on kernel smoothers exist. See for example [@BoenteMartinez2017]\nand references therein. This paper deals with a slightly more \ncomplex model (additive model), but when only component exists, it\nis the same model discussed in class. The [RBF](https://cran.r-project.org/package=RBF) package \nimplementing this method is available from [CRAN](https://cran.r-project.org/package=RBF)\nand also [here](https://github.com/msalibian/RBF).\n\n\n\n\n<!-- Effect of the degree, now quadratic: -->\n\n<!-- ```{r kernel3, fig.width=5, fig.height=5, message=FALSE, warning=FALSE} -->\n<!-- tmp <- loess(NOx ~ E, data=ethanol, span = .5, degree=2, family='gaussian') -->\n<!-- plot(NOx ~ E, data=ethanol, pch=19, col='gray', cex=1.5) -->\n<!-- lines(predict(tmp, newdata=prs) ~ prs, data=ethanol, lwd=4, col='blue') -->\n<!-- ``` -->\n\n<!-- Now quadratic, span = 0.20 -->\n\n<!-- ```{r kernel4, fig.width=5, fig.height=5, message=FALSE, warning=FALSE} -->\n<!-- tmp <- loess(NOx ~ E, data=ethanol, span = .2, degree=2, family='gaussian') -->\n<!-- plot(NOx ~ E, data=ethanol, pch=19, col='gray', cex=1.5) -->\n<!-- lines(predict(tmp)[order(E)] ~ sort(E), data=ethanol, lwd=4, col='steelblue') -->\n<!-- lines(predict(tmp, newdata=prs) ~ prs, data=ethanol, lwd=2, col='red2') -->\n<!-- ``` -->\n\n<!-- Kinks are artifact of sparsity of data -->\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}