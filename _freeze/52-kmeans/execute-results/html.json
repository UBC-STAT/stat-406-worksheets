{
  "hash": "cf601f97f5cee2df27fa10aa49e70e87",
  "result": {
    "markdown": "\n# Clustering\n\n\n\n\n\n\nA large class of unsupervised learning methods is\ncollectively called *clustering*. The objective \ncan be described as identifying different groups \nof observations that are closer to each other (\"clustered\") than to those of \nthe other groups. The data consist of *n* observations $X_1$, $X_2$, ..., \n$X_n$, each with *p* features. In general, the number of groups is\nunknown and needs to be determined from the data.\n\nIn this course we will discuss both model-free\nand model-based clustering methods. In the first\ngroup we will present K-means (and other \nrelated methods), and hierarchical (agglomerative) clustering \nmethods. Model-based clustering is based on the assumption that the data \nis a random sample, and that the distribution of\nthe vector of features **X** is a combination of \ndifferent distributions (technically: a *mixture model*). \nIn the latter case, for observations belonging to a\ngroup, the vector **X** is assumed \nto have a distribution in a specific (generally parametric) family. \nSome of these model-based methods treat the group labels as \nmissing (unobserved) responses, and rely on the assumed model to \ninfer those missing labels. \n\n## K-means, K-means++, K-medoids\n\nProbably the most intuitive and easier to explain\nunsupervised clustering algorithm is K-means (and\nits variants *K-means++* and K-medoids, a.k.a. *pam*, \npartition around medoids). The specifics of the K-means \nalgorithm were discussed in class. Here we will illustrate its use\non a few examples. \n\n### UN votes example. \n\nThese data contain the historical voting patterns\nof United Nations members. More details can be\nfound here at [@DVN/LEJUQZ_2009].\nThe UN was founded in 1946 and it contains 193 member states. \nThe data include only \"important\" votes, as classified\nby the U.S. State Department. The votes for each country \nwere coded as follows: Yes (1), Abstain (2), No (3),\nAbsent (8), Not a Member (9). There were \n368 important votes, and 77 countries\nvoted in at least 95% of these. We focus on these\nUN members. Our goal is to explore whether\nvoting patterns reflect political\nalignments, and also whether countries vote along known\npolitical blocks. Our data consists of 77 observations\nwith 368 variables each. More information on these data can be found \n[here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/12379).\n\nThe dataset is organized by vote (resolution), one per row, \nand its columns contain the corresponding vote of each country\n(one country per column).\nWe first read the data, and limit ourselves to resolutions where\nevery country voted without missing votes:\n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/unvotes_d6d9917ae1ae25a0cb6f2f4b4086aee9'}\n\n```{.r .cell-code}\nX <- read.table(file = \"data/unvotes.csv\", sep = \",\", row.names = 1, header = TRUE)\nX2 <- X[complete.cases(X), ]\n```\n:::\n\nWe now compute a K-means partition using the function `kmeans`\nwith *K = 5*, and look at the resulting groups:\n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/kmeans.un1_1957c77f86b88af075b528a983bbce99'}\n\n```{.r .cell-code}\nset.seed(123)\nb <- kmeans(t(X2), centers = 5, iter.max = 20, nstart = 1)\ntable(b$cluster)\n#> \n#>  1  2  3  4  5 \n#> 26  2 15  9 25\n```\n:::\n\nIf we run `kmeans` again, we might get a different partition:\n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/kmeans.un2_8d7af58efbbef07dc6c6f53137fb6a8b'}\n\n```{.r .cell-code}\nb <- kmeans(t(X2), centers = 5, iter.max = 20, nstart = 1)\ntable(b$cluster)\n#> \n#>  1  2  3  4  5 \n#> 27 13 12  5 20\n```\n:::\n\nIt is better to consider a large number of random starts and\ntake the **best** found solution (what does *best* mean in this\ncontext? in other words, how does the algorithm decide which one is\nthe solution it should return?) \n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/kmeans.un3_621a0772f548b78c209e4c4142e29741'}\n\n```{.r .cell-code}\n# Take the best solution out of 1000 random starts\nb <- kmeans(t(X2), centers = 5, iter.max = 20, nstart = 1000)\nsplit(colnames(X2), b$cluster)\n#> $`1`\n#>  [1] \"Algeria\"              \"Bangladesh\"           \"Belarus\"             \n#>  [4] \"Brunei.Darussalam\"    \"China\"                \"Cuba\"                \n#>  [7] \"Egypt\"                \"India\"                \"Indonesia\"           \n#> [10] \"Kuwait\"               \"Libya\"                \"Malaysia\"            \n#> [13] \"Pakistan\"             \"Russian.Federation\"   \"Sudan\"               \n#> [16] \"Syrian.Arab.Republic\" \"Venezuela\"           \n#> \n#> $`2`\n#>  [1] \"Bolivia\"             \"Botswana\"            \"Burkina.Faso\"       \n#>  [4] \"Ecuador\"             \"Ethiopia\"            \"Ghana\"              \n#>  [7] \"Guyana\"              \"Jamaica\"             \"Jordan\"             \n#> [10] \"Kenya\"               \"Mali\"                \"Nepal\"              \n#> [13] \"Nigeria\"             \"Philippines\"         \"Singapore\"          \n#> [16] \"Sri.Lanka\"           \"Tanzania\"            \"Thailand\"           \n#> [19] \"Togo\"                \"Trinidad.and.Tobago\" \"Zambia\"             \n#> \n#> $`3`\n#>  [1] \"Argentina\"  \"Bahamas\"    \"Brazil\"     \"Chile\"      \"Colombia\"  \n#>  [6] \"Costa.Rica\" \"Mexico\"     \"Panama\"     \"Paraguay\"   \"Peru\"      \n#> [11] \"Uruguay\"   \n#> \n#> $`4`\n#>  [1] \"Australia\"   \"Austria\"     \"Belgium\"     \"Bulgaria\"    \"Canada\"     \n#>  [6] \"Cyprus\"      \"Denmark\"     \"Finland\"     \"France\"      \"Greece\"     \n#> [11] \"Hungary\"     \"Iceland\"     \"Ireland\"     \"Italy\"       \"Japan\"      \n#> [16] \"Luxembourg\"  \"Malta\"       \"Netherlands\" \"New.Zealand\" \"Norway\"     \n#> [21] \"Poland\"      \"Portugal\"    \"Spain\"       \"Sweden\"      \"UK\"         \n#> [26] \"Ukraine\"    \n#> \n#> $`5`\n#> [1] \"Israel\" \"USA\"\n```\n:::\n\nIt may be better to look at the groups on a map:\n\n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/kmeans.map1_72a604147c595772a3ffe3535a88ce71'}\n\n```{.r .cell-code}\nlibrary(rworldmap)\nlibrary(countrycode)\nthese <- countrycode(colnames(X2), \"country.name\", \"iso3c\")\nmalDF <- data.frame(country = these, cluster = b$cluster)\n# malDF is a data.frame with the ISO3 country names plus a variable to merge to\n# the map data\n\n# This line will join your malDF data.frame to the country map data\nmalMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\n#> 77 codes from your data successfully matched countries in the map\n#> 0 codes from your data failed to match with a country code in the map\n#> 166 codes from the map weren't represented in your data\n# colors()[grep('blue', colors())] fill the space on the graphical device\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", missingCountryCol = \"white\",\n    addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\",\n        \"blueviolet\", \"yellow\"), oceanCol = \"dodgerblue\")\n```\n\n::: {.cell-output-display}\n![](52-kmeans_files/figure-html/kmeans.map1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe can compare this partition with the one we obtain using PAM (K-medoids), \nwhich is implemented in the function `pam` of the package `cluster`. Recall\nfrom the discussion in class that `pam` does not need to manipulate the\nactual observations, only its pairwise distances (or dissimilarities). In\nthis case we use Euclidean distances, but it may be interesting to \nexplore other distances, particulary in light of the categorical \nnature of the data. Furthermore, to obtain clusters that may\nbe easier to interpret we use `K = 3`:\n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/un.pam_b05c5e35541d17907d89ad8b743d6eb9'}\n\n```{.r .cell-code}\nlibrary(cluster)\n# Use Euclidean distances\nd <- dist(t(X))\n# what happens with missing values?\nset.seed(123)\na <- pam(d, k = 3)\n```\n:::\n\nCompare the resulting groups with those of *K-means*:\n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/un.pam2_4c8ce8d19fc381fc329f7f4c969f93d2'}\n\n```{.r .cell-code}\nb <- kmeans(t(X2), centers = 3, iter.max = 20, nstart = 1000)\ntable(a$clustering)\n#> \n#>  1  2  3 \n#> 26 24 27\ntable(b$cluster)\n#> \n#>  1  2  3 \n#> 16 28 33\n```\n:::\n\nAn better visualization is done using the map. interesting \nWe plot the 3 groups found by `pam` on the map, followed by those\nfound by K-means:\n\n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/un.pam.map_2f1b2a8943c4e3e25e15485ed3e07cb4'}\n\n```{.r .cell-code}\nthese <- countrycode(colnames(X), \"country.name\", \"iso3c\")\nmalDF <- data.frame(country = these, cluster = a$clustering)\nmalMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\n#> 77 codes from your data successfully matched countries in the map\n#> 0 codes from your data failed to match with a country code in the map\n#> 166 codes from the map weren't represented in your data\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", missingCountryCol = \"white\",\n    addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\",\n        \"blueviolet\", \"yellow\"), oceanCol = \"dodgerblue\")\n```\n\n::: {.cell-output-display}\n![](52-kmeans_files/figure-html/un.pam.map-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nthese <- countrycode(colnames(X2), \"country.name\", \"iso3c\")\nmalDF <- data.frame(country = these, cluster = b$cluster)\nmalMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\n#> 77 codes from your data successfully matched countries in the map\n#> 0 codes from your data failed to match with a country code in the map\n#> 166 codes from the map weren't represented in your data\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", missingCountryCol = \"white\",\n    addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"yellow\", \"tomato\", \"blueviolet\"),\n    oceanCol = \"dodgerblue\")\n```\n\n::: {.cell-output-display}\n![](52-kmeans_files/figure-html/un.pam.map-2.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWhat if we use the L_1 norm instead?\n\n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/un.pam.inf_898563c3b4b0035adea1594b4b941482'}\n\n```{.r .cell-code}\nd <- dist(t(X), method = \"manhattan\")\nset.seed(123)\na <- pam(d, k = 3)\nthese <- countrycode(colnames(X), \"country.name\", \"iso3c\")\nmalDF <- data.frame(country = these, cluster = a$clustering)\nmalMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\n#> 77 codes from your data successfully matched countries in the map\n#> 0 codes from your data failed to match with a country code in the map\n#> 166 codes from the map weren't represented in your data\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap, nameColumnToPlot = \"cluster\", catMethod = \"categorical\", missingCountryCol = \"white\",\n    addLegend = FALSE, mapTitle = \"\", colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\",\n        \"blueviolet\", \"yellow\"), oceanCol = \"dodgerblue\")\n```\n\n::: {.cell-output-display}\n![](52-kmeans_files/figure-html/un.pam.inf-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nAs mentioned before, since the data set does not include a *true label*, the \ncomparison between the different results is somewhat subjective, and it often\nrelies on the knowledge of the subject matter experts. In our example above, this\nwould mean asking the opinion of a political scientist as to whether these\ngroupings correspond to known international political blocks or alignments.  \n\n### Breweries\n\nIn this example beer drinkers were asked to rate 9 breweries one 26\nattributes, e.g. whether this brewery has a rich tradition; or\nwhether it makes very good pilsner beer, etc. For each of these\nquestions, the *judges* reported a score on a 6-point scale\nranging from 1: \"not true at all\" to 6: \"very true\". The data\nare in the file `breweries.dat`:\n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/brew_cbde491d1e9f81a64d7451335cc5e625'}\n\n```{.r .cell-code}\nx <- read.table(\"data/breweries.dat\", header = FALSE)\nx <- t(x)\n```\n:::\n\n\nFor illustration purposes we use the $L_1$ distance and the\nPAM clustering method.\n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/brew2_266069f076e9d8d416bd20726113834e'}\n\n```{.r .cell-code}\nd <- dist(x, method = \"manhattan\")\nset.seed(123)\na <- pam(d, k = 3)\ntable(a$clustering)\n#> \n#> 1 2 3 \n#> 3 3 3\n```\n:::\n\nTo visualize the strength of these cluster partition we use\nthe silhouette plot discussed in class:\n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/brew.plot_b30bdb6f4624102f79a5f7b18ef75390'}\n\n```{.r .cell-code}\nplot(a)\n```\n\n::: {.cell-output-display}\n![](52-kmeans_files/figure-html/brew.plot-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nSince other distances may produce different partitions, an interesting\nexercise would be to compare the above clusters with those found using\nthe Euclidean or  $L_\\infty$ norms, for example. \n\n### Cancer example\n\nThis data contains gene expression levels for \n6830 genes (rows) for 64 cell samples (columns). \nMore information can be found here:\n[http://genome-www.stanford.edu/nci60/](http://genome-www.stanford.edu/nci60/). \nThe data are included in the `ElemStatLearn` package, and also\navailable on-line: \n[https://web.stanford.edu/~hastie/ElemStatLearn/](https://web.stanford.edu/~hastie/ElemStatLearn/).\n\nWe will use K-means to identify 8 possible clusters among the 64 cell samples. \nAs discussed in class this exercise can (perhaps more interestingly) be formulated \nin terms of *feature selection*. We load the data and use K-means to find 8 clusters:\n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/nci_8f2d44871939a80b449d7e8d1a9f53b7'}\n\n```{.r .cell-code}\ndata(nci, package = \"ElemStatLearn\")\nncit <- t(nci)\nset.seed(31)\na <- kmeans(ncit, centers = 8, iter.max = 5000, nstart = 100)\ntable(a$cluster)\n#> \n#>  1  2  3  4  5  6  7  8 \n#>  3  4  5  8 14  6  9 15\n```\n:::\n\nNote that in this application we do know the group to which each\nobservation belongs (its cancer type). We can look at the cancer types that\nhave been grouped together in each of the 8 clusters:\n\n::: {.cell layout-align=\"center\" hash='52-kmeans_cache/html/nci2_2413ff3876382e7cda9e0c68e073f5da'}\n\n```{.r .cell-code}\nsapply(split(colnames(nci), a$cluster), table)\n#> $`1`\n#> \n#> K562A-repro K562B-repro    LEUKEMIA \n#>           1           1           1 \n#> \n#> $`2`\n#> \n#>      BREAST MCF7A-repro MCF7D-repro \n#>           2           1           1 \n#> \n#> $`3`\n#> \n#> LEUKEMIA \n#>        5 \n#> \n#> $`4`\n#> \n#> BREAST    CNS  RENAL \n#>      2      5      1 \n#> \n#> $`5`\n#> \n#>    COLON    NSCLC  OVARIAN PROSTATE \n#>        1        6        5        2 \n#> \n#> $`6`\n#> \n#> COLON \n#>     6 \n#> \n#> $`7`\n#> \n#>   BREAST MELANOMA \n#>        2        7 \n#> \n#> $`8`\n#> \n#>   BREAST MELANOMA    NSCLC  OVARIAN    RENAL  UNKNOWN \n#>        1        1        3        1        8        1\n```\n:::\n\nNote that clusters 3, 4, 6 and 7 are dominated by one type of cancer. \nSimilarly, almost all melanoma and renal samples are in clusters 7 and 8, \nrespectively, while all CNS samples are in cluster 4. Cluster 5 is\nharder to interpret. Although all but one ovarian cancer samples are in this\ncluster, it also contains 2/3 of the NSCLC samples. It may be of interest\nto compare these results with those using different numbers of clusters. \n\n\n\n<!-- Finally, we will compare this partition (obtained with the usual K-means algorithm -->\n<!-- started from `100` random initial configurations) with the one  -->\n<!-- found by K++ as implemented in `flexclust::kcca`: -->\n<!-- ```{r nci1.kpp, fig.width=6, fig.height=6, message=FALSE, warning=FALSE, tidy=TRUE} -->\n<!-- library(flexclust) -->\n<!-- set.seed(123) -->\n<!-- kpp <- kcca(ncit, k = 8, family = kccaFamily(\"kmeans\")) -->\n<!-- table(kpp@cluster) -->\n<!-- ``` -->\n<!-- while the one obtained with `kmeans` was -->\n<!-- ```{r nci2.kpp, fig.width=6, fig.height=6, message=FALSE, warning=FALSE, tidy=TRUE} -->\n<!-- table(a$cluster) -->\n<!-- ``` -->\n<!-- Note that the two partitions are quite different. Furthermore, note that  -->\n<!-- there is still a lot of randomness in the solution returned by K++. If we run it again  -->\n<!-- with a different pseudo-random number generating seed we obtain a very different -->\n<!-- partition: -->\n<!-- ```{r nci3.kpp, fig.width=6, fig.height=6, message=FALSE, warning=FALSE, tidy=TRUE} -->\n<!-- set.seed(987) -->\n<!-- table(kcca(ncit, k = 8, family = kccaFamily(\"kmeans\"), control = list(initcent = \"kmeanspp\"))@cluster) -->\n<!-- ``` -->\n<!-- A very good exercise would be  -->\n<!-- to determine which of these different partitions -->\n<!-- (the ones found with K++ and the one returned by 100 random starts of -->\n<!-- K-means) is better in terms  -->\n<!-- of the objective function being optimized to find the clusters. -->\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}