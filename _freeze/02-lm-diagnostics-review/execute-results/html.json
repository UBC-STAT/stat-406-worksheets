{
  "hash": "e306b530dfeac8b5588b0a91358574e4",
  "result": {
    "markdown": "# Predictions using a linear model\n\n\n\n\n\n\n\nIn this document we will explore (rather superficially)\nsome challenges found when trying to estimate the forecasting \nproperties (e.g. the mean squared prediction error) of a (linear) predictor. We will\nuse the air-pollution data set, which I have split into a *training set* and a *test set*. \nThe test set will be ignored when **training** our model (in the case of a linear\nmodel, \"**training**\" simply means \"**when estimating the vector of linear regression \nparameters**\").\n\nIf you are interested in how these sets (*training* and *test*) were constructed: \nI ran the following script (you\ndo not need to do this, as I am providing both data sets to you, \nbut you can re-create them yourself if you want to):\n\n::: {.cell layout-align=\"center\" hash='02-lm-diagnostics-review_cache/html/construct_90eb2b289deb701806d997aab53efaee'}\n\n```{.r .cell-code}\nx <- read.csv(\"data/rutgers-lib-30861_CSV-1.csv\")\nset.seed(123)\nii <- sample(rep(1:4, each = 15))\n# this is the training set `pollution-train.dat`\nx.tr <- x[ii != 2, ]\n# this is the test set `pollution-test.dat`\nx.te <- x[ii == 2, ]\n# then I saved them to disk:\n# write.csv(x.tr, file='pollution-train.dat', row.names=FALSE, quote=FALSE)\n# write.csv(x.te, file='pollution-test.dat', row.names=FALSE, quote=FALSE)\n```\n:::\n\n\nWe now read the **training** data set from the file `pollution-train.dat`,\nwhich is available [here](pollution-train.dat), and check that it was read properly:\n\n::: {.cell layout-align=\"center\" hash='02-lm-diagnostics-review_cache/html/readtrain_db550b90a599d93867ac5fc54144d0d2'}\n\n```{.r .cell-code}\nx.tr <- read.table(\"data/pollution-train.dat\", header = TRUE, sep = \",\")\n# sanity check\nhead(x.tr)\n#>   PREC JANT JULT OVR65 POPN EDUC HOUS DENS NONW WWDRK POOR HC NOX SO. HUMID\n#> 1   36   27   71   8.1 3.34 11.4 81.5 3243  8.8  42.6 11.7 21  15  59    59\n#> 2   35   23   72  11.1 3.14 11.0 78.8 4281  3.5  50.7 14.4  8  10  39    57\n#> 3   44   29   74  10.4 3.21  9.8 81.6 4260  0.8  39.4 12.4  6   6  33    54\n#> 4   47   45   79   6.5 3.41 11.1 77.5 3125 27.1  50.2 20.6 18   8  24    56\n#> 5   43   35   77   7.6 3.44  9.6 84.6 6441 24.4  43.7 14.3 43  38 206    55\n#> 6   53   45   80   7.7 3.45 10.2 66.8 3325 38.5  43.1 25.5 30  32  72    54\n#>       MORT\n#> 1  921.870\n#> 2  997.875\n#> 3  962.354\n#> 4  982.291\n#> 5 1071.289\n#> 6 1030.380\n```\n:::\n\nThe response variable is `MORT`. \nOur first step is to fit a \nlinear regression model with all available\npredictors and look at a few diagnostic plots\nwhere everything looks fine:\n\n::: {.cell layout-align=\"center\" hash='02-lm-diagnostics-review_cache/html/full_7c4fe65fb5ec2f891bd4343a003ecb12'}\n\n```{.r .cell-code}\nfull <- lm(MORT ~ ., data = x.tr)\nplot(full, which = 1)\n```\n\n::: {.cell-output-display}\n![](02-lm-diagnostics-review_files/figure-html/full-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nplot(full, which = 2)\n```\n\n::: {.cell-output-display}\n![](02-lm-diagnostics-review_files/figure-html/full-2.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe also take a look at the estimated coeficients: \n\n::: {.cell layout-align=\"center\" hash='02-lm-diagnostics-review_cache/html/diag_6cfbacda2eb7ab61441d8f214eb9575b'}\n\n```{.r .cell-code}\nsummary(full)\n#> \n#> Call:\n#> lm(formula = MORT ~ ., data = x.tr)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -66.06 -14.11  -0.78  17.13  66.09 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)  2.210e+03  5.091e+02   4.341 0.000157 ***\n#> PREC         1.786e+00  1.306e+00   1.367 0.181994    \n#> JANT        -1.794e+00  1.205e+00  -1.489 0.147375    \n#> JULT        -4.767e+00  2.913e+00  -1.636 0.112591    \n#> OVR65       -1.150e+01  9.335e+00  -1.232 0.227734    \n#> POPN        -1.586e+02  7.373e+01  -2.151 0.039980 *  \n#> EDUC        -1.278e+01  1.421e+01  -0.899 0.376043    \n#> HOUS        -8.500e-01  2.013e+00  -0.422 0.676023    \n#> DENS         8.253e-03  5.274e-03   1.565 0.128473    \n#> NONW         4.844e+00  1.566e+00   3.093 0.004357 ** \n#> WWDRK       -1.666e-01  1.947e+00  -0.086 0.932408    \n#> POOR        -1.755e+00  3.530e+00  -0.497 0.622938    \n#> HC          -4.090e-01  5.452e-01  -0.750 0.459193    \n#> NOX          5.607e-01  1.109e+00   0.506 0.616884    \n#> SO.          1.762e-01  1.848e-01   0.954 0.348033    \n#> HUMID       -2.647e+00  2.160e+00  -1.225 0.230307    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 32.55 on 29 degrees of freedom\n#> Multiple R-squared:  0.7978,\tAdjusted R-squared:  0.6931 \n#> F-statistic: 7.626 on 15 and 29 DF,  p-value: 1.805e-06\n```\n:::\n\n\nThe fit appears to be routine, and reasonable (why? what did I check to come to this conclusion?). \n\n### A new focus: prediction\nThis course will be primarily concerned with making (good) predictions for cases \n(data points) that we may have not observed yet (future predictions). This is a bit\ndifferent from the focus of other Statistics courses you may have taken. You will\nsee later in the course that what you learned in other Statistics courses \n(e.g. trade-offs between flexibility and stability of different models, uncertainty\nand standard techniques to reduce it, etc.) will prove\nto be critical for building good predictions. \n\nAs a simple example, in the rest of this note we will compare the quality of this model's predictions with those of a simpler (smaller) linear model with only 5 predictors. For this illustrative example, we will not worry about how these 5 explanatory variables were selected, however, this will play a **critical** role later in the course).\n\nWe now fit this **reduced** model and look at the estimated parameters and diagnostic plots\n\n::: {.cell layout-align=\"center\" hash='02-lm-diagnostics-review_cache/html/reduced_98114824fd6ab1234ba490cee1d677a4'}\n\n```{.r .cell-code}\nreduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\nsummary(reduced)\n#> \n#> Call:\n#> lm(formula = MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -95.654 -21.848  -1.995  21.555  81.039 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 1117.2254   164.3972   6.796 4.09e-08 ***\n#> POOR          -4.7667     2.5516  -1.868 0.069268 .  \n#> HC            -1.4237     0.3705  -3.843 0.000437 ***\n#> NOX            2.6880     0.7262   3.702 0.000660 ***\n#> HOUS          -2.0595     1.7940  -1.148 0.257957    \n#> NONW           4.3004     1.0140   4.241 0.000132 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 39.44 on 39 degrees of freedom\n#> Multiple R-squared:  0.6007,\tAdjusted R-squared:  0.5495 \n#> F-statistic: 11.73 on 5 and 39 DF,  p-value: 5.844e-07\nplot(reduced, which = 1)\n```\n\n::: {.cell-output-display}\n![](02-lm-diagnostics-review_files/figure-html/reduced-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nplot(reduced, which = 2)\n```\n\n::: {.cell-output-display}\n![](02-lm-diagnostics-review_files/figure-html/reduced-2.png){fig-align='center' width=90%}\n:::\n:::\n\n\nAlthough the reduced linear model (with 5 predictors) does not seem to provide a fit\nas good as the one we get with full model, it is still acceptable. \n\n::: {.cell layout-align=\"center\" hash='02-lm-diagnostics-review_cache/html/gofs_90b534b4dccd09e5e178001aa60650f1'}\n\n```{.r .cell-code}\nsum(resid(reduced)^2)\n#> [1] 60652.22\nsum(resid(full)^2)\n#> [1] 30718.19\n```\n:::\n\nThis observation should be obvious to you, since, \nas you already now, a model will **always** yield\na better fit to the data in terms of \nresidual sum of squares than any of its submodels \n(i.e. any model using a subset of the explanatory\nvariables). I expect you to be able to formally \nprove the last satement. \n\nOur question of interest here is:\n\"Which model produces better predictions?\" In many cases one is \ninterested in predicting future observations, i.e. \npredicting the response variable for data\nthat was not available when the model / predictor was \n*fit* or *trained*. As we discussed in class, a reasonably\nfair comparison can be obtined by \ncomparing the mean squared predictions\nof these two linear models on the test set, which we\nread into `R` as follows:\n\n::: {.cell layout-align=\"center\" hash='02-lm-diagnostics-review_cache/html/pred1_80e90adf28330f6c91512385e9876cba'}\n\n```{.r .cell-code}\nx.te <- read.table(\"data/pollution-test.dat\", header = TRUE, sep = \",\")\nhead(x.te)\n#>   PREC JANT JULT OVR65 POPN EDUC HOUS DENS NONW WWDRK POOR HC NOX SO. HUMID\n#> 1   52   42   79   7.7 3.39  9.6 69.2 2302 22.2  41.3 24.2 18   8  27    56\n#> 2   33   26   76   8.6 3.20 10.9 83.4 6122 16.3  44.9 10.7 88  63 278    58\n#> 3   40   34   77   9.2 3.21 10.2 77.0 4101 13.0  45.7 15.1 26  26 146    57\n#> 4   35   46   85   7.1 3.22 11.8 79.9 1441 14.8  51.2 16.1  1   1   1    54\n#> 5   15   30   73   8.2 3.15 12.2 84.2 4824  4.7  53.1 12.7 17   8  28    38\n#> 6   43   27   72   9.0 3.25 11.5 87.1 2909  7.2  51.6  9.5  7   3  10    56\n#>       MORT\n#> 1 1017.613\n#> 2 1024.885\n#> 3  970.467\n#> 4  860.101\n#> 5  871.766\n#> 6  887.466\n```\n:::\n\nNow compute the predicted values for the test set\nwith both the **full** and **reduced** models:\n\n::: {.cell layout-align=\"center\" hash='02-lm-diagnostics-review_cache/html/pred2_ce9474b8fa031a7dc20b1f86a3f4a8b7'}\n\n```{.r .cell-code}\nx.te$pr.full <- predict(full, newdata = x.te)\nx.te$pr.reduced <- predict(reduced, newdata = x.te)\n```\n:::\n\nand compute the corresponding mean squared prediction errors:\n\n::: {.cell layout-align=\"center\" hash='02-lm-diagnostics-review_cache/html/mspe_576a3b0f33372ffa2f24a40ae736676f'}\n\n```{.r .cell-code}\nwith(x.te, mean((MORT - pr.full)^2))\n#> [1] 2859.367\nwith(x.te, mean((MORT - pr.reduced)^2))\n#> [1] 1861.884\n```\n:::\n\n\nNote that the reduced model (that did not fit the data\nas well as the full model) nevertheless produced\nbetter predictions (smaller mean squared prediction\nerrors) on the test set. \n\nAt this point you should put on your critical / skeptical \nhat and wonder if this did not happen *by chance*, i.e.\nif this may be just \nan artifact of the specific training/test partition\nwe used. The following simple experiment shows that\nthis is not the case. It would be a **very good exercise** \nfor you to repeat it many times (100, say) to verify\nmy claim.\n\nFirst, read the whole data and create a new\ntraining / test random split.\n\n::: {.cell layout-align=\"center\" hash='02-lm-diagnostics-review_cache/html/cvexperiment1_f0af1700f54cf1745ab5bda62370f6fb'}\n\n```{.r .cell-code}\n# repeat with different partitions\nx <- read.csv(\"data/rutgers-lib-30861_CSV-1.csv\")\nset.seed(456)\nii <- sample(rep(1:4, each = 15))\nx.tr <- x[ii != 2, ]\nx.te <- x[ii == 2, ]\n```\n:::\n\nIn the above code chunk, I used `x.tr` to denote the \ntraining set and `x.te` for the test set. \nNow, fit the full and reduced models \non this new training set:\n\n::: {.cell layout-align=\"center\" hash='02-lm-diagnostics-review_cache/html/cvexperiment2_e4141c5807d5c89df42d0e8fcde8a79f'}\n\n```{.r .cell-code}\nfull <- lm(MORT ~ ., data = x.tr)\nreduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\n```\n:::\n\nFinally, estimate the mean squared prediction\nerror of these models with their squared prediction\nerror on the test set:\n\n::: {.cell layout-align=\"center\" hash='02-lm-diagnostics-review_cache/html/cvexperiment3_ec44f726fcdabe900e22d552beac1cc6'}\n\n```{.r .cell-code}\nx.te$pr.full <- predict(full, newdata = x.te)\nx.te$pr.reduced <- predict(reduced, newdata = x.te)\nwith(x.te, mean((MORT - pr.full)^2))\n#> [1] 2194.79\nwith(x.te, mean((MORT - pr.reduced)^2))\n#> [1] 1393.885\n```\n:::\n\nNote that the estimated mean squared prediction error\nof the reduced model is again considerably smaller\nthan that of the full model (even though the latter always fits the \ntraining set better than the reduced one).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}