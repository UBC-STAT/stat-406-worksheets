{
  "hash": "248f3a1a8a667d0c446ad3135376a9db",
  "result": {
    "markdown": "# Parametric classifiers\n\n\n\n\n\n\nAs we discussed in class, what is commonly referred to as *classification* \ncan be thought of as prediction, when the responses are classes and we\nuse a particular loss function (the *0-1 loss* we discussed in class). \nFurthermore, it is easy to show (which we did in class) that the optimal \nclassifier (in terms of minimizing the\nexpected misclassification error) is the one that assigns an observation\nto the class with the highest probability of occuring, conditional to the\nvalue of the observed explanatory variables. \n\n<!-- A related discussion about including costs of misclassification  -->\n<!-- and the difference between prediction and classification can be found here: [http://www.fharrell.com/post/classification/](http://www.fharrell.com/post/classification/). -->\n\nMost (if not all) classification methods we will cover in this course can be\nsimply thought of as different approaches to estimate the conditional probability of\neach class, conditional on the value of the explanatory variables. In \nsymbols: `P( G = g | X = x_0)`.\nThe obvious parallel with what we have done before in this class,\nis that many (all?) regression methods we discussed in class are \ndifferent ways of estimating the conditional mean of the response \nvariable (conditional on the value of the explanatory\nvariables). Here we are\nin fact estimating the whole conditional distribution of `G`\ngiven `X = x_0`; \nin symbols: `G | X = x_0`.\n\nAs in the regression case, there are different ways to estimate this\noptimal predictor / classifier. Some will be model-based, some will\nbe non-parametric in nature. And some can be considered \"restricted\" \nnon-parametric methods (without relying on a model, but imposing some\nother type of constrain on the shape of the classifier). The equivalent\nmethods for regression with continuous responses are: linear\nor non-linear regression as model-based methods; kernel or local regression\nas non-parametric methods; and splines or regression trees as \n\"constrained\" (regularized?) non-parametric methods. \n\nBelow we first discuss model-based methods (Linear / Quadratic\nDiscriminant Analysis and logistic regression). non-parametric methods (nearest-neighbours and \nclassification trees) will be discussed later. \n\n## Linear Discriminant Analysis\n\nProbably the \"second easiest approach\"to estimate the above probability \n(what would be the easiest one?) is to model the distribution of \nthe explanatory variables **within** each class (that is, to \nmodel the distribution of `X | G = g`\nfor each possible class `g`). \nThese conditional distributions will then uniquely determine the \nprobabilities we need to estimate, as discussed above and in class. \nIn particular, one the simplest models we can use \nfor `X | G = g`\nis a Normal (Gaussian) multivariate distribution. \nAs we saw in class, if we assume that the distribution of the features\nfor each class is Gaussian with a common covariance matrix across clases, then\nit easy to show (**and I strongly suggest that you do it**) that the optimal\nclassifier (using the 0-1 loss function mentioned above) is a linear\nfunction of the explanatory variables. The coefficients of this linear\nfunction depend on the parameters of the assumed Gaussian distributions, \nwhich can be estimated using MLE on the training set. Plugging these\nparameter estimates in `P( G = g | X)` \nprovides a natural estimator of each of these conditional probabilities, \nand thus we can compute an approximation to the optimal classifier. \n\nThe function `lda` in \nthe `MASS` library implements this simple classifier. We illustrate it\nhere on the rather simple and well-known vaso constriction data, available in the \n`robustbase` package. More details, as usual, can be found on its\nhelp page. The response variable takes two values (represented below\nas **blue** and **red**), and there are only two\nexplanatory variables (which allows us to visualize our methods and results). \n\n\n::: {.cell layout-align=\"center\" hash='30-lda-logit_cache/html/lda1_7c31ab6a3119118ff7a7f74c1e126fb9'}\n\n```{.r .cell-code}\ndata(vaso, package = \"robustbase\")\nplot(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\n```\n\n::: {.cell-output-display}\n![](30-lda-logit_files/figure-html/lda1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nTo train the LDA classifier we use the function `lda` as follows (note the\n**model-like** syntax to indicate the response and explanatory variables):\n\n\n::: {.cell layout-align=\"center\" hash='30-lda-logit_cache/html/lda1.1_552d774ab042fb12ce6c547cc4da3365'}\n\n```{.r .cell-code}\nlibrary(MASS)\na.lda <- lda(Y ~ Volume + Rate, data = vaso)\n```\n:::\n\n\nNow, given any value of the explanatory variables `(Volume, Rate)` we\ncan use the method `predict` on the object returned by `lda()` to \nestimate the conditional probabilities of **blue** and **red**. \n\n\nTo visualize which regions of the feature space will be predicted to \ncontain **blue** points (and then obviously which areas will be \npredicted to correspond to **red** responses) we will\nconstruct a relatively fine 2-dimensional grid of posible values of\nthe explanatory variables (`(Volume, Rate)`):\n\n::: {.cell layout-align=\"center\" hash='30-lda-logit_cache/html/lda1.2.0_59bbf609caf0ab4a071c77ec3da80343'}\n\n```{.r .cell-code}\nxvol <- seq(0, 4, length = 200)\nxrat <- seq(0, 4, length = 200)\nthe.grid <- expand.grid(xvol, xrat)\nnames(the.grid) <- c(\"Volume\", \"Rate\")\n```\n:::\n\nand estimate the probabilities of the 2 classes for each point in this grid:\n\n::: {.cell layout-align=\"center\" hash='30-lda-logit_cache/html/lda1.2.1_c534e6df919d0a30d700e1851e9ded09'}\n\n```{.r .cell-code}\npr.lda <- predict(a.lda, newdata = the.grid)$posterior\n```\n:::\n\nFinally, we plot the corresponding \"surface\" of predictions for one class\n(i.e. the conditional probabilites for that class as a function of\nthe explanatory variables):\n\n::: {.cell layout-align=\"center\" hash='30-lda-logit_cache/html/lda1.2_f3c8eee0b9e31ca298cf217455060def'}\n\n```{.r .cell-code}\nimage(xrat, xvol, matrix(pr.lda[, 2], 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", main = \"LDA\"\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\n```\n\n::: {.cell-output-display}\n![](30-lda-logit_files/figure-html/lda1.2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nOn the plot above higher numbers are shown with lighther colors\n(dark green corresponds to very low conditional probabilities). \n\n### Further considerations\n\nThis model-based approach to classification (LDA) is optimal\nif the model is correct. The strongest assumption of this model\nis, of course, the Gaussian conditional distribution of the vector of \nexplanatory variables: `X | G = g` has a `N( mu, Sigma)` distribution. \nThe second strongest assumption is\nthat of equal \"shape\" (in other words, that the covariance matrix\n`Sigma` above does not depend on `g`). This latter assumption \ncan be relaxed slightly if\nwe assume instead that the features have a Gaussian distribution within each\nclass, but that the covariance matrix may be different across \nclasses. \nIn symbols, if we assume that \n`X | G = g` has a `N( mu, Sigma_g)` distribution.\nThe corresponding optimal classifier is now a quadratic\nfunction of the predictors (**prove it!**). The function `qda` \nin the `MASS` library implements this classifier, and it\ncan be used just like `lda` (as usual, refer to its help page for details).\n\nThis approach can be used with any number of classes. Can you think of any limitations? \n\n## Logistic regression (Review)\n\nIf we model the distribution of the  features within each class using a \nmultivariate Gaussian distribution, then it is easy to see that the \nboundaries between classes are linear functions of the features (**verify this!**)\nFurthermore, the log of the odds ratio between classes is a linear\nfunction. It is interesting to note that one can start with this last\nassumption (instead of the full Gaussian model) and arrive at a\nfully parametric model for the conditional distibution of the classes\ngiven the features (see the class slides). The parameters can be\nestimated using maximum likelihood. For two classes this is the\nlogistic regression model, which you may have seen in previous\ncourses. \n\nWe illustrate this on the `vaso` data as before. Since this is\na 2-class problem, we just need to fit a logistic regression model.\nThe function `glm` in `R` does it for us, we specify that we \nwant to fit such a model using the argument `family=binomial`. \nOnce we obtain parameter estimators (in the `glm` object `a` below), \nwe use the `predict` method to obtain predicted conditional \nprobabilities on the same grid we used before:\n\n::: {.cell layout-align=\"center\" hash='30-lda-logit_cache/html/logistic1_da2aa607267722d856753c2a5aa41aa0'}\n\n```{.r .cell-code}\na <- glm(Y ~ ., data = vaso, family = binomial)\npr <- predict(a, newdata = the.grid, type = \"response\")\n```\n:::\n\n\nWe now plot the data and the *surface* of predicted probabilities for \nblue points (higher probabilites are displayed with lighter colors). \n\n\n::: {.cell layout-align=\"center\" hash='30-lda-logit_cache/html/logistic2_f31ef4fb5baddaf5df5670c090ffd230'}\n\n```{.r .cell-code}\nimage(xrat, xvol, matrix(pr, 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", main = \"Logistic\"\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\n```\n\n::: {.cell-output-display}\n![](30-lda-logit_files/figure-html/logistic2-1.png){fig-align='center' width=90%}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}