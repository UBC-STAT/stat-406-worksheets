{
  "hash": "7b02a414dac4711bd7275be967d45923",
  "result": {
    "markdown": "# Regression trees \n\n\n\n\n\n\n\nTrees provide a non-parametric regression estimator that is\nable to overcome a serious limitation of \"classical non-parametric\"\nestimators (like those based on splines, or kernels) \nwhen several (more than 2 or 3) explanatory variables are\navailable. \n\nBelow we first describe the problem afflicting classical \nnon-parametric methods (this is also known as the \"curse of dimensionality\")\nand then describe how to compute regression trees in `R` using the\n`rpart` package (although other implementations exist). \nDetails were discussed in class. \n\n## Curse of dimensionality\n\nSuppose  you have a random sample of *n = 100* observations,\nuniformly distributed on the [0, 1] interval. How many do you \nexpect to find within 0.25 of the middle point of the\ninterval (i.e. how many will be between 0.25 and 0.75)?\nA trivial calculation shows that the expected number of\nobservations falling between 0.25 and 0.75 will be *n/2*,\nin this case *50*. This is easy verified with a simple\nnumerical experiment:\n\n::: {.cell layout-align=\"center\" hash='24-trees_cache/html/curse_74b24eb3c0e8dc851666dc60b36c90b4'}\n\n```{.r .cell-code}\n# X ~ U(0,1)\n# how many points do you expect within 0.25 of 1/2?\nset.seed(1234)\nn <- 100\nx <- runif(n)\n(sum(abs(x - 1 / 2) < 0.25)) # half the width of the dist'n\n#> [1] 50\n```\n:::\n\n(wow! what are the chances?)\n\nConsider now a sample of 100 observations, each with \n5 variables (5-dimensional observations), \nuniformly distributed in the 5-dimensional unit cube \n(*[0,1]^5*). How many do you expect to see in the\n*central hypercube* with sides [0.25, 0.75] x [0.25, 0.75] ... \nx [0.25, 0.75] = [0.25, 0.75]^5? A simple experiment shows\nthat this number is probably rather small:\n\n::: {.cell layout-align=\"center\" hash='24-trees_cache/html/curse.p5_b76df478cf924805421abe4eb761cff8'}\n\n```{.r .cell-code}\np <- 5\nx <- matrix(runif(n * p), n, p)\n# how many points in the hypercube (0.25, 0.75)^p ?\ntmp <- apply(x, 1, function(a) all(abs(a - 1 / 2) < 0.25))\n(sum(tmp))\n#> [1] 4\n```\n:::\n\nIn fact, the expected number of observations \nin that central hypercube is exactly *n / 2^5*, \nwhich is approximately *3* when *n = 100*. \n\nA relevant question for our local regression estimation\nproblem is: \"how large should our sample be if we want\nto still have about 50 observations in our central hypercube?\". \nEasy calculations show that this number is *50 / (1/2)^p*,\nwhich, for *p = 5* is *1600*. Again, we can verify\nthis with a simple experiment:\n\n::: {.cell layout-align=\"center\" hash='24-trees_cache/html/curse.2_24c100776147a57b92d0dec4a6f35b50'}\n\n```{.r .cell-code}\n# how many obs do we need to have 50 in the hypercube?\nn <- 50 / (0.5^p)\nx <- matrix(runif(n * p), n, p)\n# how many points in the hypercube (0.25, 0.75)^p ?\ntmp <- apply(x, 1, function(a) all(abs(a - 1 / 2) < 0.25))\n(sum(tmp))\n#> [1] 57\n```\n:::\n\nSo we see that if the dimension of our problem increases \nfrom *p = 1* to *p = 5*, the number of observations we \nneed to maintain an expectation of having about 50 points\nin our central hypercube increases by a factor of 16 (not 5).\nHowever, if we  double the dimension of the problem (to *p = 10*), in order to expect\n50 observations in the central [0.25, 0.75] hypercube\nwe need a sample of size *n = 51,200*. In other words, we\ndoubled the dimension, but need 32 times more data (!)\nto *fill* the central hypercube with the same number of points. \nMoreover, if we doubled the dimension again (to *p = 20*) we would need over \n52 million observations to have (just!) 50 in the central hypercube! \nNote that now we doubled the\ndimension again but need 1024 times more data! The number of\nobservations needed to maintain a fixed number of observations\nin a region of the space grows exponentially with the \ndimension of the space. \n\nAnother way to think about this problem is to \nask: \"given a sample size of *n = 1000*, say, how wide / large\nshould the central hypercube be to expect \nabout *50* observations in it?\". The answer is\neasily found to be *1 / (2 (n/50)^(1/p))*, which for \n*n = 1000* and *p = 5* equals 0.27, with \n*p = 10* is 0.37 and with *p = 20* is \n0.43, almost the full unit hypercube!\n\nIn this sense it is fair to say that in moderate to high dimensions\n*local neighbourhoods* are either empty or not really *local*.\n\n\n\n## Regression trees as constrained non-parametric regression\n\nRegression trees provide an alternative non-regression\nestimator that works well, even with many available features. \nAs discussed in class, the basic idea is to approximate the\nregression function by a linear combination of \"simple\" \nfunctions (i.e. functions $h(x) = I( x \\in A )$ which equal\n1 if the argument *x* belongs to the set *A*, and 0 otherwise. \nEach function has its own support set *A*. Furthermore, \nthis linear combination is not estimated at once, but\niteratively, and only considering a specific class of\nsets *A* (which ones?) As a result, the regression tree\nis not the *global* optimal approximation by simple functions, \nbut a good *suboptimal* one, that can be computed very rapidly.\nDetails were discussed in class, refer to your notes and\nthe corresponding slides.\n\nThere are several packages in `R` implementing trees,\nin this course we will use `rpart`. To illustrate their\nuse we will consider the `Boston` data set, that contains\ninformation on housing in the US city of Boston. The \ncorresponding help page contains more information. \n\n**Here, to simplify the comparison** of the predictions obtained\nby trees and other regression estimators, instead of using\nK-fold CV, we start by randomly splitting the \navailable data into a training and a test set:\n\n::: {.cell layout-align=\"center\" hash='24-trees_cache/html/tree_2a170210b673b13e87b846a161823c42'}\n\n```{.r .cell-code}\nlibrary(rpart)\ndata(Boston, package = \"MASS\")\n# split data into a training and\n# a test set\nset.seed(123456)\nn <- nrow(Boston)\nii <- sample(n, floor(n / 4))\ndat.te <- Boston[ii, ]\ndat.tr <- Boston[-ii, ]\n```\n:::\n\n\nWe now build a regression tree using the function `rpart` and leave most of \nits arguments to their\ndefault values. We specify the response and explanatory variables using\na `formula`, as usual, and set  `method='anova'` to indicate we\nwant to train a regression tree (as opposed to a classification one, for example).\nFinally, we use the corresponding `plot` method to display the tree\nstructure:\n\n\n\n::: {.cell layout-align=\"center\" hash='24-trees_cache/html/tree3_499a4c340ea8680555365411f2f9e757'}\n\n```{.r .cell-code}\nset.seed(123)\nbos.t <- rpart(medv ~ ., data = dat.tr, method = \"anova\")\nplot(bos.t, uniform = FALSE, margin = 0.05)\ntext(bos.t, pretty = TRUE)\n```\n\n::: {.cell-output-display}\n![](24-trees_files/figure-html/tree3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nA few questions for you:\n\n* Why did we set the pseudo-random generation seed (`set.seed(123)`) \nbefore calling `rpart`? Is there anything random about building these trees?\n* What does the `uniform` argument for `plot.rpart` do? What does `text` do here? \n\n## Compare predictions \n\nWe now compare the predictions we obtain on the test with the \nabove regression tree, the usual linear model using all\nexplanatory variables, another one constructed using stepwise\nvariable selections methods, and the \"optimal\" LASSO. \n\nFirst, we estimate the MSPE of the regression tree using the test set:\n\n::: {.cell layout-align=\"center\" hash='24-trees_cache/html/tree4_6d1dc098f493504caa43c5169bfe2740'}\n\n```{.r .cell-code}\n# predictions on the test set\npr.t <- predict(bos.t, newdata = dat.te, type = \"vector\")\nwith(dat.te, mean((medv - pr.t)^2))\n#> [1] 16.07227\n```\n:::\n\nFor a full linear model, the estimated MSPE using the test set is:\n\n::: {.cell layout-align=\"center\" hash='24-trees_cache/html/tree5_a36875925e96f6dc2a6cec3235b743cc'}\n\n```{.r .cell-code}\n# full linear model\nbos.lm <- lm(medv ~ ., data = dat.tr)\npr.lm <- predict(bos.lm, newdata = dat.te)\nwith(dat.te, mean((medv - pr.lm)^2))\n#> [1] 23.25844\n```\n:::\n\nThe estimated MSPE of a linear model constructed via stepwise is:\n\n::: {.cell layout-align=\"center\" hash='24-trees_cache/html/tree6_e05ba06df08f92ed47f83c57df99dede'}\n\n```{.r .cell-code}\nlibrary(MASS)\nnull <- lm(medv ~ 1, data = dat.tr)\nfull <- lm(medv ~ ., data = dat.tr)\nbos.aic <- stepAIC(null, scope = list(lower = null, upper = full), trace = FALSE)\npr.aic <- predict(bos.aic, newdata = dat.te)\nwith(dat.te, mean((medv - pr.aic)^2))\n#> [1] 22.99864\n```\n:::\n\nFinally, the estimated MSPE of the \"optimal\" LASSO fit is:\n\n::: {.cell layout-align=\"center\" hash='24-trees_cache/html/tree7_978979f0d219e8021a15cab51b09b8f5'}\n\n```{.r .cell-code}\n# LASSO?\nlibrary(glmnet)\nx.tr <- as.matrix(dat.tr[, -14])\ny.tr <- as.vector(dat.tr$medv)\nset.seed(123)\nbos.la <- cv.glmnet(x = x.tr, y = y.tr, alpha = 1)\nx.te <- as.matrix(dat.te[, -14])\npr.la <- predict(bos.la, s = \"lambda.1se\", newx = x.te)\nwith(dat.te, mean((medv - pr.la)^2))\n#> [1] 26.58914\n```\n:::\n\n\nNote that the regression tree appears to have the best MSPE, although\nwe cannot really assess whether the observed differences are beyond\nthe uncertainty associated with our MSPE estimators. In other words,\nwould these differences still be so if we used a different \ntraining / test data split? In fact, a very good exercise for you \nwould be to repeat the above comparison using **many**\ndifferent training/test splits, or even better: using all the data for\ntraining and K-fold CV to estimate the different MSPEs. \n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}