{
  "hash": "7732e443734a7d3c052fc99f96e53e5d",
  "result": {
    "markdown": "# Single layer neural network\n\n\n\n\n\n\nThis example using the ISOLET data illustrates the use of simple\nneural networks (NNs), and also highlights some issues of which it may \nbe important to be aware. As we discussed in class, NNs typically have \nmore parameters than observations and a number of tuning parameters\nthat need to be chosen by the user. Among these: the number of \nhidden layers, the number of units in each layer, the *activation function*,\nthe *loss function*, a decaying factor, and the initial point \nat which to start the optimization iterations. In the example below we illustrate \nsome difficulties that can be encountered when trying to find \nwhich tuning parameters to use to train a NN.\n\nIn order to focus on the concepts behind NN, we will use the `nnet` \npackage in `R`. This package is a very simple implementation \nof NNs with a single hidden layer, and relies on standard optimization\nalgorithms to train it. Such simple setting will allow us to \nseparate implementation / optimization issues from the underlying\nmodel and ideas behind NN, which carry over naturally to more\ncomplex NNs. \n\nFor our example we will use again the ISOLET data which is available here: [http://archive.ics.uci.edu/ml/datasets/ISOLET](http://archive.ics.uci.edu/ml/datasets/ISOLET), along with more information about it. It contains data on sound recordings of 150 speakers saying each letter of the alphabet (twice). See the original source for more details. The full data file is rather large and available in compressed form. \nInstead, we will read it from a private copy in plain text form I made \navailable on Dropbox.  \n\n## \"C\" and \"Z\"\nFirst we look at building a classifier to identify the letters C and Z. This \nis the simplest scenario and it will help us fix ideas. We now read the \nfull data set, and extract the training and test rows corresponding to those\ntwo letters:\n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolet0_8ccb29bbcd27a76d6e4aff1bdc14d7b7'}\n\n```{.r .cell-code}\nlibrary(nnet)\nxx.tr <- readRDS(\"data/isolet-train.RDS\")\nxx.te <- readRDS(\"data/isolet-test.RDS\")\nlets <- c(3, 26)\nLETTERS[lets]\n#> [1] \"C\" \"Z\"\n# Training set\nx.tr <- xx.tr[xx.tr$V618 %in% lets, ]\nx.tr$V618 <- as.factor(x.tr$V618)\n# Test set\nx.te <- xx.te[xx.te$V618 %in% lets, ]\ntruth <- x.te$V618 <- as.factor(x.te$V618)\n```\n:::\n\nWe train a NN with a single hidden layer, and a single unit in the hidden layer. \n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolet1_05c7399ce42d7f436cdba75497a4e943'}\n\n```{.r .cell-code}\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 1, decay = 0, maxit = 1500, MaxNWts = 2000)\n#> # weights:  620\n#> initial  value 350.425020 \n#> iter  10 value 41.176789\n#> iter  20 value 18.095256\n#> iter  30 value 18.052107\n#> iter  40 value 18.050646\n#> iter  50 value 18.050036\n#> iter  60 value 18.048042\n#> iter  70 value 12.957465\n#> iter  80 value 6.912100\n#> iter  90 value 6.483391\n#> iter 100 value 6.482796\n#> iter 110 value 6.482767\n#> iter 120 value 6.482733\n#> iter 120 value 6.482733\n#> final  value 6.482722 \n#> converged\n```\n:::\n\nNote the slow convergence. The final value of the objective value was:\n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolet1.1_0879188993bf90505ced19c0097d9fa5'}\n\n```{.r .cell-code}\na1$value\n#> [1] 6.482722\n```\n:::\n\nThe error rate on the training set (\"goodness of fit\") is\n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolet1.2_82171c809bd75fd69225d87e4f915367'}\n\n```{.r .cell-code}\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0.002083333\n```\n:::\n\nWe see that this NN fits the training set perfectly. Is this desirable? \n\nWe now run the algorithm again, with a different starting point. \n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolet2_bd72543e6accb35a65b1312b3a5abf6f'}\n\n```{.r .cell-code}\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 1, decay = 0, maxit = 1500, MaxNWts = 2000)\n#> # weights:  620\n#> initial  value 336.934868 \n#> iter  10 value 157.630462\n#> iter  20 value 61.525474\n#> iter  30 value 48.367799\n#> iter  40 value 42.896353\n#> iter  50 value 37.039697\n#> iter  60 value 36.481582\n#> iter  70 value 27.239536\n#> iter  80 value 20.422772\n#> iter  90 value 20.410547\n#> final  value 20.410540 \n#> converged\n```\n:::\n\nCompare\nthe attained value of the objective and the error rate on the training set\nwith those above (6.482722 and 0, respectively):\n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolet2.1_beaaf54c34c840c16d0605afbdad711a'}\n\n```{.r .cell-code}\na2$value\n#> [1] 20.41054\nb2 <- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#> [1] 0.008333333\n```\n:::\n\nSo, we see that the second run of NN produces a much worse solution.\nHow are their performances on the test set?\n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolet2.2_2b78dfe6b4fc7656b54678ec2872822c'}\n\n```{.r .cell-code}\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.03333333\nb2 <- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#> [1] 0.03333333\n```\n:::\n\nThe second (worse) solution performs better on the test set. \n\nWhat if we add more units to the hidden layer? We increase the\nnumber of units on the hidden layer from 3 to 6. \n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolet3_39fcc538f3de8bb13bdad11f9a64beb6'}\n\n```{.r .cell-code}\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 3, decay = 0, maxit = 1500, MaxNWts = 2000, trace = FALSE)\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 3, decay = 0, maxit = 1500, MaxNWts = 2000, trace = FALSE)\n```\n:::\n\nThe objective functions are \n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolet3.1_3eb001d4ee03c53654711d213e44ba97'}\n\n```{.r .cell-code}\na1$value\n#> [1] 6.482738\na2$value\n#> [1] 9.052402e-05\n```\n:::\n\nrespectively, and their performance on the training and test sets are:\n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolet4_1c32b972da9328e49462bd7e0bba7377'}\n\n```{.r .cell-code}\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0.002083333\nb2 <- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#> [1] 0\n\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.03333333\nb2 <- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#> [1] 0.04166667\n```\n:::\n\nAgain we note that the (seemingly much) worse solution (in terms of the objective\nfunction whose optimization defines the NN) performs better \non the test set. \n\nWhat if we add a decaying factor as a form of regularization? \n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolet10_6d6d64e7a2e0ccd13a687eea34c1b56d'}\n\n```{.r .cell-code}\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 3, decay = 0.05, maxit = 500, MaxNWts = 2000, trace = FALSE)\na1$value\n#> [1] 5.345279\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 3, decay = 0.05, maxit = 500, MaxNWts = 2000, trace = FALSE)\na2$value\n#> [1] 5.345279\n```\n:::\n\nNow the two solutions starting from these random initial values \nare the same (the reader is encouraged to \ntry more random starts). How does this NN do on the training and test sets?\n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolet10.1_646f06c4ff1fb18c8e6f380bd18b8eb8'}\n\n```{.r .cell-code}\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.008333333\n```\n:::\n\n\nNote that this \"regularized\" solution which corresponds to a \nslightly better solution than the worse one above in terms\nof objective function (but still much worse than the best ones)\nperforms noticeably better on the test set. This seem to suggest\nthat it is not easy to select which of the many local extrema to used\nbased  on the objective function values they attain. \n\nAnother tuning parameter we can vary is the number of units\nin the hidden layer, which will also increase significantly the\nnumber of possible weight parameters in our model. \nThe above solution uses 1858 weights. We now add more \nunits to the hidden layer (6 instead of 3) and increase the limit on\nthe number of allowable weights to 4000: \n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolet11_509eab3bfcb086da2bcdcad775c3081a'}\n\n```{.r .cell-code}\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.05, maxit = 500, MaxNWts = 4000, trace = FALSE)\na1$value\n#> [1] 4.172022\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.05, maxit = 500, MaxNWts = 4000, trace = FALSE)\na2$value\n#> [1] 4.172023\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0\n\nb2 <- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#> [1] 0\n\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.008333333\n\nb2 <- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#> [1] 0.008333333\n```\n:::\n\nNote that both of these two distinct solutions fit the training set \nexactly (0 apparent error rate), and have the same performance\non the test set. We leave it to the reader to perform a more\nexhaustive study of the prediction properties of these solutions\nusing an appropriate CV experiment. \n\n## More letters\n\nWe now repeat the same exercise above but on a 4-class\nsetting. \n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/iso20_494c4afd955c22b1831579e1018a205d'}\n\n```{.r .cell-code}\nlets <- c(3, 7, 9, 26)\nx.tr <- xx.tr[xx.tr$V618 %in% lets, ]\nx.tr$V618 <- as.factor(x.tr$V618)\n# testing set\nx.te <- xx.te[xx.te$V618 %in% lets, ]\ntruth <- x.te$V618 <- as.factor(x.te$V618)\n```\n:::\n\n\nThe following tries show that a NN with \nonly one unit in the hidden layer does not perform well.\nAs before, we compare two local minima of the NN training\nalgorithm. First we show the values of the\ncorresponding local minima of the objective function, and then\ntheir error rates on the training and test sets.\n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/iso20.1_c395b9a76e1096964eee98cb7bcfed65'}\n\n```{.r .cell-code}\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 1, decay = 0, maxit = 1500, MaxNWts = 2000, trace = FALSE)\na1$value\n#> [1] 6.482735\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 1, decay = 0, maxit = 1500, MaxNWts = 2000, trace = FALSE)\na2$value\n#> [1] 789.9009\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0.001041667\nb2 <- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#> [1] 0.5010417\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.4708333\nb2 <- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#> [1] 0.4791667\n```\n:::\n\nNote that the error rates on the test set are\n0.471 and \n0.479, which are\nvery high.\nBetter results are obtained with 6 units on the hidden layer\nand a slightly regularized solution. As before, \nuse two runs of the training\nalgorithm and look at the corresponding values of the\nobjective function, and the error rates \nof both NNs on the training and test sets.\n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolate31_ad0f537f52de5721516848a0a10162f9'}\n\n```{.r .cell-code}\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.05, maxit = 500, MaxNWts = 4000, trace = FALSE)\na1$value\n#> [1] 9.037809\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.05, maxit = 500, MaxNWts = 4000, trace = FALSE)\na2$value\n#> [1] 9.171046\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0\nb2 <- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#> [1] 0\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.0125\nb2 <- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#> [1] 0.0125\n```\n:::\n\nThe error rates on the test set are now \n0.013 and \n0.013, which are\nmuch better than before.\n\n## Even more letters\n\nWe now consider building a classifier with 7 classes, which \nis a more challenging problem. \n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolate40_c6d7758bb25b778903dbf9152a5d6e79'}\n\n```{.r .cell-code}\nlets <- c(3, 5, 7, 9, 12, 13, 26)\nLETTERS[lets]\n#> [1] \"C\" \"E\" \"G\" \"I\" \"L\" \"M\" \"Z\"\nx.tr <- xx.tr[xx.tr$V618 %in% lets, ]\nx.tr$V618 <- as.factor(x.tr$V618)\n# testing set\nx.te <- xx.te[xx.te$V618 %in% lets, ]\ntruth <- x.te$V618 <- as.factor(x.te$V618)\n```\n:::\n\nThe following code trains a NN with 6 units on the hidden layer and\nmoderate regularization (via a decaying factor of `0.3` and \nan upper limit of 4000 weights).\n\n::: {.cell layout-align=\"center\" hash='45-single-layer-nn_cache/html/isolate40.1_aa1201f128a72ac4df7be035e12d9037'}\n\n```{.r .cell-code}\nset.seed(123)\na1 <- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.3, maxit = 1500, MaxNWts = 4000, trace = FALSE)\na1$value\n#> [1] 102.1805\nset.seed(456)\na2 <- nnet(V618 ~ ., data = x.tr, size = 6, decay = 0.3, maxit = 1500, MaxNWts = 4000, trace = FALSE)\na2$value\n#> [1] 100.5938\nb1 <- predict(a1, type = \"class\") # , type='raw')\nmean(b1 != x.tr$V618)\n#> [1] 0\nb2 <- predict(a2, type = \"class\") # , type='raw')\nmean(b2 != x.tr$V618)\n#> [1] 0\nb1 <- predict(a1, newdata = x.te, type = \"class\") # , type='raw')\nmean(b1 != x.te$V618)\n#> [1] 0.01909308\nb2 <- predict(a2, newdata = x.te, type = \"class\") # , type='raw')\nmean(b2 != x.te$V618)\n#> [1] 0.01193317\n```\n:::\n\nNote that in this case the NN with a better objective\nfunction (100.5938029 versus 102.1805373) achieves a better performance on \nthe test set (0.012 \nversus 0.019), although the\ndifference is rather small. Conclusions based on a \nproper CV study would be much more reliable.\n\nYou are strongly encouraged to study what happens with other\ncombinations of decay, number of weights and number of units\non the hidden layer, using a proper CV setting to evaluate\nthe results. \n\n<!-- #### Additional resources for discussion (refer to the lecture for context) -->\n\n<!-- * [https://arxiv.org/abs/1412.6572](https://arxiv.org/abs/1412.6572) -->\n<!-- * [https://arxiv.org/abs/1312.6199](https://arxiv.org/abs/1312.6199) -->\n<!-- * [https://www.axios.com/ai-pioneer-advocates-starting-over-2485537027.html](https://www.axios.com/ai-pioneer-advocates-starting-over-2485537027.html) -->\n<!-- * [https://medium.com/intuitionmachine/the-deeply-suspicious-nature-of-backpropagation-9bed5e2b085e](https://medium.com/intuitionmachine/the-deeply-suspicious-nature-of-backpropagation-9bed5e2b085e) -->\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}