{
  "hash": "1b3cdc4df4fc40dc2a3fcc90f566003f",
  "result": {
    "markdown": "# Comparing models\n\n\n\n\n\n\n\n## General strategy\n\nSuppose we have a set of competing models from which we want to choose the\n\"best\" one. In order to properly define our problem we need the following:\n\n* a list of models to be considered;\n* a numerical measure to compare any two models in our list;\n* a strategy (algorithm, criterion) to navigate the set of models; and\n* a criterion to stop the search. \n\nFor example, in stepwise methods the models under consideration in \neach step are those that differ from the current model only by one\ncoefficient (variable). The numerical measure used to compare models\ncould be AIC, or Mallow's Cp, etc. The strategy is to only consider \nsubmodels with one fewer variable than the current one, and we stop\nif either none of these \"p-1\" submodels is better than the current one, or \nwe reach an empty model. \n\n## What is AIC?\n\nOne intuitively sensible quantity that can be used to compare models is a\ndistance measuring how \"close\" the distributions implied by these models are from the actual stochastic process generating the data (here \"stochastic process\" refers to the random mechanism that generated the observations). In order to do this we need:\n\na. a distance / metric (or at least a \"quasimetric\") between models; and \na. a way of estimating this distance when the \"true\" model is unknown.\n\nAIC provides an unbiased estimator of the Kullback-Leibler divergence \nbetween the estimated model and the \"true\" one. See the lecture slides\nfor more details. \n\n## Using stepwise + AIC to select a model\n\nWe apply stepwise regression based on AIC to select a linear\nregression model for the airpollution data. In `R` we can use\nthe function `stepAIC` in package `MASS` to perform a stepwise\nsearch, for the synthetic data set discussed in class:\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/step1_7f91be11048124bb6fb7673262e06264'}\n\n```{.r .cell-code}\nset.seed(123)\nx1 <- rnorm(506)\nx2 <- rnorm(506, mean = 2, sd = 1)\nx3 <- rexp(506, rate = 1)\nx4 <- x2 + rnorm(506, sd = .1)\nx5 <- x1 + rnorm(506, sd = .1)\nx6 <- x1 - x2 + rnorm(506, sd = .1)\nx7 <- x1 + x3 + rnorm(506, sd = .1)\ny <- x1 * 3 + x2 / 3 + rnorm(506, sd = 2.2)\n\nx <- data.frame(\n  y = y, x1 = x1, x2 = x2,\n  x3 = x3, x4 = x4, x5 = x5, x6 = x6, x7 = x7\n)\n\nlibrary(MASS)\nnull <- lm(y ~ 1, data = x)\nfull <- lm(y ~ ., data = x)\nst <- stepAIC(null, scope = list(lower = null, upper = full), trace = FALSE)\n```\n:::\n\nIf you want to see the progression of the search step-by-step, set the\nargument `trace=TRUE` in the call to `stepAIC` above. \nThe selected model is automatically fit and returned, so that\nin the code above `st` is an object of class `lm` containing the\n\"best\" linear regression fit. \n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/best1_04605ab21631abc6a468b3e0d14f0d92'}\n\n```{.r .cell-code}\nst\n#> \n#> Call:\n#> lm(formula = y ~ x1 + x6, data = x)\n#> \n#> Coefficients:\n#> (Intercept)           x1           x6  \n#>   -0.000706     3.175239    -0.282906\n```\n:::\n\n\nWe will now compare the mean squared prediction errors of \nthe **full** model and that selected with **stepwise**. \nWe use 50 runs of 5-fold CV, and obtain\nthe following:\n\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/cv1_9fd4981316f7f20d0dcf8b829487b9a4'}\n\n```{.r .cell-code}\nk <- 5\nn <- nrow(x)\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 50\nmspe.t <- mspe.f <- mspe.st <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.t <- pr.f <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    x0 <- x[ii != j, ]\n    null0 <- lm(y ~ 1, data = x0)\n    full0 <- lm(y ~ ., data = x0) # needed for stepwise\n    true0 <- lm(y ~ x1 + x2, data = x0)\n    step.lm0 <- stepAIC(null0, scope = list(lower = null0, upper = full0), trace = FALSE)\n    pr.st[ii == j] <- predict(step.lm0, newdata = x[ii == j, ])\n    pr.f[ii == j] <- predict(full0, newdata = x[ii == j, ])\n    pr.t[ii == j] <- predict(true0, newdata = x[ii == j, ])\n  }\n  mspe.st[i] <- mean((x$y - pr.st)^2)\n  mspe.f[i] <- mean((x$y - pr.f)^2)\n  mspe.t[i] <- mean((x$y - pr.t)^2)\n}\nboxplot(mspe.st, mspe.f,\n  names = c(\"Stepwise\", \"Full\"),\n  col = c(\"gray60\", \"hotpink\"), ylab = \"MSPE\"\n)\n```\n\n::: {.cell-output-display}\n![](12-model-selection-aic_files/figure-html/cv1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote that since this is a synthetic data set, we can also\nestimate the MSPE of the **true** model (could we compute it analytically instead?)\nand compare it with that of the **full** and **stepwise** models. \nWe obtain:\n\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/cv2_1cca9ba20dca6f8d3e149d00154d684c'}\n\n```{.r .cell-code}\nboxplot(mspe.t, mspe.st, mspe.f,\n  names = c(\"True\", \"Stepwise\", \"Full\"),\n  col = c(\"tomato\", \"gray60\", \"hotpink\"), ylab = \"MSPE\"\n)\n```\n\n::: {.cell-output-display}\n![](12-model-selection-aic_files/figure-html/cv2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n### Stepwise applied to the \"air pollution\" data \n\nWe now use stepwise on the air pollution data to select a model, and\nestimate its MSPE using 5-fold CV. We compare the predictions of \nthis model with that of the full model. \n\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/stepair_c9b092d93f5b2b2e5369f4b7144591c1'}\n\n```{.r .cell-code}\nlibrary(MASS)\nairp <- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\nnull <- lm(MORT ~ 1, data = airp)\nfull <- lm(MORT ~ ., data = airp)\n(tmp.st <- stepAIC(full, scope = list(lower = null), trace = FALSE))\n#> \n#> Call:\n#> lm(formula = MORT ~ PREC + JANT + JULT + OVR65 + POPN + EDUC + \n#>     NONW + HC + NOX, data = airp)\n#> \n#> Coefficients:\n#> (Intercept)         PREC         JANT         JULT        OVR65         POPN  \n#>   1934.0539       1.8565      -2.2620      -3.3200     -10.9205    -137.3831  \n#>        EDUC         NONW           HC          NOX  \n#>    -23.4211       4.6623      -0.9221       1.8710\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/mspe.air_be5d262b0e0739c4186719ba9b12a74e'}\n\n```{.r .cell-code}\nk <- 5\nn <- nrow(airp)\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 50\nmspe.f <- mspe.st <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.f <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    x0 <- airp[ii != j, ]\n    null0 <- lm(MORT ~ 1, data = x0)\n    full0 <- lm(MORT ~ ., data = x0) # needed for stepwise\n    step.lm0 <- stepAIC(null0, scope = list(lower = null0, upper = full0), trace = FALSE)\n    pr.st[ii == j] <- predict(step.lm0, newdata = airp[ii == j, ])\n    pr.f[ii == j] <- predict(full0, newdata = airp[ii == j, ])\n  }\n  mspe.st[i] <- mean((airp$MORT - pr.st)^2)\n  mspe.f[i] <- mean((airp$MORT - pr.f)^2)\n}\nboxplot(mspe.st, mspe.f,\n  names = c(\"Stepwise\", \"Full\"),\n  col = c(\"gray60\", \"hotpink\"), ylab = \"MSPE\"\n)\n```\n\n::: {.cell-output-display}\n![](12-model-selection-aic_files/figure-html/mspe.air-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe can also use the package `leaps` to run a more thorough search\namong all possible subsets. We do this with the air pollution data:\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/leaps1_803c567ba8f9cc0924ce66ab018d1a7a'}\n\n```{.r .cell-code}\nlibrary(leaps)\na <- leaps(x = as.matrix(airp[, -16]), y = airp$MORT, int = TRUE, method = \"Cp\", nbest = 10)\n```\n:::\n\nIn the call above we asked `leaps` to compute the 10 best models\nof each size, according to Mallow's Cp criterion. We can look at\nthe returned object\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/leaps.what_c9910a0ae222f4a380cddece7d21dbeb'}\n\n```{.r .cell-code}\nstr(a)\n#> List of 4\n#>  $ which: logi [1:141, 1:15] FALSE FALSE TRUE FALSE FALSE FALSE ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:141] \"1\" \"1\" \"1\" \"1\" ...\n#>   .. ..$ : chr [1:15] \"1\" \"2\" \"3\" \"4\" ...\n#>  $ label: chr [1:16] \"(Intercept)\" \"1\" \"2\" \"3\" ...\n#>  $ size : num [1:141] 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ Cp   : num [1:141] 53.6 82.3 82.6 97 97.2 ...\n```\n:::\n\nWe now find the best model (based on Mallow's Cp), and \nfit the corresponding model:\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/leaps.best_46af5eee2a93f91da9ba5ea88b0d048c'}\n\n```{.r .cell-code}\nj0 <- which.min(a$Cp)\n(m1 <- lm(MORT ~ ., data = airp[, c(a$which[j0, ], TRUE)]))\n#> \n#> Call:\n#> lm(formula = MORT ~ ., data = airp[, c(a$which[j0, ], TRUE)])\n#> \n#> Coefficients:\n#> (Intercept)         PREC         JANT         JULT         EDUC         NONW  \n#>   1180.3565       1.7970      -1.4836      -2.3553     -13.6190       4.5853  \n#>         SO.  \n#>      0.2596\n```\n:::\n\nWe compare which variables are used in this model with those\nused in the model found with stepwise:\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/leaps.step_eae8e793266056b19b9376c224b37e68'}\n\n```{.r .cell-code}\nformula(m1)[[3]]\n#> PREC + JANT + JULT + EDUC + NONW + SO.\nformula(tmp.st)[[3]]\n#> PREC + JANT + JULT + OVR65 + POPN + EDUC + NONW + HC + NOX\n```\n:::\n\nIt is reasonable to ask whether the model found by `leaps` is \nmuch better than the one returned by `stepAIC`:\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/leaps.vs.aic_9e4a1c1eecae3171cd039218b4ef29d1'}\n\n```{.r .cell-code}\nextractAIC(m1)\n#> [1]   7.0000 429.0017\nextractAIC(tmp.st)\n#> [1]  10.000 429.634\n```\n:::\n\nFinally, what is the MSPE of the model found by `leaps`?\n\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/mspe.leaps.cv_aec32f0995ea2cb58e5a18d1da8e97c0'}\n\n```{.r .cell-code}\n# proper way\nk <- 5\nn <- nrow(airp)\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 50\nmspe.l <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.l <- rep(0, n)\n  for (j in 1:k) {\n    x0 <- airp[ii != j, ]\n    tmp.leaps <- leaps(x = as.matrix(x0[, -16]), y = as.vector(x0[, 16]), int = TRUE, method = \"Cp\", nbest = 10)\n    j0 <- which.min(tmp.leaps$Cp)\n    step.leaps <- lm(MORT ~ ., data = x0[, c(tmp.leaps$which[j0, ], TRUE)])\n    pr.l[ii == j] <- predict(step.leaps, newdata = airp[ii == j, ])\n  }\n  mspe.l[i] <- mean((airp$MORT - pr.l)^2)\n}\nboxplot(mspe.st, mspe.f, mspe.l,\n  names = c(\"Stepwise\", \"Full\", \"Leaps\"),\n  col = c(\"gray60\", \"hotpink\", \"steelblue\"), ylab = \"MSPE\"\n)\n```\n\n::: {.cell-output-display}\n![](12-model-selection-aic_files/figure-html/mspe.leaps.cv-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote that a \"suboptimal\" model (stepwise) seems to be better than\nthe one found with a \"proper\" (exhaustive) search, as that returned by\n`leaps`. This is intriguing, but we will see the same phenomenon \noccur in different contexts later in the course. \n\n## An example where one may not need to select variables\n\nIn some cases one may not need to select a subset of explanatory\nvariables, and in fact, doing so may affect negatively the accuracy of\nthe resulting predictions. In what follows we discuss such an example. \nConsider the credit card data set that contains information\non credit card users. The interest is in predicting the \nbalance carried by a client. We first load the data, and to\nsimplify the presentation here we consider only the numerical\nexplanatory variables:\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/credit1_6f140f73f1fc46ce986eb916e4102c21'}\n\n```{.r .cell-code}\nx <- read.table(\"data/Credit.csv\", sep = \",\", header = TRUE, row.names = 1)\nx <- x[, c(1:6, 11)]\n```\n:::\n\nThere are 6 available covariates, and a stepwise search selects \na model with 5 of them (discarding `Education`):\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/credit2_70dc14eb73d6d402fcddb92f7f85cc1e'}\n\n```{.r .cell-code}\nlibrary(MASS)\nnull <- lm(Balance ~ 1, data = x)\nfull <- lm(Balance ~ ., data = x)\n(tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0))\n#> \n#> Call:\n#> lm(formula = Balance ~ Rating + Income + Limit + Age + Cards, \n#>     data = x)\n#> \n#> Coefficients:\n#> (Intercept)       Rating       Income        Limit          Age        Cards  \n#>   -449.3610       2.0224      -7.5621       0.1286      -0.8883      11.5527\n```\n:::\n\nIt is an easy exercise to check that the MSPE of this\nsmaller model is in fact worse than the one for the **full** one:\n\n\n::: {.cell layout-align=\"center\" hash='12-model-selection-aic_cache/html/credit3_2c4240214d9718d287c58d4c715fb153'}\n\n```{.r .cell-code}\nn <- nrow(x)\nk <- 5\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 100\nmspe.st <- mspe.f <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.f <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    null <- lm(Balance ~ 1, data = x[ii != j, ])\n    full <- lm(Balance ~ ., data = x[ii != j, ])\n    tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n    pr.st[ii == j] <- predict(tmp.st, newdata = x[ii == j, ])\n    pr.f[ii == j] <- predict(full, newdata = x[ii == j, ])\n  }\n  mspe.st[i] <- mean((x$Balance - pr.st)^2)\n  mspe.f[i] <- mean((x$Balance - pr.f)^2)\n}\nboxplot(mspe.st, mspe.f,\n  names = c(\"Stepwise\", \"Full\"),\n  col = c(\"tomato\", \"springgreen\"), cex.axis = 1.5,\n  cex.lab = 1.5, cex.main = 2\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n```\n\n::: {.cell-output-display}\n![](12-model-selection-aic_files/figure-html/credit3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n<!-- ## Shrinkage methods / Ridge regression  -->\n\n<!-- Stepwise methods are highly variable, and thus their predictions may not  -->\n<!-- be very accurate (high MSPE).  -->\n<!-- A different way to manage correlated explanatory variables (to \"reduce\" their -->\n<!-- presence in the model without removing them) is... -->\n\n<!-- ### Selecting the amount of shrinkage -->\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}