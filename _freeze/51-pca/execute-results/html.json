{
  "hash": "748ae1aaaebf422ad9170b22eb093671",
  "result": {
    "markdown": "\n\n# Introduction\n\n\n\n\n\n\nUnsupervised learning methods differ from the \nsupervised ones we have studied so far in that \nthere is no response variable. The objective is\nnot related to prediction but rather\nto the identification of different possible structures that may be\npresent in the data. For example, one may be\ninterested in determining whether the observations \nare \"grouped\" in some way (clustering), or if the data can be\nefficiently represented using fewer variables or features \n(dimension reduction). \n\nMany of these methods do not rely on any probabilistic\nmodel, and thus there may not be a clear *target* to \nbe estimated or approximated. As a consequence,\nthe conclusions that can be reached from this type\nof analyses are often of an exploratory nature. \n\n## Principal Components Analysis\n\nAlthough principal components can be easily computed with the spectral\ndecomposition of the covariance matrix of the data (using the function \n`svd` in `R`, for example), there are a few dedicated implementations\nin `R`, among them `prcomp` and `princomp`). The main difference between\nthese two is which internal function is used to compute eigenvalues and\neigenvectors: `prcomp` uses `svd` and `princomp` uses the less preferred\nfunction `eigen`. Both `princomp` and `prcomp` return the matrix of \nloadings (eigenvectors), the scores (projections of the data on the \nbasis of eigenvectors), and other auxiliary objects. They also include\nplot and summary methods.\n\nInstead of reviewing those (which can be easily done individually), in\nthese notes I will reproduce two of the examples used in class (the simple\n2-dimensional one used to motivate the topic, and the more interesting\n256-dimensional one using the digits data). \n\nFinally, I will also \nshow that principal components can be computed using an iterative\nalgorithm (alternate regression), which may be faster than factorizing\nthe covariance matrix, particularly when one is only interested in a\nfew principal components and the dimension of the data is large (but \nalso look at the arguments `nu` and `nv` for the function `svd` in `R`).\n\n\n## Simple 2-dimensional example \n\nWe first read the data for the simple illustration of PC's as best lower \ndimensional approximations.\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/pca1_775957309f20f38c71657cfcb6361802'}\n\n```{.r .cell-code}\nx <- read.table(\"data/t8-5.dat\", header = FALSE)\n```\n:::\n\nNote that the data has 5 explanatory variables. Here \nwe only use two of them in order to be able to visualize the analysis\nmore easily:\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/pca2_97ca63efe96ae7458082a4f0d643b4d7'}\n\n```{.r .cell-code}\nxx <- x[, c(2, 5)]\ncolnames(xx) <- c(\"Prof degree\", \"Median home value\")\n```\n:::\n\nAs discussed in class, we standardize the data to avoid a large difference\nin scales \"hijacking\" the principal components:\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/pca3_01e420fcfd1f7acb9ad7f1ffcac7303a'}\n\n```{.r .cell-code}\nxx <- scale(xx, center = colMeans(xx), scale = TRUE)\n```\n:::\n\nWe now define two auxiliary functions to compute Euclidean\nnorms and squared Euclidean norms (less general by probably\nfaster than `R`'s `base::norm`):\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/norms_db58d7a082dcdcfcc316e4969fac4b8d'}\n\n```{.r .cell-code}\nnorm2 <- function(a) sum(a^2)\nnorm <- function(a) sqrt(norm2(a))\n```\n:::\n\n\nWe start by looking at the data with a scatter plot:\n\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/data_7df43913983a9bbf7d7a4ad5594b66ee'}\n\n```{.r .cell-code}\nplot(xx, pch = 19, col = \"black\", cex = 2, xlim = c(-1.5, 4.5), ylim = c(-1.5, 4.5))\nabline(h = 0, lwd = 2, col = \"grey\", lty = 2)\nabline(v = 0, lwd = 2, col = \"grey\", lty = 2)\n```\n\n::: {.cell-output-display}\n![](51-pca_files/figure-html/data-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe now compute projections along the direction of the vector\n$\\mathbf{v} \\propto (1, 0.05)^\\top$. Recall from your linear algebra courses that\nthe orthogonal projection of a point $\\mathbf{x}$ on the linear subspace spanned by $\\mathbf{v}$\n(where $\\| \\mathbf{v} \\| = 1$)\nis given by $\\pi_{\\mathbf{v}} ( \\mathbf{x} ) = \\langle \\mathbf{x}, \\mathbf{v} \\rangle \\, \\mathbf{v}$\nwhich can also be written as \n$\\pi_{\\mathbf{v}} ( \\mathbf{x} ) = ( \\mathbf{v} \\, \\mathbf{v}^\\top) \\mathbf{x}$. \nWe first find the coordinates of the orthogonal projects of each observation \nalong the subspace generated by $\\mathbf{v} = (1, 0.05)^\\top$ (these are\nthe scalars\n$\\langle \\mathbf{x}_i, \\mathbf{v} \\rangle =  \\mathbf{x}_i^\\top \\mathbf{v}$ for each \npoint $\\mathbf{x}_i$:\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/proj1_d27a1ed76e7e8f86c3caf37fcd8e7130'}\n\n```{.r .cell-code}\na <- c(1, 0.05)\na <- a / norm(a)\n# Find the projections (coordinates of the\n# observations on this basis of size 1)\nprs <- (xx %*% a)\n```\n:::\n\nWe now compute the projections \n$\\pi_{\\mathbf{v}} ( \\mathbf{x}_i ) = \\langle \\mathbf{x}_i, \\mathbf{v} \\rangle \\, \\mathbf{v}$:\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/proj1.1_44002063b3c1d195d093621b7351837a'}\n\n```{.r .cell-code}\npr <- prs %*% a\n```\n:::\n\nand add them to the plot, with a few observations highlighted. The subspace\nis shown in red, and the orthogonal projections as solid red dots on that line:\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/proj1.2_ea433538ac86ff6ee2948c05b0556823'}\n\n```{.r .cell-code}\n# Plot the data\nplot(xx, pch = 19, col = \"black\", cex = 2, xlim = c(-1.5, 4.5), ylim = c(-1.5, 4.5))\nabline(h = 0, lwd = 2, col = \"grey\", lty = 2)\nabline(v = 0, lwd = 2, col = \"grey\", lty = 2)\n# Show the subspace on which we are projecting\nabline(0, a[2] / a[1], lwd = 2, col = \"red\")\n# Add the projections of the data on this subspace\npoints(pr[, 1], pr[, 2], pch = 19, cex = 1.5, col = \"red\")\n# Highlight a few of them\nind <- c(26, 25, 48, 36)\npr2 <- pr[ind, ]\nfor (j in 1:length(ind)) {\n  lines(c(xx[ind[j], 1], pr2[j, 1]), c(xx[ind[j], 2], pr2[j, 2]),\n    col = \"blue\", lwd = 3.5, lty = 2\n  )\n}\n```\n\n::: {.cell-output-display}\n![](51-pca_files/figure-html/proj1.2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe repeat the above but projecting on a different direction\n$\\mathbf{v} \\propto (-1, 3)^\\top$: \n\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/proj2_feb9c2e4e912f7355fc90f56561bda27'}\n\n```{.r .cell-code}\na <- c(-1, 3)\na <- a / norm(a)\n# Find the projections (coordinates of the\n# observations on this basis of size 1)\nprs <- (xx %*% a)\n# Find the orthogonal projections of each\n# observation on this subspace of dimension 1\npr <- prs %*% a\n# Plot the data\nplot(xx, pch = 19, col = \"black\", cex = 2, xlim = c(-1.5, 4.5), ylim = c(-1.5, 4.5))\nabline(h = 0, lwd = 2, col = \"grey\", lty = 2)\nabline(v = 0, lwd = 2, col = \"grey\", lty = 2)\n\n# Show the subspace on which we are projecting\nabline(0, a[2] / a[1], lwd = 2, col = \"red\")\n\n# Add the projections of the data on this subspace\npoints(pr[, 1], pr[, 2], pch = 19, cex = 1.5, col = \"red\")\n\n# Highlight a few of them\nind <- c(26, 25, 48, 36)\npr2 <- pr[ind, ]\nfor (j in 1:length(ind)) {\n  lines(c(xx[ind[j], 1], pr2[j, 1]), c(xx[ind[j], 2], pr2[j, 2]),\n    col = \"blue\", lwd = 3.5, lty = 2\n  )\n}\n```\n\n::: {.cell-output-display}\n![](51-pca_files/figure-html/proj2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe saw in class that the direction $\\mathbf{v}$ that results \nin orthogonal projections closest to the original data (in the sense of \nminimizing the mean (or sum) of the residuals Euclidean norm squared)\nis given by the \"first\" eigenvector of the covariance\nmatrix of the data. This is the first principal component. \nRefer to the class slides and discussion for more details\nand the definition and properties of the other principal components. \n\n## Digits example\n\nIn this example we use principal components to explore the zip code \ndata. In particular, we focus on images from a single digit (we\nuse 3, but the reader is strongly encouraged to re-do this analysis\nfor other digits to explore whether similar conclusions hold for \nthem). We load the training data from the `ElemStatLearn` package in R,\nand extract the images that correspond to the digit 3. For more\ninformation use `help(zip.train, package='ElemStatLearn')`.\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/digits1_9c0e579677605a287c57f2d3e05583be'}\n\n```{.r .cell-code}\ndata(zip.train, package = \"ElemStatLearn\")\na <- zip.train[zip.train[, 1] == 3, -1]\n```\n:::\n\nDefine an auxiliary function to compute \nthe squared Euclidean distance between two vectors\n(recall that we have already defined the function `norm2` above):\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/aux2_315959dcd280493fb026d6de57bc9258'}\n\n```{.r .cell-code}\ndist <- function(a, b) norm2(a - b)\n```\n:::\n\n\nTo display the images we adapt the following function\nfor plotting a matrix, which was \noriginally available at [http://www.phaget4.org/R/image_matrix.html](http://www.phaget4.org/R/image_matrix.html):\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/plotfunction_f89ce285807ed7a8ac66287a9925a7b1'}\n\n```{.r .cell-code}\nmyImagePlot <- function(x) {\n  min <- min(x)\n  max <- max(x)\n  ColorRamp <- grey(seq(1, 0, length = 256))\n  ColorLevels <- seq(min, max, length = length(ColorRamp))\n  # Reverse Y axis\n  reverse <- nrow(x):1\n  x <- x[reverse, ]\n  image(1:ncol(x), 1:nrow(x), t(x),\n    col = ColorRamp, xlab = \"\",\n    ylab = \"\", axes = FALSE, zlim = c(min, max)\n  )\n}\n```\n:::\n\nUsing this function, we plot 9 randomly chosen images from our data set:\n\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/digitsplot_e3bea4619b2db6f61798e0aa23d985f6'}\n\n```{.r .cell-code}\nset.seed(31)\nsa <- sample(nrow(a), 9)\npar(mai = c(1, 1, 1, 1) / 5, xaxs = \"i\", yaxs = \"i\")\npar(mfrow = c(3, 3))\nfor (j in 1:9) myImagePlot(t(matrix(unlist(a[sa[j], ]), 16, 16)))\n```\n\n::: {.cell-output-display}\n![](51-pca_files/figure-html/digitsplot-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n<!-- # plot the average 3 -->\n<!-- myImagePlot(t(matrix(colMeans(a), 16, 16))) -->\n\n\n<!-- # Plot the first 3 on the data set -->\n<!-- myImagePlot(t(matrix(unlist(a[1,]), 16, 16))) -->\n\nNext, we centre the observations in order to compute the eigenvectors \nand eigenvalues of the covariance matrix more efficiently. In fact,\nnote that we do not need to even compute the covariance matrix\nand can use the SVD of the centred data.\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/digits3_3ccc3de24f04ab36ba06e8ce821af0a5'}\n\n```{.r .cell-code}\nac <- scale(a, center = TRUE, scale = FALSE)\nsi.svd <- svd(ac)\n```\n:::\n\n\nUsing the relationship between the eigenvectors of the covariance matrix \nand the SVD of the $n \\times p$ data matrix, we compute the coordinates of the \ncentered data on their orthogonal projections along each of the first and 2nd and 3rd \nprincipal directions (eigenvectors of the covariance matrix). Recall that the data \nare stored as rows of the matrix `a`:\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/digits.p1_958f129a51d63b1e445cbc727920b978'}\n\n```{.r .cell-code}\nv1 <- as.vector(ac %*% si.svd$v[, 1])\nv2 <- as.vector(ac %*% si.svd$v[, 2])\n```\n:::\n\nAs discussed in class, we identify 5 quantiles of each of these \ncoordinates to use as our 2-dimensional grid:\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/digits.p2_52c4cfc40dfa6e22bfa53540990fded9'}\n\n```{.r .cell-code}\nqv1 <- quantile(v1, c(.05, .25, .5, .75, .95))\nqv2 <- quantile(v2, c(.05, .25, .5, .75, .95))\n```\n:::\n\nWe can visualize the grid of these 5 x 5 = 25 points \nover the scatter plot of all the 2-dimensional projections \nof the data (their coordinates on the principal components\nbasis):\n\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/digits.p3_07a6a9eeed21d92080c80c1519bebfab'}\n\n```{.r .cell-code}\nqv <- expand.grid(qv1, qv2)\nplot(v1, v2, pch = 19, cex = 1, col = \"grey\")\npoints(qv[, 1], qv[, 2], pch = 19, cex = 1.5, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](51-pca_files/figure-html/digits.p3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe now find the points in our data set (images) \nwith projections closest to each of the 5 x 5 = 25 points in the grid\n(note that these distances between points in the principal-subspace,\nwhich is in the 256 dimensional space) can be computed \nin terms of their coordinates on the principal-basis only (which \nare 2-dimensional points):\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/digits.p4_f091ff5b713cc5a242af82b7f8572a1d'}\n\n```{.r .cell-code}\nvs <- cbind(v1, v2)\ncvs <- array(0, dim = dim(qv))\nfor (j in 1:dim(qv)[1]) cvs[j, ] <- vs[which.min(apply(vs, 1, dist, b = qv[j, ])), ]\n```\n:::\n\nWe now add these points to our plot (we use color blue for them):\n\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/digits.p5_3813d5c11a98feef057fb32ff4321518'}\n\n```{.r .cell-code}\nplot(v1, v2, pch = 19, cex = 1, col = \"grey\")\npoints(qv[, 1], qv[, 2], pch = 19, cex = 1.5, col = \"red\")\nfor (j in 1:dim(qv)[1]) points(cvs[j, 1], cvs[j, 2], pch = 19, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](51-pca_files/figure-html/digits.p5-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nUsing these \"blue\" coordinates, we construct the corresponding points \nin the 256-dimensional space:\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/digits.p6_ee05d9f7272c441201366a1c8499bd33'}\n\n```{.r .cell-code}\napp <- t(si.svd$v[, 1:2] %*% t(cvs))\n```\n:::\n\nand identify the images in our data set that are closest to these points\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/digits.p7_d53c9c9f357baade19f9fa797af282cd'}\n\n```{.r .cell-code}\nrepre <- matrix(0, dim(qv)[1], dim(app)[2])\nfor (j in 1:dim(qv)[1]) repre[j, ] <- ac[which.min(apply(ac, 1, dist, b = app[j, ])), ]\n```\n:::\n\nThese are the actual images that are closest to the points in the array `app` \nabove. Now add the column means and display these 25 images according to the points\nthey represent in the red grid:\n\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/digits.p8_00f72fe6536968da756c6b33fb6fbc28'}\n\n```{.r .cell-code}\nrepre <- scale(repre, center = -colMeans(a), scale = FALSE)\npar(mai = c(1, 1, 1, 1) / 5, xaxs = \"i\", yaxs = \"i\")\npar(mfrow = c(5, 5))\nfor (j in 1:dim(repre)[1]) {\n  myImagePlot(t(matrix(unlist(repre[j, ]), 16, 16)))\n}\n```\n\n::: {.cell-output-display}\n![](51-pca_files/figure-html/digits.p8-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote how these images change when we \"traverse\" the 256-dimensional space\nalong each of these 2 principal directions. \n\n\n## Alternating regression to compute principal components\n\nFor details see Appendix \\@ref(alt-pca).\n\nA function implementing this method to compute the first\nprincipal component is:\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/alt1_d515925db6b840c0d92155129153cb75'}\n\n```{.r .cell-code}\nalter.pca.k1 <- function(x, max.it = 500, eps = 1e-10) {\n  n2 <- function(a) sum(a^2)\n  p <- dim(x)[2]\n  x <- scale(x, scale = FALSE)\n  it <- 0\n  old.a <- c(1, rep(0, p - 1))\n  err <- 10 * eps\n  while (((it <- it + 1) < max.it) & (abs(err) > eps)) {\n    b <- as.vector(x %*% old.a) / n2(old.a)\n    a <- as.vector(t(x) %*% b) / n2(b)\n    a <- a / sqrt(n2(a))\n    err <- sqrt(n2(a - old.a))\n    old.a <- a\n  }\n  conv <- (it < max.it)\n  return(list(a = a, b = b, conv = conv))\n}\n```\n:::\n\nWe use it on the digits data above to compute the \nfirst principal component (we also time it):\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/alt2_b4152b9b314892c2097c60eeb66dd6b2'}\n\n```{.r .cell-code}\nsystem.time(tmp <- alter.pca.k1(ac)$a)\n#>    user  system elapsed \n#>   0.058   0.007   0.065\n```\n:::\n\nand compare it with the one given by `svd`,\n which we also time. Note that the\nsign of the eigenvectors is arbitrary, so we adjust \nthese vectors in order to have first elements with the \nsame sign. \n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/alt3_1bfc98a5e96a7eb92223042657dd1dd1'}\n\n```{.r .cell-code}\nsystem.time(tmp2 <- svd(ac)$v[, 1])\n#>    user  system elapsed \n#>   0.183   0.002   0.183\ntmp <- tmp * sign(tmp2[1] * tmp[1])\nsummary(abs(tmp - tmp2))\n#>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#> 4.200e-16 1.195e-12 4.012e-12 7.272e-12 1.169e-11 3.524e-11\n```\n:::\n\nNote that both eigenvectors are essentially identical, and that the \nalternating regression method is typically faster than\na full SVD decomposition of the covariance matrix.\n\nThis difference in speed is more striking for problems in higer dimensions.\n\nTo further illustrate the potential gain in speed for larger dimensions, \nconsider the following synthetic data set with n = 2000 observation\nand p = 1000, and compare the timing and the results\n(even when forcing `svd` to only compute a single component).\n\nFirst generate the data set\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/alt4_6d8553051306cf73c30c1b414af387ef'}\n\n```{.r .cell-code}\nn <- 2000\np <- 1000\nx <- matrix(rt(n * p, df = 2), n, p)\n```\n:::\n\nCompute the first eigenvector using alternating regression, and\ntime it:\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/alt5_fbd88be63fe9a0e61a5ae1d73fe74f04'}\n\n```{.r .cell-code}\nsystem.time(tmp <- alter.pca.k1(x))\n#>    user  system elapsed \n#>   0.143   0.013   0.157\na1 <- tmp$a\n```\n:::\n\nCompute the first eigenvector using `svd`, and\ntime it:\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/alt6_fd20d87c996c9e537e9c624310e60a7f'}\n\n```{.r .cell-code}\nsystem.time(e1 <- svd(cov(x))$u[, 1])\n#>    user  system elapsed \n#>   5.011   0.038   5.053\n```\n:::\n\nAsking `svd` to only compute one component does not\nseem to make the algorithm faster (the results are\nidentical):\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/alt7_009e896c23b31e69f2346e81b07fdd26'}\n\n```{.r .cell-code}\nsystem.time(e1.1 <- svd(cov(x), nu = 1, nv = 1)$u[, 1])\n#>    user  system elapsed \n#>   4.974   0.024   4.999\nsummary(abs(e1 - e1.1))\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>       0       0       0       0       0       0\n```\n:::\n\nFinally, check that the first eigenvector computed with \n`svd` and with the alternating regression approach\nare practially identical:\n\n::: {.cell layout-align=\"center\" hash='51-pca_cache/html/alt8_4977914b0360d3a16b6f270bc48f17d2'}\n\n```{.r .cell-code}\na1 <- a1 * sign(e1[1] * a1[1])\nsummary(abs(e1 - a1))\n#>      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n#> 0.000e+00 9.376e-18 2.076e-17 5.745e-17 4.107e-17 2.232e-14\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}