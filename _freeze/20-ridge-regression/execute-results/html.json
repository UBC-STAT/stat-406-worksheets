{
  "hash": "9c3889afda8162b7713f53c2dd26e5d0",
  "result": {
    "markdown": "\n# Ridge regression \n\n\n\n\n\n\nVariable selection methods like stepwise can be highly variable. To illustrate this\nissue consider the following simple experiment. As before, \nwe apply stepwise on 5 randomly selected folds of the data, and look at the\nmodels selected in each of them.\n\n::: {.cell layout-align=\"center\" hash='20-ridge-regression_cache/html/stepvariable_868f6b254b06f2250b6a79cdef0794d9'}\n\n```{.r .cell-code}\nairp <- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\nlibrary(MASS)\nk <- 5\nn <- nrow(airp)\nset.seed(123456)\nii <- sample((1:n) %% k + 1)\nfor (j in 1:k) {\n  x0 <- airp[ii != j, ]\n  null0 <- lm(MORT ~ 1, data = x0)\n  full0 <- lm(MORT ~ ., data = x0) # needed for stepwise\n  step.lm0 <- stepAIC(null0,\n    scope = list(lower = null0, upper = full0),\n    trace = FALSE\n  )\n  print(formula(step.lm0)[[3]])\n}\n#> NONW + EDUC + JANT + OVR65 + SO.\n#> NONW + EDUC + PREC + SO. + JULT\n#> NONW + EDUC + JANT + SO. + PREC\n#> NONW + SO. + JANT + PREC + DENS\n#> NONW + JANT + EDUC + DENS + POPN + JULT + PREC + OVR65\n```\n:::\n\nAlthough many variables appear in more than one model, only `NONW` and `SO.` \nare in all of them, and `JANT` and `PREC` in 4 out of the 5. \nThere are also several that appear in only one model (`HOUS`, `WWDRK` and `POPN`). \nThis variability may in turn impact (negatively) the accuracy of the\nresulting predictions. \n\nA different approach to dealing with potentially correlated explanatory\nvariables (with the goal of obtaining less variable / more accurate \npredictions) is to \"regularize\" the parameter estimates. In other words\nwe modify the optimization problem that defines the parameter \nestimators (in the case of linear regression fits we tweak \nthe least squares problem) to limit their size (in fact restricting \nthem to be in a bounded and possibly small subset of the parameter\nspace). \n\nThe first proposal for a regularized / penalized estimator for \nlinear regression models is Ridge Regression. \nWe will use the function `glmnet` in package `glmnet` to\ncompute the Ridge Regression estimator. Note that this \nfunction implements a larger family of regularized estimators,\nand in order to obtain a Ridge Regression estimator\nwe need to set the argument `alpha = 0` of `glmnet()`. \n<!-- We use Ridge Regression with the air pollution data to obtain a -->\n<!-- more stable predictor. -->\nWe also specify a range of possible values of the penalty \ncoefficient (below we use a grid of 50 values between \nexp(-3) and exp(10)). \n\n::: {.cell layout-align=\"center\" hash='20-ridge-regression_cache/html/ridge.air1_3a1ac8da3ce1082bb710e0ed626ceda7'}\n\n```{.r .cell-code}\nlibrary(glmnet)\n# alpha = 0 - Ridge\n# alpha = 1 - LASSO\ny <- as.vector(airp$MORT)\nxm <- as.matrix(airp[, -16])\nlambdas <- exp(seq(-3, 10, length = 50))\na <- glmnet(\n  x = xm, y = y, lambda = rev(lambdas),\n  family = \"gaussian\", alpha = 0\n)\n```\n:::\n\nThe returned object contains the estimated regression coefficients for\neach possible value of the regularization parameter. We can look at\nthem using the `plot` method for objects of class `glmnet` as \nfollows:\n\n\n::: {.cell layout-align=\"center\" hash='20-ridge-regression_cache/html/ridge.plot_e2988c05021ddd67b90133c2377662fd'}\n\n```{.r .cell-code}\nplot(a, xvar = \"lambda\", label = TRUE, lwd = 6, cex.axis = 1.5, cex.lab = 1.2, ylim = c(-20, 20))\n```\n\n::: {.cell-output-display}\n![](20-ridge-regression_files/figure-html/ridge.plot-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## Selecting the level of regularization\n\nDifferent values of the penalization parameter will typically yield estimators with\nvarying predictive accuracies. To select a good level of regularization we estimate\nthe MSPE of the estimator resulting from each value of the penalization parameter.\nOne way to do this is to run K-fold cross validation for each value of\nthe penalty. The `glmnet` package provides a built-in function to do this, \nand a `plot` method to display the results:\n\n\n::: {.cell layout-align=\"center\" hash='20-ridge-regression_cache/html/ridge.cv_f912f3e8ddc7f68918c233ca18eb8460'}\n\n```{.r .cell-code}\n# run 5-fold CV\nset.seed(123)\ntmp <- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\nplot(tmp, lwd = 6, cex.axis = 1.5, cex.lab = 1.2)\n```\n\n::: {.cell-output-display}\n![](20-ridge-regression_files/figure-html/ridge.cv-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nIn the above plot the red dots are the estimated MSPE's for each value of the\npenalty, and the vertical lines mark plus/minus one (estimated) standard deviations (for each\nof those estimated MSPE's). The `plot` method will also mark the optimal value of\nthe regularization parameter, and also the largest one for which the estimated MSPE\nis within 1-SD of the optimal. The latter is meant to provide a more regularized\nestimator with estimated MSPE within the error-margin of our estimated minimum.\n\nNote, however, that the above \"analysis\" is random (because of the intrinsic randomness of\nK-fold CV). If we run it again, we will most likely get different results. In many cases, \nhowever, the results will be qualitatively similar. If we run 5-fold CV again for this\ndata get the following plot:\n\n\n::: {.cell layout-align=\"center\" hash='20-ridge-regression_cache/html/ridge.cv2_29e6f817036ef9f3a24b0fa9c8c1b87b'}\n\n```{.r .cell-code}\nset.seed(23)\ntmp <- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\nplot(tmp, lwd = 6, cex.axis = 1.5, cex.lab = 1.2)\n```\n\n::: {.cell-output-display}\n![](20-ridge-regression_files/figure-html/ridge.cv2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote that both plots are similar, but not equal. It would be a good idea to repeat this\na few times and explore how much variability is involved. If one were interested\nin selecting one value of the penalization parameter that was more stable than \nthat obtained from a single 5-fold CV run, one could run it several times and\ntake the average of the estimated optimal values. For example:\n\n::: {.cell layout-align=\"center\" hash='20-ridge-regression_cache/html/ridge.opt_c99d36cf3ad1b116cffb6a98b900999f'}\n\n```{.r .cell-code}\nset.seed(123)\nop.la <- 0\nfor (j in 1:20) {\n  tmp <- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\n  op.la <- op.la + tmp$lambda.min # tmp$lambda.1se\n}\n(op.la <- op.la / 20)\n#> [1] 11.44547\nlog(op.la)\n#> [1] 2.437594\n```\n:::\n\nThis value is reasonably close to the ones we saw in the plots above. \n\n## Comparing predictions \n\nWe now run a cross-validation experiment to compare the \nMSPE of 3 models: the **full** model, the one\nselected by **stepwise** and the **ridge regression**\none. \n\n\n::: {.cell layout-align=\"center\" hash='20-ridge-regression_cache/html/ridge.mspe_f94e492301c8ace53f9b7d3d3f7cfa50'}\n\n```{.r .cell-code}\nn <- nrow(xm)\nk <- 5\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 100\nmspe.st <- mspe.ri <- mspe.f <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.f <- pr.ri <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    tmp.ri <- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 0, family = \"gaussian\"\n    )\n    null <- lm(MORT ~ 1, data = airp[ii != j, ])\n    full <- lm(MORT ~ ., data = airp[ii != j, ])\n    tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n    pr.ri[ii == j] <- predict(tmp.ri, s = \"lambda.min\", newx = xm[ii == j, ])\n    pr.st[ii == j] <- predict(tmp.st, newdata = airp[ii == j, ])\n    pr.f[ii == j] <- predict(full, newdata = airp[ii == j, ])\n  }\n  mspe.ri[i] <- mean((airp$MORT - pr.ri)^2)\n  mspe.st[i] <- mean((airp$MORT - pr.st)^2)\n  mspe.f[i] <- mean((airp$MORT - pr.f)^2)\n}\nboxplot(mspe.ri, mspe.st, mspe.f,\n  names = c(\"Ridge\", \"Stepwise\", \"Full\"),\n  col = c(\"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1.5, cex.lab = 1.5,\n  cex.main = 2, ylim = c(1300, 3000)\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n```\n\n::: {.cell-output-display}\n![](20-ridge-regression_files/figure-html/ridge.mspe-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## A more stable Ridge Regression?\n\nHere we try to obtain a ridge regression estimator\nwith more stable predictions by using the \naverage optimal penalty value using 20 runs. \nThe improvement does not appear to be \nsubstantial.\n\n\n::: {.cell layout-align=\"center\" hash='20-ridge-regression_cache/html/stableridge.mspe_208660019cdb918349b6262d9e3fa869'}\n\n```{.r .cell-code}\nn <- nrow(xm)\nk <- 5\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 100\nmspe.ri2 <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.ri2 <- rep(0, n)\n  for (j in 1:k) {\n    op.la <- 0\n    for (h in 1:20) {\n      tmp <- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\n      op.la <- op.la + tmp$lambda.min # tmp$lambda.1se\n    }\n    op.la <- op.la / 20\n    tmp.ri <- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas, nfolds = 5,\n      alpha = 0, family = \"gaussian\"\n    )\n    pr.ri2[ii == j] <- predict(tmp.ri, s = op.la, newx = xm[ii == j, ])\n  }\n  mspe.ri2[i] <- mean((airp$MORT - pr.ri2)^2)\n}\nboxplot(mspe.ri2, mspe.ri, mspe.st, mspe.f,\n  names = c(\"Stable R\", \"Ridge\", \"Stepwise\", \"Full\"),\n  col = c(\"steelblue\", \"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1.5, cex.lab = 1.5,\n  cex.main = 2, ylim = c(1300, 3000)\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n```\n\n::: {.cell-output-display}\n![](20-ridge-regression_files/figure-html/stableridge.mspe-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## An example where one may not need to select variables\n\nIn some cases one may not need to select a subset of explanatory\nvariables, and in fact, doing so may affect negatively the accuracy of\nthe resulting predictions. In what follows we discuss such an example. \nConsider the credit card data set that contains information\non credit card users. The interest is in predicting the \nbalance carried by a client. We first load the data, and to\nsimplify the presentation here we consider only the numerical\nexplanatory variables:\n\n::: {.cell layout-align=\"center\" hash='20-ridge-regression_cache/html/credit1_e27919c4a273a0d08a5496625ad6f525'}\n\n```{.r .cell-code}\nx <- read.table(\"data/Credit.csv\", sep = \",\", header = TRUE, row.names = 1)\nx <- x[, c(1:6, 11)]\n```\n:::\n\nThere are 6 available covariates, and a stepwise search selects \na model with 5 of them (discarding `Education`):\n\n::: {.cell layout-align=\"center\" hash='20-ridge-regression_cache/html/credit2_2e30e8a32fea4ea6e89919b35b33c4f6'}\n\n```{.r .cell-code}\nlibrary(MASS)\nnull <- lm(Balance ~ 1, data = x)\nfull <- lm(Balance ~ ., data = x)\n(tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0))\n#> \n#> Call:\n#> lm(formula = Balance ~ Rating + Income + Limit + Age + Cards, \n#>     data = x)\n#> \n#> Coefficients:\n#> (Intercept)       Rating       Income        Limit          Age        Cards  \n#>   -449.3610       2.0224      -7.5621       0.1286      -0.8883      11.5527\n```\n:::\n\nIt is an easy exercise to check that the MSPE of this\nsmaller model is in fact worse than the one for the **full** one:\n\n\n::: {.cell layout-align=\"center\" hash='20-ridge-regression_cache/html/credit3_92d3d624447b4a1980930eba01871fac'}\n\n```{.r .cell-code}\nn <- nrow(x)\nk <- 5\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 100\nmspe.st <- mspe.f <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.f <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    null <- lm(Balance ~ 1, data = x[ii != j, ])\n    full <- lm(Balance ~ ., data = x[ii != j, ])\n    tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n    pr.st[ii == j] <- predict(tmp.st, newdata = x[ii == j, ])\n    pr.f[ii == j] <- predict(full, newdata = x[ii == j, ])\n  }\n  mspe.st[i] <- mean((x$Balance - pr.st)^2)\n  mspe.f[i] <- mean((x$Balance - pr.f)^2)\n}\nboxplot(mspe.st, mspe.f,\n  names = c(\"Stepwise\", \"Full\"),\n  col = c(\"tomato\", \"springgreen\"), cex.axis = 1.5,\n  cex.lab = 1.5, cex.main = 2\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n```\n\n::: {.cell-output-display}\n![](20-ridge-regression_files/figure-html/credit3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nUsing ridge regression instead of stepwise to prevent \nthe negative effect of possible correlations among the\ncovariates yields a slight improvement (over the **full** model), \nbut it is not clear the gain is worth the effort. \n\n\n::: {.cell layout-align=\"center\" hash='20-ridge-regression_cache/html/credit4_244a6422915e83cb95b33ff6d817e46c'}\n\n```{.r .cell-code}\ny <- as.vector(x$Balance)\nxm <- as.matrix(x[, -7])\nlambdas <- exp(seq(-3, 10, length = 50))\nn <- nrow(xm)\nk <- 5\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 100\nmspe.st <- mspe.ri <- mspe.f <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.f <- pr.ri <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    tmp.ri <- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 0, family = \"gaussian\"\n    )\n    null <- lm(Balance ~ 1, data = x[ii != j, ])\n    full <- lm(Balance ~ ., data = x[ii != j, ])\n    tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n    pr.ri[ii == j] <- predict(tmp.ri, s = \"lambda.min\", newx = xm[ii == j, ])\n    pr.st[ii == j] <- predict(tmp.st, newdata = x[ii == j, ])\n    pr.f[ii == j] <- predict(full, newdata = x[ii == j, ])\n  }\n  mspe.ri[i] <- mean((x$Balance - pr.ri)^2)\n  mspe.st[i] <- mean((x$Balance - pr.st)^2)\n  mspe.f[i] <- mean((x$Balance - pr.f)^2)\n}\nboxplot(mspe.ri, mspe.st, mspe.f,\n  names = c(\n    \"Ridge\", \"Stepwise\",\n    \"Full\"\n  ), col = c(\"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1.5,\n  cex.lab = 1.5, cex.main = 2\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n```\n\n::: {.cell-output-display}\n![](20-ridge-regression_files/figure-html/credit4-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n\n## An important limitation of Ridge Regression\n\nRidge Regression typically yields estimators with more accurate (less variable) \npredictions, specially when there is noticeable correlation among covariates. \nHowever, it is important to note that Ridge Regression does not select \nvariables, and in that sense it does not \"replace\" methods like stepwise when\nthe interest is in using a smaller number of explanatory variables. Furthermore,\nthe interpretation of the Ridge Regression coefficient estimates is \ngenerally difficult. LASSO regression estimates were proposed to \naddress these two issues (more stable predictions when correlated \ncovariates are present **and** variable selection) simultaneously. \n\n## Effective degrees of freedom \n\nIntuitively, if we interpret \"degrees of freedom\" as the number of\n\"free\" parameters that are available to us for tuning when\nwe fit / train\na model or predictor, then we would expect a Ridge Regression estimator \nto have less \n\"degrees of freedom\" than a regular least squares regression \nestimator, given that it is the solution of a constrained\noptimization problem. This is, of course, an informal \nargument, particularly since there is no proper definition\nof \"degrees of freedom\". \n\nThe more general definition discussed in class, called \"effective\ndegrees of freedom\" (EDF), reduces to the trace of the \"hat\" matrix for\nany linear predictor (including, but not limited to, linear\nregression models), and is due to Efron [@Efron1986].\nYou may also want to look at some of the more recent papers that\ncite the one above. \n\nIt is easy (but worth your time doing it) to see that for a Ridge \nRegression estimator computed with a penalty / regularization \nparameter equal to **b**, the corresponding EDF are the sum of the \nratio of each eigenvalue of **X'X** with respect to itself plus **b**\n(see the formula on the lecture slides). We compute the EDF \nof the Ridge Regression fit to the air pollution data when the \npenalty parameter is considered to be fixed at the average optimal value \nover 20 runs of 5-fold CV:\n\n\n::: {.cell layout-align=\"center\" hash='20-ridge-regression_cache/html/ridge2_a89bb78d6064247f9a421b4b6564c6aa'}\n\n```{.r .cell-code}\nairp <- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\ny <- as.vector(airp$MORT)\nxm <- as.matrix(airp[, -16])\nlibrary(glmnet)\nlambdas <- exp(seq(-3, 10, length = 50))\nset.seed(123)\nop.la <- 0\nfor (j in 1:20) {\n  tmp <- cv.glmnet(x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0, family = \"gaussian\")\n  op.la <- op.la + tmp$lambda.min # tmp$lambda.1se\n}\nop.la <- op.la / 20\nxm <- scale(as.matrix(airp[, -16]), scale = FALSE)\nxm.svd <- svd(xm)\n(est.edf <- sum(xm.svd$d^2 / (xm.svd$d^2 + op.la)))\n#> [1] 12.99595\n```\n:::\n\n\n## Important caveat!\n\nNote that in the above discussion of EDF we have assumed that\nthe matrix defining the linear predictor does not depend on the \nvalues of the response variable (that it only depends on the matrix **X**), \nas it is the case in linear regression. This is fine for \nRidge Regression estimators **as long as the penalty parameter\nwas not chosen using the data**. This is typically not the case \nin practice. Although the general definition of EDF still holds, \nit is not longer true that Ridge Regression yields a linear \npredictor, and thus the corresponding EDF may not be\nequal to the trace of the corresponding\nmatrix. \n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}