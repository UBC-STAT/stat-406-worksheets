{
  "hash": "2b5960237ae61d7719f482ac3ff681ff",
  "result": {
    "markdown": "# Pruning regression trees with `rpart`\n\n\n\n\n\n\n\n_**Important note**: As discussed in class, the K-fold CV methodology\nimplemented in the package `rpart` seems to consider \na sequence of trees (or, equivalently, of complexity parameters)\nbased on the full training set. For more details\nrefer to the corresponding documentation: pages 12 and ff of the\npackage vignette, which can be accessed from `R` using the\ncommand `vignette('longintro', package='rpart')`. \nFor an alternative implementation of CV-based pruning, \nplease see also the Section **\"Pruning regression trees with `tree`\"** below._ \n\nThe stopping criteria generally used when fitting regression trees do not\ntake into account explicitly the complexity of the tree. Hence, we \nmay end up with either an overfitting tree, or a very simple one, \nwhich typically results in a decline in the quality of the corresponding predictions. \nAs discussed in class, one solution is to purposedly grow / train a very large overfitting\ntree, and then prune it. One can also estimate the corresponding MSPE\nof each tree in the prunning sequence and choose an optimal one. \nThe function `rpart` implements this approach, and we illustrate it\nbelow. \n\nWe force `rpart` to build a very large tree via the arguments\nof the function `rpart.control`. At the same time, to obtain a good\npicture of the evolution of MSPE for different subtrees, we set the smallest \ncomplexity parameter to be considered by the cross-validation\nexperiment to a very low value (here we use `1e-8`).\n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/prune_3202c32449b8363b519234096d8c3a9d'}\n\n```{.r .cell-code}\nlibrary(rpart)\ndata(Boston, package = \"MASS\")\n# split data into a training and\n# a test set\nset.seed(123456)\nn <- nrow(Boston)\nii <- sample(n, floor(n / 4))\ndat.te <- Boston[ii, ]\ndat.tr <- Boston[-ii, ]\n\nmyc <- rpart.control(minsplit = 2, cp = 1e-5, xval = 10)\nset.seed(123456)\nbos.to <- rpart(medv ~ .,\n  data = dat.tr, method = \"anova\",\n  control = myc\n)\nplot(bos.to, compress = TRUE) # type='proportional')\n```\n\n::: {.cell-output-display}\n![](25-more-trees_files/figure-html/prune-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNot surprisingly, the predictions of this large tree are \nnot very good:\n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/prune2_e5feb6f0420e1c0892dca9abd5b2a0c7'}\n\n```{.r .cell-code}\n# predictions are poor, unsurprisingly\npr.to <- predict(bos.to, newdata = dat.te, type = \"vector\")\nwith(dat.te, mean((medv - pr.to)^2))\n#> [1] 19.22826\n```\n:::\n\n\nTo prune we explore the *CP table* returned in the\n`rpart` object to find the value of the complexity\nparameter with optimal estimated prediction error. The estimated\nprediction error of each subtree (corresponding to each value of `CP`)\nis contained in the column `xerror`, and the associated \nstandard deviation is in column `xstd`. We would like to find\nthe value of `CP` that yields a corresponding pruned tree with smallest\nestimated prediction error. The function `printcp` shows the\nCP table corresponding to an `rpart` object:\n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/prune3_5647ba0003f3c0174c14563adfa3369a'}\n\n```{.r .cell-code}\nprintcp(bos.to)\n#> \n#> Regression tree:\n#> rpart(formula = medv ~ ., data = dat.tr, method = \"anova\", control = myc)\n#> \n#> Variables actually used in tree construction:\n#>  [1] age     black   chas    crim    dis     indus   lstat   nox     ptratio\n#> [10] rad     rm      tax     zn     \n#> \n#> Root node error: 32946/380 = 86.7\n#> \n#> n= 380 \n#> \n#>             CP nsplit  rel error  xerror     xstd\n#> 1   4.7150e-01      0 1.00000000 1.00363 0.094075\n#> 2   1.5701e-01      1 0.52850063 0.60388 0.063381\n#> 3   7.9798e-02      2 0.37149536 0.40412 0.050267\n#> 4   5.7540e-02      3 0.29169700 0.34109 0.048146\n#> 5   3.4802e-02      4 0.23415748 0.35907 0.054713\n#> 6   2.0424e-02      5 0.19935554 0.27486 0.041559\n#> 7   1.9408e-02      6 0.17893128 0.26826 0.041522\n#> 8   1.6414e-02      7 0.15952348 0.27163 0.041861\n#> 9   1.1118e-02      8 0.14310945 0.26680 0.041809\n#> 10  9.6449e-03      9 0.13199106 0.26576 0.048628\n#> 11  7.7292e-03     10 0.12234619 0.26166 0.047488\n#> 12  6.5545e-03     11 0.11461702 0.26243 0.047476\n#> 13  5.7344e-03     12 0.10806249 0.24154 0.044548\n#> 14  5.3955e-03     14 0.09659371 0.24499 0.043512\n#> 15  4.6018e-03     15 0.09119826 0.24284 0.043821\n#> 16  3.7390e-03     16 0.08659643 0.24439 0.044009\n#> 17  3.2170e-03     17 0.08285743 0.24188 0.044019\n#> 18  2.5445e-03     18 0.07964044 0.23957 0.043896\n#> 19  2.3205e-03     20 0.07455137 0.24288 0.043992\n#> 20  2.1485e-03     21 0.07223089 0.23905 0.043463\n#> 21  2.1316e-03     22 0.07008242 0.24546 0.044444\n#> 22  2.0477e-03     23 0.06795084 0.24556 0.044447\n#> 23  2.0283e-03     24 0.06590313 0.24493 0.044439\n#> 24  1.9878e-03     25 0.06387482 0.24448 0.044442\n#> 25  1.9781e-03     26 0.06188702 0.24495 0.044438\n#> 26  1.9686e-03     27 0.05990894 0.24495 0.044438\n#> 27  1.6400e-03     28 0.05794032 0.24296 0.044425\n#> 28  1.6357e-03     29 0.05630030 0.24257 0.044401\n#> 29  1.6212e-03     30 0.05466464 0.24257 0.044401\n#> 30  1.5386e-03     31 0.05304346 0.24272 0.044402\n#> 31  1.4205e-03     32 0.05150482 0.24609 0.044437\n#> 32  1.3390e-03     33 0.05008431 0.24671 0.044485\n#> 33  1.2731e-03     34 0.04874534 0.24823 0.044688\n#> 34  1.2294e-03     35 0.04747228 0.24985 0.044671\n#> 35  1.1693e-03     36 0.04624285 0.25214 0.044656\n#> 36  1.1587e-03     37 0.04507358 0.25609 0.044764\n#> 37  1.1306e-03     38 0.04391487 0.25484 0.044738\n#> 38  1.1235e-03     39 0.04278422 0.25484 0.044738\n#> 39  1.1117e-03     40 0.04166071 0.25420 0.044733\n#> 40  1.0183e-03     41 0.04054904 0.25240 0.044720\n#> 41  1.0016e-03     42 0.03953071 0.25440 0.044772\n#> 42  9.8001e-04     43 0.03852907 0.25480 0.044799\n#> 43  9.5959e-04     45 0.03656906 0.25607 0.044791\n#> 44  9.5612e-04     47 0.03464987 0.25828 0.044779\n#> 45  8.9091e-04     48 0.03369375 0.26231 0.045282\n#> 46  8.8600e-04     49 0.03280284 0.25879 0.044930\n#> 47  8.7103e-04     50 0.03191684 0.25836 0.044925\n#> 48  8.4075e-04     51 0.03104580 0.25901 0.044895\n#> 49  8.3105e-04     52 0.03020505 0.25848 0.044897\n#> 50  8.2287e-04     53 0.02937400 0.25882 0.044924\n#> 51  8.2159e-04     54 0.02855113 0.25893 0.044922\n#> 52  7.9802e-04     55 0.02772954 0.25984 0.044889\n#> 53  7.7379e-04     56 0.02693152 0.26006 0.044887\n#> 54  7.6674e-04     57 0.02615772 0.26006 0.044887\n#> 55  7.4051e-04     58 0.02539098 0.26070 0.044591\n#> 56  6.5174e-04     59 0.02465047 0.26100 0.044598\n#> 57  6.4506e-04     60 0.02399873 0.26196 0.044601\n#> 58  6.1748e-04     61 0.02335367 0.26243 0.044616\n#> 59  5.7918e-04     62 0.02273620 0.26455 0.044635\n#> 60  5.6590e-04     63 0.02215702 0.26531 0.044647\n#> 61  5.3958e-04     64 0.02159112 0.26456 0.044653\n#> 62  5.2778e-04     65 0.02105154 0.26771 0.045102\n#> 63  5.2595e-04     66 0.02052376 0.26878 0.045150\n#> 64  4.9608e-04     67 0.01999781 0.26887 0.045148\n#> 65  4.9581e-04     68 0.01950173 0.26876 0.045137\n#> 66  4.6477e-04     69 0.01900592 0.26899 0.045164\n#> 67  4.5562e-04     70 0.01854115 0.26883 0.045164\n#> 68  4.3208e-04     72 0.01762991 0.26818 0.045157\n#> 69  4.2934e-04     74 0.01676575 0.26768 0.045149\n#> 70  4.0512e-04     76 0.01590708 0.26831 0.045173\n#> 71  4.0437e-04     77 0.01550196 0.26865 0.045176\n#> 72  3.8959e-04     78 0.01509758 0.26928 0.045191\n#> 73  3.3745e-04     79 0.01470799 0.27223 0.045179\n#> 74  3.2839e-04     80 0.01437054 0.27232 0.045055\n#> 75  3.2113e-04     81 0.01404215 0.27316 0.045075\n#> 76  3.1358e-04     82 0.01372102 0.27216 0.044977\n#> 77  3.0960e-04     83 0.01340743 0.27273 0.045001\n#> 78  2.8639e-04     84 0.01309783 0.27342 0.045009\n#> 79  2.7607e-04     85 0.01281145 0.27447 0.045074\n#> 80  2.7189e-04     87 0.01225931 0.27379 0.045071\n#> 81  2.6958e-04     88 0.01198742 0.27385 0.045067\n#> 82  2.6552e-04     89 0.01171784 0.27361 0.045096\n#> 83  2.6115e-04     90 0.01145232 0.27350 0.045093\n#> 84  2.5749e-04     91 0.01119117 0.27350 0.045093\n#> 85  2.5578e-04     92 0.01093368 0.27280 0.045100\n#> 86  2.5257e-04     93 0.01067790 0.27277 0.045101\n#> 87  2.2556e-04     94 0.01042532 0.27385 0.045136\n#> 88  2.2386e-04     95 0.01019976 0.27266 0.045131\n#> 89  2.1854e-04     96 0.00997590 0.27266 0.045130\n#> 90  2.1012e-04     97 0.00975736 0.27363 0.045146\n#> 91  2.0946e-04     98 0.00954723 0.27444 0.045161\n#> 92  2.0776e-04     99 0.00933778 0.27444 0.045161\n#> 93  2.0488e-04    100 0.00913001 0.27230 0.044975\n#> 94  2.0296e-04    101 0.00892513 0.27225 0.044975\n#> 95  2.0035e-04    102 0.00872217 0.27223 0.044976\n#> 96  1.9446e-04    103 0.00852182 0.27232 0.044975\n#> 97  1.9166e-04    104 0.00832736 0.27133 0.044929\n#> 98  1.8824e-04    105 0.00813570 0.27103 0.044910\n#> 99  1.8713e-04    106 0.00794747 0.27072 0.044913\n#> 100 1.7808e-04    107 0.00776033 0.26983 0.044895\n#> 101 1.7610e-04    108 0.00758225 0.27000 0.044893\n#> 102 1.7325e-04    109 0.00740615 0.26984 0.044895\n#> 103 1.7018e-04    110 0.00723291 0.26968 0.044896\n#> 104 1.6527e-04    111 0.00706273 0.27027 0.044898\n#> 105 1.5789e-04    112 0.00689746 0.27057 0.044895\n#> 106 1.5735e-04    113 0.00673957 0.27046 0.044898\n#> 107 1.4751e-04    114 0.00658222 0.27066 0.044897\n#> 108 1.4632e-04    115 0.00643470 0.27055 0.044899\n#> 109 1.3986e-04    116 0.00628839 0.27039 0.044902\n#> 110 1.3925e-04    117 0.00614852 0.27111 0.044899\n#> 111 1.3479e-04    120 0.00573078 0.27113 0.044898\n#> 112 1.3357e-04    121 0.00559599 0.27084 0.044895\n#> 113 1.3245e-04    122 0.00546242 0.27097 0.044894\n#> 114 1.3171e-04    123 0.00532997 0.27101 0.044893\n#> 115 1.2728e-04    124 0.00519826 0.27137 0.044889\n#> 116 1.2691e-04    125 0.00507098 0.27085 0.044877\n#> 117 1.2493e-04    126 0.00494407 0.27066 0.044880\n#> 118 1.1699e-04    127 0.00481913 0.27148 0.044907\n#> 119 1.1655e-04    129 0.00458516 0.27168 0.044909\n#> 120 1.1542e-04    130 0.00446861 0.27209 0.044907\n#> 121 1.0244e-04    131 0.00435319 0.27047 0.044874\n#> 122 1.0244e-04    132 0.00425075 0.27085 0.044872\n#> 123 1.0205e-04    133 0.00414831 0.27085 0.044872\n#> 124 9.8401e-05    134 0.00404627 0.27127 0.044871\n#> 125 9.7938e-05    135 0.00394786 0.27064 0.044858\n#> 126 9.7938e-05    136 0.00384993 0.27069 0.044857\n#> 127 9.7128e-05    137 0.00375199 0.27069 0.044857\n#> 128 9.4118e-05    138 0.00365486 0.27006 0.044763\n#> 129 9.3663e-05    139 0.00356074 0.26991 0.044771\n#> 130 9.3243e-05    140 0.00346708 0.26991 0.044771\n#> 131 8.2635e-05    141 0.00337384 0.27063 0.044771\n#> 132 8.2635e-05    142 0.00329120 0.26996 0.044773\n#> 133 7.3547e-05    143 0.00320857 0.27017 0.044774\n#> 134 7.3049e-05    144 0.00313502 0.27061 0.044796\n#> 135 6.8395e-05    145 0.00306197 0.27067 0.044795\n#> 136 6.6928e-05    146 0.00299358 0.27005 0.044771\n#> 137 6.6928e-05    147 0.00292665 0.27005 0.044773\n#> 138 6.5562e-05    148 0.00285972 0.27007 0.044773\n#> 139 5.8916e-05    149 0.00279416 0.27023 0.044772\n#> 140 5.6726e-05    151 0.00267633 0.27085 0.044768\n#> 141 5.6471e-05    152 0.00261960 0.27079 0.044774\n#> 142 5.5090e-05    153 0.00256313 0.27079 0.044774\n#> 143 5.4263e-05    155 0.00245295 0.27081 0.044773\n#> 144 5.1296e-05    156 0.00239869 0.27094 0.044772\n#> 145 5.1296e-05    157 0.00234739 0.27146 0.044770\n#> 146 5.1053e-05    158 0.00229610 0.27146 0.044770\n#> 147 5.1003e-05    159 0.00224504 0.27146 0.044770\n#> 148 4.9576e-05    160 0.00219404 0.27117 0.044769\n#> 149 4.9308e-05    161 0.00214446 0.27118 0.044768\n#> 150 4.8615e-05    162 0.00209516 0.27114 0.044769\n#> 151 4.8615e-05    163 0.00204654 0.27154 0.044767\n#> 152 4.5354e-05    164 0.00199793 0.27154 0.044767\n#> 153 4.2544e-05    165 0.00195257 0.27175 0.044768\n#> 154 4.2519e-05    166 0.00191003 0.27179 0.044772\n#> 155 4.1488e-05    167 0.00186751 0.27179 0.044772\n#> 156 4.0759e-05    169 0.00178453 0.27173 0.044772\n#> 157 4.0675e-05    172 0.00166226 0.27164 0.044773\n#> 158 4.0141e-05    173 0.00162158 0.27112 0.044756\n#> 159 3.9661e-05    174 0.00158144 0.27111 0.044756\n#> 160 3.9133e-05    175 0.00154178 0.27111 0.044756\n#> 161 3.8851e-05    176 0.00150265 0.27118 0.044755\n#> 162 3.6878e-05    177 0.00146380 0.27161 0.044763\n#> 163 3.6524e-05    178 0.00142692 0.27163 0.044763\n#> 164 3.4197e-05    179 0.00139039 0.27160 0.044763\n#> 165 3.2895e-05    180 0.00135620 0.27153 0.044756\n#> 166 3.2781e-05    181 0.00132330 0.27172 0.044754\n#> 167 3.2438e-05    182 0.00129052 0.27183 0.044753\n#> 168 2.9746e-05    184 0.00122564 0.27187 0.044752\n#> 169 2.9503e-05    185 0.00119590 0.27228 0.044763\n#> 170 2.9381e-05    186 0.00116639 0.27228 0.044763\n#> 171 2.9381e-05    187 0.00113701 0.27228 0.044763\n#> 172 2.9139e-05    188 0.00110763 0.27228 0.044763\n#> 173 2.8420e-05    189 0.00107849 0.27255 0.044764\n#> 174 2.6761e-05    190 0.00105007 0.27232 0.044759\n#> 175 2.4484e-05    191 0.00102331 0.27260 0.044758\n#> 176 2.4282e-05    192 0.00099883 0.27181 0.044537\n#> 177 2.3311e-05    193 0.00097455 0.27213 0.044538\n#> 178 2.3083e-05    194 0.00095124 0.27216 0.044537\n#> 179 2.2309e-05    195 0.00092815 0.27216 0.044537\n#> 180 2.1930e-05    196 0.00090584 0.27171 0.044505\n#> 181 2.1854e-05    198 0.00086198 0.27169 0.044508\n#> 182 2.1854e-05    199 0.00084013 0.27169 0.044508\n#> 183 2.1409e-05    201 0.00079642 0.27169 0.044508\n#> 184 2.0325e-05    202 0.00077501 0.27181 0.044510\n#> 185 2.0235e-05    203 0.00075469 0.27120 0.044502\n#> 186 2.0235e-05    204 0.00073445 0.27120 0.044502\n#> 187 2.0235e-05    205 0.00071422 0.27120 0.044502\n#> 188 2.0235e-05    206 0.00069398 0.27120 0.044502\n#> 189 1.8439e-05    207 0.00067375 0.27120 0.044502\n#> 190 1.8363e-05    208 0.00065531 0.27111 0.044501\n#> 191 1.8363e-05    210 0.00061858 0.27113 0.044501\n#> 192 1.8363e-05    211 0.00060022 0.27113 0.044501\n#> 193 1.8262e-05    212 0.00058185 0.27113 0.044501\n#> 194 1.7099e-05    213 0.00056359 0.27096 0.044498\n#> 195 1.7099e-05    214 0.00054649 0.27094 0.044499\n#> 196 1.6390e-05    215 0.00052940 0.27108 0.044499\n#> 197 1.6390e-05    216 0.00051300 0.27106 0.044500\n#> 198 1.4620e-05    217 0.00049661 0.27091 0.044504\n#> 199 1.4620e-05    218 0.00048199 0.27104 0.044503\n#> 200 1.4610e-05    219 0.00046737 0.27104 0.044503\n#> 201 1.3380e-05    220 0.00045276 0.27124 0.044505\n#> 202 1.3380e-05    221 0.00043938 0.27143 0.044517\n#> 203 1.2950e-05    222 0.00042600 0.27144 0.044517\n#> 204 1.2950e-05    223 0.00041305 0.27144 0.044517\n#> 205 1.1382e-05    224 0.00040010 0.27168 0.044520\n#> 206 1.1382e-05    225 0.00038872 0.27179 0.044520\n#> 207 1.0927e-05    226 0.00037734 0.27176 0.044520\n#> 208 1.0118e-05    227 0.00036641 0.27189 0.044526\n#> 209 1.0118e-05    228 0.00035629 0.27189 0.044526\n#> 210 1.0000e-05    229 0.00034618 0.27189 0.044526\n```\n:::\n\nIt is probably better and easier to find this \noptimal value *programatically* as follows: \n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/prune4.2_5d6b291c38fd70f9ae1cd650e14d9170'}\n\n```{.r .cell-code}\n(b <- bos.to$cptable[which.min(bos.to$cptable[, \"xerror\"]), \"CP\"])\n#> [1] 0.00214847\n```\n:::\n\n<!-- > **R coding digression**: Note that above we could also have used the following: -->\n<!-- > ```{r prune4.alt, fig.width=6, fig.height=6, message=FALSE, warning=FALSE} -->\n<!-- > tmp <- bos.to$cptable[,\"xerror\"] -->\n<!-- > (b <- bos.to$cptable[ max( which(tmp == min(tmp)) ), \"CP\"] ) -->\n<!-- > ``` -->\n<!-- > What is the difference between `which.min(a)` and `max( which( a == min(a) ) )`? -->\n\nWe can now use the function \n`prune` on the `rpart` object setting the complexity parameter\nto the estimated optimal value found above:\n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/prune4_3a7f921c3b0170c1e126fd828b573f0e'}\n\n```{.r .cell-code}\nbos.t3 <- prune(bos.to, cp = b)\n```\n:::\n\n\nThis is how the optimally pruned tree looks:\n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/prune4.5_c0bdc6352e9d5574932edb9357747369'}\n\n```{.r .cell-code}\nplot(bos.t3, uniform = FALSE, margin = 0.01)\ntext(bos.t3, pretty = FALSE)\n```\n\n::: {.cell-output-display}\n![](25-more-trees_files/figure-html/prune4.5-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nFinally, we can check the predictions of the pruned \ntree on the test set:\n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/prune5_0ff5b7faf9108768f736765c993f80bc'}\n\n```{.r .cell-code}\n# predictions are better\npr.t3 <- predict(bos.t3, newdata = dat.te, type = \"vector\")\nwith(dat.te, mean((medv - pr.t3)^2))\n#> [1] 16.59113\n```\n:::\n\n\nAgain, it would be a **very good exercise** for you to \ncompare the MSPE of the pruned tree with that of several\nof the alternative methods we have seen in class so far,\n**without using a training / test split**. \n\n\n\n## Pruning regression trees with `tree`\n\nThe implementation of trees in the `R` package `tree` follows\nthe original CV-based pruning strategy, as discussed in \nSection 3.4 of the book\n\n> Breiman, L., Friedman, J.H., Olshen, R.A. and Stone, C.J. (1984). Classification and regression trees. Chapman & Hall.\n\nor Section 7.2 of:\n\n> Ripley, Brian D. (1996). Pattern recognition and neural networks. Cambridge University Press\n\nBoth books are available in electronic form from the UBC Library:\n[Breiman et al.](http://tinyurl.com/y3g2femt) and \n[Ripley, B.D.](http://tinyurl.com/yylchlys).\n\nWe now use the function `tree::tree()` to fit the same regression\ntree as above. Note that the default stopping criteria in this\nimplementation of regression trees is different from the one in\n`rpart::rpart()`, hence to obtain the same results as above we\nneed to modify the default stopping criteria using the argument\n`control`: \n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/prunetree0_0bcf7d6b37efc9fff5312c87336e2baa'}\n\n```{.r .cell-code}\nlibrary(tree)\nbos.t2 <- tree(medv ~ ., data = dat.tr, control = tree.control(nobs = nrow(dat.tr), mincut = 6, minsize = 20))\n```\n:::\n\n\nWe plot the resulting tree\n\n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/prunetree1_160cfe118ba2b43b240f5a4a7e43ffcc'}\n\n```{.r .cell-code}\nplot(bos.t2)\ntext(bos.t2)\n```\n\n::: {.cell-output-display}\n![](25-more-trees_files/figure-html/prunetree1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nAs discussed before, we now fit a very large tree, which will be \npruned later:\n\n\n::: {.cell layout-align=\"center\" pretty='true' hash='25-more-trees_cache/html/prunetree2_18292f48970e4d1eac02e095956225d8'}\n\n```{.r .cell-code}\nset.seed(123)\nbos.to2 <- tree(medv ~ .,\n  data = dat.tr,\n  control = tree.control(nobs = nrow(dat.tr), mincut = 1, minsize = 2, mindev = 1e-5)\n)\nplot(bos.to2)\n```\n\n::: {.cell-output-display}\n![](25-more-trees_files/figure-html/prunetree2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe now use the function `tree:cv.tree()` to estimate the MSPE of \nthe subtrees of `bos.to2`, using 5-fold CV, and plot the estimated\nMSPE (here labeled as \"deviance\") as a function of the \ncomplexity parameter (or, equivalently, the size of the tree): \n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/prunetree3_6fa3769632c67a7f61063473fad5750f'}\n\n```{.r .cell-code}\nset.seed(123)\ntt <- cv.tree(bos.to2, K = 5)\nplot(tt)\n```\n\n::: {.cell-output-display}\n![](25-more-trees_files/figure-html/prunetree3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nFinally, we use the function `prune.tree` to prune the larger tree\nat the \"optimal\" size, as estimated by `cv.tree` above:\n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/prunetree3.2_a15acf0a895165804389c78e4ecd8214'}\n\n```{.r .cell-code}\nbos.pr2 <- prune.tree(bos.to2, k = tt$k[max(which(tt$dev == min(tt$dev)))])\nplot(bos.pr2)\ntext(bos.pr2)\n```\n\n::: {.cell-output-display}\n![](25-more-trees_files/figure-html/prunetree3.2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nCompare this pruned tree with the one obtained with the regression trees\nimplementation in `rpart`. In particular, we can compare the\npredictions of this other pruned \ntree on the test set:\n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/prune.tree.pred_b561c94b165e2dce999b6ea1604921ea'}\n\n```{.r .cell-code}\n# predictions are worse than the rpart-pruned tree\npr.tree <- predict(bos.pr2, newdata = dat.te, type = \"vector\")\nwith(dat.te, mean((medv - pr.tree)^2))\n#> [1] 15.7194\n```\n:::\n\nNote that the predictions of the tree pruned with the `tree` \npackage seem to be better than those of the tree pruned with \nthe `rpart` package. **Does this mean that `rpart` gives\ntrees with worse predictions than `tree` for data coming\nfrom the process than generated our training set?** \n**Or could it all be an artifact of the specific test set we used?**\n**Can you think of an experiment to check this?**\nAgain, it would be a **very good exercise** for you to \ncheck which fit (`tree` or `rpart`) gives pruned \ntrees with better prediction properties in this case. \n\n\n## Instability of regression trees \n\nTrees can be rather unstable, in the sense that small changes in the\ntraining data set may result in relatively large differences in the\nfitted trees. As a simple illustration we randomly split the \n`Boston` data used before into two halves and fit a regression\ntree to each portion. We then display both trees.\n\n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/inst1_756ded304c0c3bbe17b2cd40dd4a047d'}\n\n```{.r .cell-code}\n# Instability of trees...\nlibrary(rpart)\ndata(Boston, package = \"MASS\")\nset.seed(123)\nn <- nrow(Boston)\nii <- sample(n, floor(n / 2))\ndat.t1 <- Boston[-ii, ]\nbos.t1 <- rpart(medv ~ ., data = dat.t1, method = \"anova\")\nplot(bos.t1, uniform = FALSE, margin = 0.01)\ntext(bos.t1, pretty = TRUE, cex = .8)\n```\n\n::: {.cell-output-display}\n![](25-more-trees_files/figure-html/inst1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/inst2_df0d0f301cb9e4c24dc233f88da9d20c'}\n\n```{.r .cell-code}\ndat.t2 <- Boston[ii, ]\nbos.t2 <- rpart(medv ~ ., data = dat.t2, method = \"anova\")\nplot(bos.t2, uniform = FALSE, margin = 0.01)\ntext(bos.t2, pretty = TRUE, cex = .8)\n```\n\n::: {.cell-output-display}\n![](25-more-trees_files/figure-html/inst2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nAlthough we would expect both random halves of the same (moderately large) \ntraining set to beat least qualitatively similar, \nNote that the two trees are rather different. \nTo compare with a more stable predictor, we fit a linear\nregression model to each half, and look at the two sets of estimated\ncoefficients side by side:\n\n::: {.cell layout-align=\"center\" hash='25-more-trees_cache/html/inst3_0568d729edf99490185590cd98949280'}\n\n```{.r .cell-code}\n# bos.lmf <- lm(medv ~ ., data=Boston)\nbos.lm1 <- lm(medv ~ ., data = dat.t1)\nbos.lm2 <- lm(medv ~ ., data = dat.t2)\ncbind(\n  round(coef(bos.lm1), 2),\n  round(coef(bos.lm2), 2)\n)\n#>               [,1]   [,2]\n#> (Intercept)  35.47  32.35\n#> crim         -0.12  -0.09\n#> zn            0.04   0.05\n#> indus         0.01   0.03\n#> chas          0.90   3.98\n#> nox         -23.90 -12.33\n#> rm            5.01   3.39\n#> age          -0.01   0.00\n#> dis          -1.59  -1.41\n#> rad           0.33   0.28\n#> tax          -0.01  -0.01\n#> ptratio      -1.12  -0.72\n#> black         0.01   0.01\n#> lstat        -0.31  -0.66\n```\n:::\n\nNote that most of the estimated regression coefficients are \nsimilar, and all of them are at least qualitatively comparable. \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}