{
  "hash": "0cd37ea980b9635bf2c0a0c5b2135f1d",
  "result": {
    "markdown": "\n\n# PCA and alternating regression {#alt-pca}\n\n\n\n\n\n\nLet $X_1, \\ldots, X_n \\in \\mathbb{R}^p$ be the observations for which we\nwant to compute the corresponding PCA. Without loss of generality we can\nalways assume that \n$$\n\\frac{1}{n} \\sum_{i=1}^n X_i \\ = (0,\\ldots,0)^\\top \\, ,\n$$ \nso that the sample covariance matrix $S_n$ is \n$$\nS_n \\ = \\ \\frac{1}{n-1} \\, \\sum_{i=1}^n X_i \\, X_i^\\top \\, .\n$$ \nWe saw in class that if $B \\in \\mathbb{R}^{p \\times k}$ has in its\ncolumns the eigenvectors of $S_n$ associated with its $k$ largest\neigenvalues, then \n$$\n\\frac{1}{n} \\, \\sum_{i=1}^n \\left\\| X_i - P( L_{B}, X_i\n) \\right\\|^2 \\ \\le \\ \n\\frac{1}{n} \\, \\sum_{i=1}^n \\left\\| X_i - P( L, X_i\n) \\right\\|^2 \\, ,\n$$ \nfor any $k$-dimensional linear subspace $L \\subset \\mathbb{R}^p$\nwhere $P( L, X)$ denotes the orthogonal projection of $X$ onto the\nsubspace $L$, $P( L_{B}, X) = {B} {B}^\\top X$ (whenever ${B}$ is chosen\nso that ${B}^\\top {B} = I$) and $L_{B}$ denotes the subspace spanned by\nthe columns of $B$.\n\nWe will show now that, instead of finding the spectral decomposition of\n$S_n$, principal components can also be computed via a sequence of\n\"alternating least squares\" problems. To fix ideas we will consider the\ncase $k=1$, but the method is trivially extended to arbitrary values of\n$k$.\n\nWhen $k=1$ we need to solve the following problem\n\\begin{equation}\n\\min_{\\left\\| a \\right\\|=1, v \\in \\mathbb{R}^n} \\\n \\sum_{i=1}^n \\left\\| X_i - a \\, v_i \\right\\|^2, \n (\\#eq:pca1)\n\\end{equation}\nwhere $v = (v_1, \\ldots v_n)^\\top$ (in general, for any $k$ we have \n$$\n\\min_{ A^\\top A = I, v_1, \\ldots, v_n \\in \\mathbb{R}^k} \\\n \\sum_{i=1}^n \\left\\| X_i - A \\, v_i \\right\\|^2 \\, ).\n$$ \nThe objective function in Equation \\@ref(eq:pca1) can also be written\nas\n\\begin{equation}\n \\sum_{i=1}^n \\sum_{j=1}^p \\left( X_{i,j} - a_j \\, v_i \\right)^2 \\, , (\\#eq:pca2)\n\\end{equation}\nand hence, for a given vector $a$, the minimizing values of\n$v_1, \\ldots, v_n$ in Equation \\@ref(eq:pca2) can be found solving $n$\nseparate least squares problems: \n$$\nv_\\ell \\, = \\, \\arg \\,  \\min_{d \\in \\mathbb{R}}\n\\sum_{j=1}^p \\left( X_{\\ell,j} - a_j \\, d \\right)^2 \\, , \\qquad\n\\ell = 1, \\ldots, n \\, .\n$$ \nSimilarly, for a given set $v_1, \\ldots, v_n$ the entries of $a$ can\nbe found solving $p$ separate least squares problems: $$\na_r \\, = \\, \\arg \\,  \\min_{d \\in \\mathbb{R}}\n\\sum_{i=1}^n \\left( X_{i, r} - d \\, v_i \\right)^2 \\, , \\qquad\nr = 1, \\ldots, p \\, .\n$$ We can then set $a \\leftarrow a / \\| a \\|$ and iterate to find new\n$v$'s, then a new $a$, etc.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}