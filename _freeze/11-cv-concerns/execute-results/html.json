{
  "hash": "5b0f24f4daceb6b44f94af3c133ac90c",
  "result": {
    "markdown": "# Cross-validation concerns\n\n\n\n\n\n\nIn this document we study how to perform cross-validation \nwhen the model was selected or determined using the \ntraining data. Consider the following synthetic data \nset\n\n::: {.cell layout-align=\"center\" hash='11-cv-concerns_cache/html/load.fallacy_90bf0de84421d168fb634e03645a19f4'}\n\n```{.r .cell-code}\ndat <- read.table(\"data/fallacy.dat\", header = TRUE, sep = \",\")\n```\n:::\n\nThis is the same data used in class. In this example\nwe know what the \"true\" model is, and thus we also know\nwhat the \"optimal\" predictor is. \nHowever, let us ignore this knowledge, and build a \nlinear model instead. \nGiven how many variables are available, we use\nforward stepwise (AIC-based) to select a good subset of\nthem to include in our linear model:\n\n::: {.cell layout-align=\"center\" hash='11-cv-concerns_cache/html/fallacy_17aa62368644995be781b776187948dc'}\n\n```{.r .cell-code}\nlibrary(MASS)\np <- ncol(dat)\nnull <- lm(Y ~ 1, data = dat)\nfull <- lm(Y ~ ., data = dat) # needed for stepwise\nstep.lm <- stepAIC(null, scope = list(lower = null, upper = full), trace = FALSE)\n```\n:::\n\nWithout thinking too much, we use 50 runs of 5-fold CV (ten runs) \nto compare the MSPE of the \n**null** model (which we know is \"true\") and the\none we obtained using forward stepwise:\n\n::: {.cell layout-align=\"center\" hash='11-cv-concerns_cache/html/wrong_082e0777c5c5b1f0078045bce01397ce'}\n\n```{.r .cell-code}\nn <- nrow(dat)\nii <- (1:n) %% 5 + 1\nset.seed(17)\nN <- 50\nmspe.n <- mspe.st <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.n <- pr.st <- rep(0, n)\n  for (j in 1:5) {\n    tmp.st <- update(step.lm, data = dat[ii != j, ])\n    pr.st[ii == j] <- predict(tmp.st, newdata = dat[ii == j, ])\n    pr.n[ii == j] <- with(dat[ii != j, ], mean(Y))\n  }\n  mspe.st[i] <- with(dat, mean((Y - pr.st)^2))\n  mspe.n[i] <- with(dat, mean((Y - pr.n)^2))\n}\nboxplot(mspe.st, mspe.n, names = c(\"Stepwise\", \"NULL\"), col = c(\"gray60\", \"hotpink\"), main = \"Wrong\")\n```\n\n::: {.cell-output-display}\n![](11-cv-concerns_files/figure-html/wrong-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nsummary(mspe.st)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>  0.5931  0.6392  0.6658  0.6663  0.6945  0.7517\nsummary(mspe.n)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   1.044   1.050   1.057   1.057   1.062   1.084\n```\n:::\n\n\n* **Something is wrong!** What? Why? \n* What would you change above to obtain reliable estimates for the MSPE of the \nmodel selected with the stepwise approach? \n\n\n<!-- ## Correlated covariates -->\n\n<!-- Technological advances in recent decades have resulted in data  -->\n<!-- being collected in a fundamentally different way from the way  -->\n<!-- it was when \"classical\" statistical methods were proposed.  -->\n<!-- Specifically, it is not at all uncommon to have data sets with -->\n<!-- an abundance of potentially useful explanatory variables.  -->\n<!-- Sometimes the investigators are not sure which of them can be  -->\n<!-- expected to be useful or meaningful. In many applications one -->\n<!-- finds data with many more variables than cases.  -->\n\n<!-- A consequence of this \"wide net\" data collection strategy is  -->\n<!-- that many of the explanatory variables may be correlated with -->\n<!-- each other. In what follows we will illustrate some of the -->\n<!-- problems that this can cause both when training and interpreting -->\n<!-- models, and also with the resulting predictions. -->\n\n<!-- ### Significant variables \"dissappear\" -->\n\n<!-- Consider the air pollution data set, and the fit to the  -->\n<!-- **reduced** linear regression model used previously in class: -->\n<!-- ```{r signif} -->\n<!-- # Correlated covariates -->\n<!-- x <- read.table('../Lecture1/rutgers-lib-30861_CSV-1.csv', header=TRUE, sep=',') -->\n<!-- reduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data=x) -->\n<!-- round( summary(reduced)$coef, 3) -->\n<!-- ``` -->\n<!-- Note that all coefficients seem to be significant based on -->\n<!-- the individual tests of hypothesis (with `POOR` and  -->\n<!-- `HOUS` maybe only marginally so). In this sense all 5 -->\n<!-- explanatory varibles in this model appear to be relevant. -->\n\n<!-- Now, we fit the **full** model, that is, we include -->\n<!-- all available explanatory variables in the data set: -->\n<!-- ```{r signif2} -->\n<!-- full <- lm(MORT ~ ., data=x) -->\n<!-- round( summary(full)$coef, 3) -->\n<!-- ``` -->\n<!-- Now we have many more parameters to estimate, and while two of -->\n<!-- them appear to be significantly different from zero (`NONW` -->\n<!-- and `PREC`), all the others seem to be redundant.  -->\n<!-- In particular, note that the p-values for the individual -->\n<!-- test of hypotheses for 4 out of the 5   -->\n<!-- regression coefficients for the variables of the **reduced** -->\n<!-- model have now become not significant. -->\n<!-- ```{r signif3} -->\n<!-- round( summary(full)$coef[ names(coef(reduced)), ], 3) -->\n<!-- ``` -->\n\n<!-- ### Why does this happen?  -->\n\n<!-- Recall that the covariance matrix of the least squares estimator involves the -->\n<!-- inverse of (X'X), where X' denotes the transpose of the n x p matrix X (that -->\n<!-- contains each vector of explanatory variables as a row). It is easy to see  -->\n<!-- that if two columns of X are linearly dependent, then X'X will be rank deficient.  -->\n<!-- When two columns of X are \"close\" to being linearly dependent (e.g. their -->\n<!-- linear corrleation is high), then the matrix X'X will be ill-conditioned, and -->\n<!-- its inverse will have very large entries. This means that the estimated  -->\n<!-- standard errors of the least squares estimator will be unduly large, resulting -->\n<!-- in non-significant test of hypotheses for each parameter separately, even if -->\n<!-- the global test for all of them simultaneously is highly significant. -->\n\n<!-- ### Why is this a problem if we are interested in prediction? -->\n\n<!-- Although in many applications one is interested in interpreting the parameters -->\n<!-- of the model, even if one is only trying to fit / train a model to do -->\n<!-- predictions, highly variable parameter estimators will typically result in -->\n<!-- a noticeable loss of prediction accuracy. This can be easily seen from the  -->\n<!-- bias / variance factorization of the mean squared prediction error (MSPE)  -->\n<!-- mentioned in class. Hence, better predictions can be obtained if one -->\n<!-- uses less-variable parameter estimators.  -->\n\n<!-- ### What can we do? -->\n\n<!-- A commonly used strategy is to remove some explanatory variables from the -->\n<!-- model, leaving only non-redundant covariates. However, this is easier said than -->\n<!-- done. You have seen some strategies in other courses (stepwise variable selection, etc.) -->\n<!-- In coming weeks we will investigate other methods to deal with this problem. -->\n\n\n## Estimating MSPE with CV when the model was built using the data\n\n<!--Last week we learned that one needs to be careful when using cross-validation (in any of its flavours--leave one out, K-fold, etc.) -->\n\nMisuse of cross-validation is, unfortunately,\nnot unusual. For [one example](https://doi.org/10.1073/pnas.102102699) see [@Ambroise6562].\n\nIn particular, for every fold one needs to repeat **everything** that was done with the training set (selecting variables, looking at pairwise correlations, AIC values, etc.)\n\n## Correlated covariates\n\nTechnological advances in recent decades have resulted in data\nbeing collected in a fundamentally different manner from the way\nit was done when most \"classical\" statistical methods were developed\n(early to mid 1900's).\nSpecifically, it is now not at all uncommon to have data sets with\nan abundance of potentially useful explanatory variables \n(for example with more variables than observations). \nSometimes the investigators are not sure which of the collected variables\ncan be\nexpected to be useful or meaningful. \n\nA consequence of this \"wide net\" data collection strategy is\nthat many of the explanatory variables may be correlated with\neach other. In what follows we will illustrate some of the\nproblems that this can cause both when training and interpreting\nmodels, and also with the resulting predictions.\n\n### Variables that were important may suddenly \"dissappear\"\n\nConsider the air pollution data set we used \nearlier, and the \n**reduced** linear regression model discussed in class:\n\n::: {.cell layout-align=\"center\" hash='11-cv-concerns_cache/html/signif_81218798f45858c7b901da378fccc27c'}\n\n```{.r .cell-code}\nx <- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\nreduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x)\nround(summary(reduced)$coef, 3)\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) 1172.831    143.241   8.188    0.000\n#> POOR          -4.065      2.238  -1.817    0.075\n#> HC            -1.480      0.333  -4.447    0.000\n#> NOX            2.846      0.652   4.369    0.000\n#> HOUS          -2.911      1.533  -1.899    0.063\n#> NONW           4.470      0.846   5.283    0.000\n```\n:::\n\nNote that all coefficients seem to be significant based on\nthe individual tests of hypothesis (with `POOR` and\n`HOUS` maybe only marginally so). In this sense all 5\nexplanatory varibles in this model appear to be relevant.\n\nNow, we fit the **full** model, that is, we include\nall available explanatory variables in the data set:\n\n::: {.cell layout-align=\"center\" hash='11-cv-concerns_cache/html/signif2_2614141d3cc49fa24f655cb25e06b4c1'}\n\n```{.r .cell-code}\nfull <- lm(MORT ~ ., data = x)\nround(summary(full)$coef, 3)\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) 1763.981    437.327   4.034    0.000\n#> PREC           1.905      0.924   2.063    0.045\n#> JANT          -1.938      1.108  -1.748    0.087\n#> JULT          -3.100      1.902  -1.630    0.110\n#> OVR65         -9.065      8.486  -1.068    0.291\n#> POPN        -106.826     69.780  -1.531    0.133\n#> EDUC         -17.157     11.860  -1.447    0.155\n#> HOUS          -0.651      1.768  -0.368    0.714\n#> DENS           0.004      0.004   0.894    0.376\n#> NONW           4.460      1.327   3.360    0.002\n#> WWDRK         -0.187      1.662  -0.113    0.911\n#> POOR          -0.168      3.227  -0.052    0.959\n#> HC            -0.672      0.491  -1.369    0.178\n#> NOX            1.340      1.006   1.333    0.190\n#> SO.            0.086      0.148   0.585    0.562\n#> HUMID          0.107      1.169   0.091    0.928\n```\n:::\n\nIn the **full** model there \nare many more parameters that need to be estimated, and while two of\nthem appear to be significantly different from zero (`NONW`\nand `PREC`), all the others appear to be redundant.\nIn particular, note that the p-values for the individual\ntest of hypotheses for 4 out of the 5\nregression coefficients for the variables of the **reduced**\nmodel have now become not significant.\n\n::: {.cell layout-align=\"center\" hash='11-cv-concerns_cache/html/signif3_ed604463db13646d18acd477db9a6855'}\n\n```{.r .cell-code}\nround(summary(full)$coef[names(coef(reduced)), ], 3)\n#>             Estimate Std. Error t value Pr(>|t|)\n#> (Intercept) 1763.981    437.327   4.034    0.000\n#> POOR          -0.168      3.227  -0.052    0.959\n#> HC            -0.672      0.491  -1.369    0.178\n#> NOX            1.340      1.006   1.333    0.190\n#> HOUS          -0.651      1.768  -0.368    0.714\n#> NONW           4.460      1.327   3.360    0.002\n```\n:::\n\nIn other words, the coeffficients of \nexplanatory variables that appeared to \nbe relevant in one model may turn\nto be \"not significant\" when other variables\nare included. This could pose some challenges \nfor interpreting the estimated parameters of the\nmodels.\n\n\n### Why does this happen?\n\nRecall that the covariance matrix of the least squares estimator involves the\ninverse of (X'X), where X' denotes the transpose of the n x p matrix X (that\ncontains each vector of explanatory variables as a row). It is easy to see\nthat if two columns of X are linearly dependent, then X'X will be rank deficient.\nWhen two columns of X are \"close\" to being linearly dependent (e.g. their\nlinear corrleation is high), then the matrix X'X will be ill-conditioned, and\nits inverse will have very large entries. This means that the estimated\nstandard errors of the least squares estimator will be unduly large, resulting\nin non-significant test of hypotheses for each parameter separately, even if\nthe global test for all of them simultaneously is highly significant.\n\n### Why is this a problem if we are interested in prediction?\n\nAlthough in many applications one is interested in interpreting the parameters\nof the model, even if one is only trying to fit / train a model to do\npredictions, highly variable parameter estimators will typically result in\na noticeable loss of prediction accuracy. This can be easily seen from the\nbias / variance factorization of the mean squared prediction error (MSPE)\nmentioned in class. Hence, better predictions can be obtained if one\nuses less-variable parameter (or regression function) estimators.\n\n### What can we do?\n\nA commonly used strategy is to remove some explanatory variables from the\nmodel, leaving only non-redundant covariates. However, this is easier said than\ndone. You will have seen some strategies in previous Statistics\ncourses (e.g. stepwise variable selection). \nIn coming weeks we will investigate other methods to deal with this problem.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}