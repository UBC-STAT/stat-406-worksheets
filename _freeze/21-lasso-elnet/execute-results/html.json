{
  "hash": "0eb6e2337c67aa93576d156b2e66845e",
  "result": {
    "markdown": "# LASSO \n\n\n\n\n\n\nA different approach to perform *some kind* of variable selection that may be\nmore stable than stepwise methods is to use an L1 regularization term\n(instead of the L2 one used in ridge regression). Notwidthstanding the\ngeometric \"interpretation\" of the effect of using an L1 penalty, \nit can also be argued that the L1 norm is, in some cases, a convex relaxation\n(envelope) of the \"L0\" norm (the number of non-zero elements). As a result,\nestimators based on the LASSO (L1-regularized regression) will typically have some \nof their entries equal to zero. \n\nJust as it was the case for Ridge Regression, \nas the value of the penalty parameter increases, the solutions \nto the L1 regularized problem change\nfrom the usual least squares estimator (when the regularization parameter equals\nto zero) to a vector of all zeroes (when the penalty constant is sufficiently \nlarge). One difference between using an L1 or an L2 penalty is that\nfor an L1-regularized problem, there usually is a finite value of the penalty term\nthat produces a solution of all zeroes, whereas for the L2 penalizations\nthis is not generally true. \n\nThe sequence of solutions changing by value of the penalty parameter\nis often used as a way to rank (or \"sequence\") the explanatory variables, listing them\nin the order in which they \"enter\" (their estimated coefficient changes from \nzero to a non-zero value). We can \nalso estimate the MSPE of each solution (on a finite\ngrid of values of the penalty parameter) to select one with\ngood prediction properties. If any of the \nestimated regression coefficients in the selected solution are exactly zero it\nis commonly said that those explanatory variables are not included \nin the chosen model. \n\nThere are two main implementation of the LASSO in `R`, one is\nvia the `glmnet` function (in package `glmnet`), and the other\nis with the function `lars` in package `lars`. Both, of course,\ncompute the same estimators, but they do so in different ways. \n\nWe first compute the path of LASSO solutions for the `credit` data\nused in previous lectures:\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/creditlasso_0c721edcaf3cab28fe52d63355604792'}\n\n```{.r .cell-code}\nx <- read.table(\"data/Credit.csv\", sep = \",\", header = TRUE, row.names = 1)\n# use non-factor variables\nx <- x[, c(1:6, 11)]\ny <- as.vector(x$Balance)\nxm <- as.matrix(x[, -7])\nlibrary(glmnet)\n# alpha = 1 - LASSO\nlambdas <- exp(seq(-3, 10, length = 50))\na <- glmnet(\n  x = xm, y = y, lambda = rev(lambdas),\n  family = \"gaussian\", alpha = 1, intercept = TRUE\n)\n```\n:::\n\n\nThe `plot` method can be used to show the path of solutions, just as\nwe did for ridge regression:\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/creditlasso3_4e95d9534d404f81a9a9677088f8587c'}\n\n```{.r .cell-code}\nplot(a, xvar = \"lambda\", label = TRUE, lwd = 6, cex.axis = 1.5, cex.lab = 1.2)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/creditlasso3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nUsing `lars::lars()` we obtain:\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/creditlars1_8aea043f3a29e8d6e1338e6de5220afe'}\n\n```{.r .cell-code}\nlibrary(lars)\nb <- lars(x = xm, y = y, type = \"lasso\", intercept = TRUE)\nplot(b, lwd = 4)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/creditlars1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWith `lars` the returned object is a matrix of regression estimators, one\nfor each value of the penalty constant where a new coefficient \"enters\" the\nmodel:\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/creditlars2_5ed2460fbc4ec93d07f86a228a14445d'}\n\n```{.r .cell-code}\n# see the variables\ncoef(b)\n#>         Income      Limit   Rating     Cards        Age Education\n#> [1,]  0.000000 0.00000000 0.000000  0.000000  0.0000000  0.000000\n#> [2,]  0.000000 0.00000000 1.835963  0.000000  0.0000000  0.000000\n#> [3,]  0.000000 0.01226464 2.018929  0.000000  0.0000000  0.000000\n#> [4,] -4.703898 0.05638653 2.433088  0.000000  0.0000000  0.000000\n#> [5,] -5.802948 0.06600083 2.545810  0.000000 -0.3234748  0.000000\n#> [6,] -6.772905 0.10049065 2.257218  6.369873 -0.6349138  0.000000\n#> [7,] -7.558037 0.12585115 2.063101 11.591558 -0.8923978  1.998283\nb\n#> \n#> Call:\n#> lars(x = xm, y = y, type = \"lasso\", intercept = TRUE)\n#> R-squared: 0.878 \n#> Sequence of LASSO moves:\n#>      Rating Limit Income Age Cards Education\n#> Var       3     2      1   5     4         6\n#> Step      1     2      3   4     5         6\n```\n:::\n\n\nThe presentation below exploits the fact that the LASSO regression estimators\nare piecewise linear between values of the regularization parameter where\na variable enters or drops the model.\n\nIn order to select one LASSO estimator (among the infinitely many that\nare possible) we can use K-fold CV to estimate the MSPE of a few of them \n(for a grid of values of the penalty parameter, for example), and \nchoose the one with smallest estimated MSPE:\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/creditlars3_83ffcc7ac377be28d5eb09c63a44ff46'}\n\n```{.r .cell-code}\n# select one solution\nset.seed(123)\ntmp.la <- cv.lars(\n  x = xm, y = y, intercept = TRUE, type = \"lasso\", K = 5,\n  index = seq(0, 1, length = 20)\n)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/creditlars3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nGiven their random nature, it is always a good idea to run K-fold CV experiments \nmore than once:\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/creditlars4_b39d9240bb3b4887d6fb4f545e6258ef'}\n\n```{.r .cell-code}\nset.seed(23)\ntmp.la <- cv.lars(\n  x = xm, y = y, intercept = TRUE, type = \"lasso\", K = 5,\n  index = seq(0, 1, length = 20)\n)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/creditlars4-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe now repeat the same steps as above but using the implementation\nin `glmnet`:\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/creditcv_3d62d4c0744f1c7809e33cae0d96a65e'}\n\n```{.r .cell-code}\n# run 5-fold CV with glmnet()\nset.seed(123)\ntmp <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 1,\n  family = \"gaussian\", intercept = TRUE\n)\nplot(tmp, lwd = 6, cex.axis = 1.5, cex.lab = 1.2)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/creditcv-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe ran CV again:\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/creditcv2_8334a421e652cc08890bea5eb1454f28'}\n\n```{.r .cell-code}\nset.seed(23)\ntmp <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 1,\n  family = \"gaussian\", intercept = TRUE\n)\nplot(tmp, lwd = 6, cex.axis = 1.5, cex.lab = 1.2)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/creditcv2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nZoom in the CV plot to check the 1-SE rule:\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/creditcv4_00bdb384a20f2a3489c61e763cfa3d10'}\n\n```{.r .cell-code}\nplot(tmp, lwd = 6, cex.axis = 1.5, cex.lab = 1.2, ylim = c(22000, 33000))\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/creditcv4-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThe returned object includes the \"optimal\" value of the \npenalization parameter, which can be used to \nfind the corresponding estimates for the regression\ncoefficients, using the method `coef`:\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/creditcv3_f94e718232915b18660ba94fa4e65815'}\n\n```{.r .cell-code}\n# optimal lambda\ntmp$lambda.min\n#> [1] 0.04978707\n# coefficients for the optimal lambda\ncoef(tmp, s = tmp$lambda.min)\n#> 7 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                       s1\n#> (Intercept) -481.9460966\n#> Income        -7.5489897\n#> Limit          0.1141714\n#> Rating         2.2352534\n#> Cards         10.7283522\n#> Age           -0.8914429\n#> Education      2.0194979\n```\n:::\n\nWe can also use `coef` to compute the coefficients at\nany value of the penalty parameter. For example we\nshow below the coefficients corresponding \nto penalty values of exp(4) and exp(4.5):\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/creditcoeffs_597c2ef84a1f2188b0a26875e219658e'}\n\n```{.r .cell-code}\n# coefficients for other values of lambda\ncoef(tmp, s = exp(4))\n#> 7 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                        s1\n#> (Intercept) -262.35053476\n#> Income        -0.63094341\n#> Limit          0.02749778\n#> Rating         1.91772580\n#> Cards          .         \n#> Age            .         \n#> Education      .\ncoef(tmp, s = exp(4.5)) # note no. of zeroes...\n#> 7 x 1 sparse Matrix of class \"dgCMatrix\"\n#>                        s1\n#> (Intercept) -175.98151842\n#> Income         .         \n#> Limit          0.01492881\n#> Rating         1.76170516\n#> Cards          .         \n#> Age            .         \n#> Education      .\n```\n:::\n\n\n## Compare MSPEs of Ridge & LASSO on the credit data\n\nWe now use 50 runs of 5-fold cross-validation to\nestimate (and compare) the MSPEs of the different \nestimators / predictors:\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/mspecredit_fc3b0c3956db144e66285dbaa4f9e0a5'}\n\n```{.r .cell-code}\nlibrary(MASS)\nn <- nrow(xm)\nk <- 5\nii <- (1:n)%%k + 1\nset.seed(123)\nN <- 50\nmspe.la <- mspe.st <- mspe.ri <- mspe.f <- rep(0, N)\nfor (i in 1:N) {\n    ii <- sample(ii)\n    pr.la <- pr.f <- pr.ri <- pr.st <- rep(0, n)\n    for (j in 1:k) {\n        tmp.ri <- cv.glmnet(x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n            nfolds = 5, alpha = 0, family = \"gaussian\")\n        tmp.la <- cv.glmnet(x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n            nfolds = 5, alpha = 1, family = \"gaussian\")\n        null <- lm(Balance ~ 1, data = x[ii != j, ])\n        full <- lm(Balance ~ ., data = x[ii != j, ])\n        tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = 0)\n        pr.ri[ii == j] <- predict(tmp.ri, s = \"lambda.min\", newx = xm[ii == j, ])\n        pr.la[ii == j] <- predict(tmp.la, s = \"lambda.min\", newx = xm[ii == j, ])\n        pr.st[ii == j] <- predict(tmp.st, newdata = x[ii == j, ])\n        pr.f[ii == j] <- predict(full, newdata = x[ii == j, ])\n    }\n    mspe.ri[i] <- mean((x$Balance - pr.ri)^2)\n    mspe.la[i] <- mean((x$Balance - pr.la)^2)\n    mspe.st[i] <- mean((x$Balance - pr.st)^2)\n    mspe.f[i] <- mean((x$Balance - pr.f)^2)\n}\nboxplot(mspe.la, mspe.ri, mspe.st, mspe.f, names = c(\"LASSO\", \"Ridge\", \"Stepwise\",\n    \"Full\"), col = c(\"steelblue\", \"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1,\n    cex.lab = 1, cex.main = 2)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/mspecredit-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe see that in this example LASSO does not seem to provide better\npredictions than Ridge Regression. However, it does yield a \nsequence of explanatory variables that can be interpreted as\nbased on \"importance\" for the linear regression model (see\nabove).\n\n\n\n\n## Comparing LASSO with Ridge Regression on the air pollution data\n\nLet us compare the Ridge Regression and LASSO fits to the\nair pollution data. Of course, by *the Ridge Regression fit*\nand *the LASSO fit* we mean the fit obtained with the\noptimal value of the penalty constant chosen in terms\nof the corresponding estimated MSPE (which is in \ngeneral estimated using K-fold cross validation). \n\nWe first load the data and use `cv.glmnet()` with \n`alpha = 0` to select an **approximately optimal** \nRidge Regression fit (what makes the calculation \nbelow **only approximately** optimal?).\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/comparing.airp_5d0f92db0c896eee6fb374b84885feac'}\n\n```{.r .cell-code}\nairp <- read.table(\"data/rutgers-lib-30861_CSV-1.csv\", header = TRUE, sep = \",\")\ny <- as.vector(airp$MORT)\nxm <- as.matrix(airp[, names(airp) != \"MORT\"])\nlambdas <- exp(seq(-3, 10, length = 50))\n# Ridge Regression\nset.seed(23)\nair.l2 <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0,\n  family = \"gaussian\", intercept = TRUE\n)\nplot(air.l2)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/comparing.airp-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThe plot above is included for illustration purposes only. \nSimilarly, we now compute an approximately optimal LASSO fit, \nand look at the curve of estimated MSPEs:\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/airp.lasso_ac523400525432f031596c4005f0a944'}\n\n```{.r .cell-code}\n# LASSO\nset.seed(23)\nair.l1 <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 1,\n  family = \"gaussian\", intercept = TRUE\n)\nplot(air.l1)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/airp.lasso-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nIt is interesting to compare the corresponding estimated regression coefficients, \nso we put them side by side in two columns:\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/airp.corr.groups_80ddf56c72b7c8bca8ad75f8d66183b8'}\n\n```{.r .cell-code}\ncbind(\n  round(coef(air.l2, s = \"lambda.min\"), 3),\n  round(coef(air.l1, s = \"lambda.min\"), 3)\n)\n#> 16 x 2 sparse Matrix of class \"dgCMatrix\"\n#>                   s1       s1\n#> (Intercept) 1129.267 1070.341\n#> PREC           1.493    1.420\n#> JANT          -0.999   -1.124\n#> JULT          -1.054   -0.877\n#> OVR65         -2.260    .    \n#> POPN          -1.621    .    \n#> EDUC          -8.280  -10.800\n#> HOUS          -1.164   -0.380\n#> DENS           0.005    0.003\n#> NONW           2.895    3.825\n#> WWDRK         -0.464    .    \n#> POOR           0.653    .    \n#> HC            -0.030    .    \n#> NOX            0.056    .    \n#> SO.            0.237    0.226\n#> HUMID          0.388    .\n```\n:::\n\n\nNote how several of them are relatively similar, but LASSO includes fewer of them.\nA possible explanation for this is the particular correlation structure among the\nexplanatory variables. More specifically, when groups of \ncorrelated covariates are present, \nLASSO tends to choose only one of them, whereas Ridge Regression will tend \nto keep all of them. For a formal statement see [@ZouHastie2005, Lemma 2].\n\nIt is important to note here that the above observations regarding the Ridge Regression\nand LASSO fits trained on the air pollution data should be made on a more \nreliable (more stable, less variable) choice of penalty parameter. For example,\nwe may want to run the above 5-fold CV experiments several times and take the\naverage of the estimated optimal penalty parameters. To simplify the presentation\nwe do not purse this here, but it may be a very good exercise for the reader to\ndo so.\n\nThe following heatmap of the pairwise correlations among explanatory variables\nreveals certain patterns that may be used to explain the difference\nmentioned above. Note that in this visualization method variables were\ngrouped (\"clustered\") according to their pairwise correlations in order to\nimprove the interpretability of the plot. We will see later in this course\nthe particular clustering method used here (hierarchical clustering).\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/airp.correlations_5e08af238cd85c1920ca54480edebdad'}\n\n```{.r .cell-code}\nlibrary(ggcorrplot)\nggcorrplot(cor(xm), hc.order = TRUE, outline.col = \"white\")\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/airp.correlations-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n## Compare MSPE of Ridge and LASSO on air pollution data\n\nSince our focus was on the properties of the resulting predictions, it may be\ninteresting to compare the estimated MSPE of the different models / predictors\nwe have considered so far: a full linear model, a model selected via stepwise + AIC, \nridge regression and LASSO. As usual, we use 50 runs of 5-fold CV, and obtain\nthe following boxplots:\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/bigcompare_cfc21a522d23c5d611d7e2fff5c4d7bf'}\n\n```{.r .cell-code}\nlibrary(MASS)\nn <- nrow(xm)\nk <- 5\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 50\nmspe.la <- mspe.st <- mspe.ri <- mspe.f <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.la <- pr.f <- pr.ri <- pr.st <- rep(0, n)\n  for (j in 1:k) {\n    tmp.ri <- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 0, family = \"gaussian\"\n    )\n    tmp.la <- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 1, family = \"gaussian\"\n    )\n    null <- lm(MORT ~ 1, data = airp[ii != j, ])\n    full <- lm(MORT ~ ., data = airp[ii != j, ])\n    tmp.st <- stepAIC(null, scope = list(lower = null, upper = full), trace = FALSE)\n    pr.ri[ii == j] <- predict(tmp.ri, s = \"lambda.min\", newx = xm[ii == j, ])\n    pr.la[ii == j] <- predict(tmp.la, s = \"lambda.min\", newx = xm[ii == j, ])\n    pr.st[ii == j] <- predict(tmp.st, newdata = airp[ii == j, ])\n    pr.f[ii == j] <- predict(full, newdata = airp[ii == j, ])\n  }\n  mspe.ri[i] <- mean((airp$MORT - pr.ri)^2)\n  mspe.la[i] <- mean((airp$MORT - pr.la)^2)\n  mspe.st[i] <- mean((airp$MORT - pr.st)^2)\n  mspe.f[i] <- mean((airp$MORT - pr.f)^2)\n}\nboxplot(mspe.la, mspe.ri, mspe.st, mspe.f, names = c(\"LASSO\", \"Ridge\", \"Stepwise\", \"Full\"), col = c(\"steelblue\", \"gray80\", \"tomato\", \"springgreen\"), cex.axis = 1, cex.lab = 1, cex.main = 2)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/bigcompare-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe see that there is a marginal advantage of LASSO, but it is rather minor, and \nthe three methods we have seen so far improve by similar margins \non the predictions obtained by using a full linear regression model. \n\n## Less desirable properties of LASSO\n\nAs important as the LASSO estimator has been, its properties may sometimes \nnot be fully satisfactory. In particular:\n\n* The LASSO selects the right variables only under very restrictive conditions (in other words, it is generally not \"variable selection\"-consistent).\n* The LASSO sampling distribution is not the same as the one we would obtain with the standard least squares estimator if we knew which features to include and which ones to exclude from the model (in orther words, the LASSO does not have an \"oracle\" property). \n* When groups of correlated explanatory variables are present the LASSO tends to include only one variable (randomly) from the group, relegate the others to the end of the sequence. \n\nFor precise statements and theoretical results regarding the three points above, see [@ZouHastie2005; @Zou2006].\n\n\n\n## Elastic net\n\nElastic Net estimators were introduced to find an \ninformative compromise between LASSO and Ridge Regression. \n\nNote that `cv.glmnet` only considers fits with variying \nvalues of one of the penalty constants, while the other\none (`alpha`) is kept fixed. To compare different \nElastic Net fits we run `cv.glmnet` with 4 values of\n`alpha`: 0.05, 0.1, 0.5 and 0.75. \n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/airp.en_a7ff01b2e504e08eea3492ffd7c40d5d'}\n\n```{.r .cell-code}\n# EN\nset.seed(23)\nair.en.75 <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0.75,\n  family = \"gaussian\", intercept = TRUE\n)\nset.seed(23)\nair.en.05 <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0.05,\n  family = \"gaussian\", intercept = TRUE\n)\nset.seed(23)\nair.en.1 <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0.1,\n  family = \"gaussian\", intercept = TRUE\n)\nset.seed(23)\nair.en.5 <- cv.glmnet(\n  x = xm, y = y, lambda = lambdas, nfolds = 5, alpha = 0.5,\n  family = \"gaussian\", intercept = TRUE\n)\nplot(air.en.05)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/airp.en-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nplot(air.en.5)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/airp.en-2.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nplot(air.en.75)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/airp.en-3.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n### Run EN on airpollution data, compare fits\n\nWe now compare the estimates of the regression coefficients\nobtained with the different methods discussed so far to \nalleviate potential problems caused by correlated covariates. \n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/airp.en.compare_b2d6305314317f07475d1d85abfc1e21'}\n\n```{.r .cell-code}\na <- cbind(\n  round(coef(air.l2, s = \"lambda.min\"), 3),\n  round(coef(air.l1, s = \"lambda.min\"), 3),\n  round(coef(air.en.05, s = \"lambda.min\"), 3),\n  round(coef(air.en.1, s = \"lambda.min\"), 3),\n  round(coef(air.en.5, s = \"lambda.min\"), 3),\n  round(coef(air.en.75, s = \"lambda.min\"), 3)\n)\ncolnames(a) <- c(\"Ridge\", \"LASSO\", \"EN-05\", \"EN-10\", \"EN-50\", \"EN-75\")\na\n#> 16 x 6 sparse Matrix of class \"dgCMatrix\"\n#>                Ridge    LASSO    EN-05    EN-10    EN-50    EN-75\n#> (Intercept) 1129.267 1070.341 1116.791 1112.228 1101.074 1099.067\n#> PREC           1.493    1.420    1.479    1.481    1.498    1.495\n#> JANT          -0.999   -1.124   -0.968   -0.990   -1.124   -1.153\n#> JULT          -1.054   -0.877   -1.036   -1.041   -1.156   -1.182\n#> OVR65         -2.260    .       -1.099   -0.265    .        .    \n#> POPN          -1.621    .        .        .        .        .    \n#> EDUC          -8.280  -10.800   -8.277   -8.413   -9.585  -10.147\n#> HOUS          -1.164   -0.380   -1.136   -1.102   -0.705   -0.575\n#> DENS           0.005    0.003    0.005    0.005    0.004    0.004\n#> NONW           2.895    3.825    3.187    3.454    3.816    3.895\n#> WWDRK         -0.464    .       -0.422   -0.391   -0.141   -0.052\n#> POOR           0.653    .        0.268    .        .        .    \n#> HC            -0.030    .       -0.006   -0.003    .        .    \n#> NOX            0.056    .        0.000    .        .        .    \n#> SO.            0.237    0.226    0.242    0.241    0.233    0.230\n#> HUMID          0.388    .        0.290    0.241    0.061    0.005\n```\n:::\n\n\nThe same comment made above regarding the need of a \nmore stable choice of \"optimal\" fits (for each of these\nmethods) applies here. Again, here we limit ourselves to one\nrun of 5-fold CV purely based on simplifying the\npresentation. \n\n\n### Compare MSPE's of Full, LASSO, Ridge, EN and stepwise\n\n\n\n::: {.cell layout-align=\"center\" hash='21-lasso-elnet_cache/html/bigcompare2_2c45ce2ecb066d5d41122c46779f48f6'}\n\n```{.r .cell-code}\nii <- (1:n) %% k + 1\nset.seed(123)\nN <- 50\nmspe.en <- rep(0, N)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.en <- rep(0, n)\n  for (j in 1:k) {\n    tmp.en <- cv.glmnet(\n      x = xm[ii != j, ], y = y[ii != j], lambda = lambdas,\n      nfolds = 5, alpha = 0.75, family = \"gaussian\"\n    )\n    pr.en[ii == j] <- predict(tmp.en, s = \"lambda.min\", newx = xm[ii == j, ])\n  }\n  mspe.en[i] <- mean((airp$MORT - pr.en)^2)\n}\nboxplot(mspe.en, mspe.la, mspe.ri, mspe.st, mspe.f,\n  names = c(\"EN\", \"LASSO\", \"Ridge\", \"Stepwise\", \"Full\"),\n  col = c(\"hotpink\", \"steelblue\", \"gray80\", \"tomato\", \"springgreen\"),\n  cex.axis = 1, cex.lab = 1, cex.main = 2\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n```\n\n::: {.cell-output-display}\n![](21-lasso-elnet_files/figure-html/bigcompare2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe see that in this example Elastic Net with `alpha = 0.75` (which is not far from \nthe LASSO) provides slightly better estimated MSPEs.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}