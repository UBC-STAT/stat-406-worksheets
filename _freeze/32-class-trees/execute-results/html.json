{
  "hash": "554ac82510640cf55c1264e7adaa38e2",
  "result": {
    "markdown": "# Classification Trees\n\n\n\n\n\n\n\nJust as in the continuous regression case, when the number of available\nexplanatory variables is moderate or large, methods like nearest neighbours\nquickly become unfeasible, or their performance is not satisfactory. Classification\ntrees provide a good alternative: they are still model-free (we do not need to\nassume anything about the true conditional probabilities of each class for a given\nvector of features **X**), but they are constrained to have a fairly specifc \nform. Intuitively (and informally) we could say (if nobody was listening) that\nthis restriction provides some form of regularization or penalization. \n\nClassification trees are constructed in much the same was as regression trees.\nWe will construct a partition of the feature space (in \"rectangular\" areas),\nand within each region we will predict the class to be the most common class\namong the training points that  are in that region. It is reasonable then\nto try to find a partition of the feature space so that in each area there is\nonly one class (or at least, such that one class clearly dominates the others\nin that region). Hence, to build a classification tree we need a quantitative measure of \nthe homogeneity of the classes present in a node. Given such a \nnumerical measure, we can build the tree\nby selecting, at each step, the optimal split in the sense of yielding the most\nhomogeneous child leaves possible (i.e. by maximizing at each step the \nchosen homogeneity measure). The two most common homogeneity measures\nare the Gini Index and the deviance (refer to the discussion in class). \nAlthough the resulting trees are generally different depending on which \nloss function is used, we will later see that this difference is not\ncritical in practice. \n\nAs usual, in order to be able to visualize what is going on, we will \nillustrate the training and use of classification trees on a simple toy example.\nThis example contains data on admissions to graduate school. There are\n2 explanatory variables (GPA and GMAT scores), and the response has 3 \nlevels: Admitted, No decision, Not admitted. The purpose here is to \nbuild a classifier to decide if a student will be admitted to \ngraduate school based on her/his GMAT and GPA scores. \n\nWe first read the data, convert the response into a proper `factor`\nvariable, and visualize the training set:\n\n\n::: {.cell layout-align=\"center\" hash='32-class-trees_cache/html/trees1_2bd746148a1db9531729e1c1095365ac'}\n\n```{.r .cell-code}\nmm <- read.table(\"data/T11-6.DAT\", header = FALSE)\nmm$V3 <- as.factor(mm$V3)\n# re-scale one feature, for better plots\nmm[, 2] <- mm[, 2] / 150\nplot(mm[, 1:2],\n  pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]],\n  xlab = \"GPA\", \"GMAT\", xlim = c(2, 5), ylim = c(2, 5)\n)\n```\n\n::: {.cell-output-display}\n![](32-class-trees_files/figure-html/trees1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNext we build a classification tree using the Gini index as splitting\ncriterion. \n\n::: {.cell layout-align=\"center\" hash='32-class-trees_cache/html/trees2_9513ca8fdf8e0d7b8cc6027f4204cf3a'}\n\n```{.r .cell-code}\nlibrary(rpart)\na.t <- rpart(V3 ~ V1 + V2, data = mm, method = \"class\", parms = list(split = \"gini\"))\nplot(a.t, margin = 0.05)\ntext(a.t, use.n = TRUE)\n```\n\n::: {.cell-output-display}\n![](32-class-trees_files/figure-html/trees2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nIf we use the deviance as splitting criterion instead, we obtain the following\nclassification tree (also using the default stopping criteria): \n\n\n::: {.cell layout-align=\"center\" hash='32-class-trees_cache/html/trees3_08994f6586f044ff2833c1293fc4b0a2'}\n\n```{.r .cell-code}\na.t <- rpart(V3 ~ V1 + V2, data = mm, method = \"class\", parms = list(split = \"information\"))\nplot(a.t, margin = 0.05)\ntext(a.t, use.n = TRUE)\n```\n\n::: {.cell-output-display}\n![](32-class-trees_files/figure-html/trees3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\nThe predicted conditional probabilities for each class on the range of values\nof the explanatory variables present on the training set can be visualized\nexactly as before: \n\n\n::: {.cell layout-align=\"center\" hash='32-class-trees_cache/html/trees4_ca54f568a989775a12eb64a857979662'}\n\n```{.r .cell-code}\naa <- seq(2, 5, length = 200)\nbb <- seq(2, 5, length = 200)\ndd <- expand.grid(aa, bb)\nnames(dd) <- names(mm)[1:2]\np.t <- predict(a.t, newdata = dd, type = \"prob\")\n```\n:::\n\n\nWe display the estimated conditional probabilities for each class:\n\n\n::: {.cell layout-align=\"center\" hash='32-class-trees_cache/html/trees5_df07f5e87347781a83a26b291bb5cd1e'}\n\n```{.r .cell-code}\nfilled.contour(aa, bb, matrix(p.t[, 1], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](32-class-trees_files/figure-html/trees5-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nfilled.contour(aa, bb, matrix(p.t[, 2], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](32-class-trees_files/figure-html/trees5-2.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nfilled.contour(aa, bb, matrix(p.t[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](32-class-trees_files/figure-html/trees5-3.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n## Pruning\n\nJust like regression trees, classification trees generally perform better if they are built \nby pruning an overfitting one. This is done in the same way as it is done for classification\ntrees. When we do it on the graduate school admissions data we indeed obtain estimated\nconditional probabilities that appear to be more sensible (less \"simple\"): \n\n\n::: {.cell layout-align=\"center\" hash='32-class-trees_cache/html/trees7_74745413a225255c883a4c7bc3f1d6a5'}\n\n```{.r .cell-code}\nset.seed(123)\na.t <- rpart(V3 ~ V1 + V2,\n  data = mm, method = \"class\", control = rpart.control(minsplit = 3, cp = 1e-8, xval = 10),\n  parms = list(split = \"information\")\n)\nb <- a.t$cptable[which.min(a.t$cptable[, \"xerror\"]), \"CP\"]\na.t <- prune(a.t, cp = b)\np.t <- predict(a.t, newdata = dd, type = \"prob\")\nfilled.contour(aa, bb, matrix(p.t[, 1], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](32-class-trees_files/figure-html/trees7-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nfilled.contour(aa, bb, matrix(p.t[, 2], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](32-class-trees_files/figure-html/trees7-2.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nfilled.contour(aa, bb, matrix(p.t[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](32-class-trees_files/figure-html/trees7-3.png){fig-align='center' width=90%}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}