{
  "hash": "7c948f51e9928a36f917b27140cf2e64",
  "result": {
    "markdown": "\n# Predictions using a linear model (continued)\n\n\n\n\n\n\nIn these notes we continue looking at the problem of \ncomparing different models based on their \nprediction properties. As in the previous lecture, we consider \na **full** and a **reduced** model, and in all that follows we assume that the\nvariables included in the **reduced** model were not selected using the training data. \n**This seemingly innocent assumption is in fact critical, and we will later come back to it.**\n\n\n## Estimating the MSPE with a test set\n\nOne way to estimate the mean squared prediction error of a \nmodel or predictor is to use it on a test set (where the\nresponses are known, but that was not used when training the\npredcitor or estimating the model), and the comparing the\npredictions with the actual responses. \n\nFirst, we load the training set and fit both models:\n\n::: {.cell layout-align=\"center\" hash='10-test-set-and-cv_cache/html/read_34bbb3a4e3ba54822f0cccf363b1a371'}\n\n```{.r .cell-code}\nx.tr <- read.table(\"data/pollution-train.dat\", header = TRUE, sep = \",\")\nfull <- lm(MORT ~ ., data = x.tr)\nreduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\n```\n:::\n\nAlthough the **full** model fits the data better than the\nreduced one (see Lecture 1), its predictions on the test set are better.\nFirst, compute the two vectors of test set predictions:\n\n::: {.cell layout-align=\"center\" hash='10-test-set-and-cv_cache/html/pred1_5d5280532edc7593ac5f79b4dafa8673'}\n\n```{.r .cell-code}\nx.te <- read.table(\"data/pollution-test.dat\", header = TRUE, sep = \",\")\npr.full <- predict(full, newdata = x.te)\npr.reduced <- predict(reduced, newdata = x.te)\n```\n:::\n\nAnd now, use them to estimate the mean squared prediction error of \neach model:\n\n::: {.cell layout-align=\"center\" hash='10-test-set-and-cv_cache/html/pred1.2_0cb9d340b4c08b7b303eca9873f38f67'}\n\n```{.r .cell-code}\nwith(x.te, mean((MORT - pr.full)^2))\n#> [1] 2859.367\nwith(x.te, mean((MORT - pr.reduced)^2))\n#> [1] 1861.884\n```\n:::\n\nPreviously, we also saw that \nthis is not just an artifact of the specific\ntraining / test split of the data. The **reduced**\nmodel generally produces better predictions, \nregardless of the specific training / test\nsplit we use. We can verify this repeating \nthe procedure many times (100, say) and looking\nat the estimated mean squared prediction errors\nobtained each time for each model. \n\n\n::: {.cell layout-align=\"center\" hash='10-test-set-and-cv_cache/html/testrain_89ec55866c6d0bb63e57ca8ff3c3098b'}\n\n```{.r .cell-code}\nx <- read.csv(\"data/rutgers-lib-30861_CSV-1.csv\")\nset.seed(123)\nNsplits <- 100\nmspe.full <- mspe.red <- vector(\"numeric\", Nsplits)\nfor (j in 1:Nsplits) {\n  g <- sample(rep(1:4, each = 15))\n  a.tr <- x[g != 2, ]\n  a.te <- x[g == 2, ]\n  full <- lm(MORT ~ ., data = a.tr)\n  reduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = a.tr)\n  a.te$pr.full <- predict(full, newdata = a.te)\n  a.te$pr.reduced <- predict(reduced, newdata = a.te)\n  mspe.full[j] <- with(a.te, mean((MORT - pr.full)^2))\n  mspe.red[j] <- with(a.te, mean((MORT - pr.reduced)^2))\n}\nboxplot(mspe.full, mspe.red,\n  names = c(\"Full\", \"Reduced\"),\n  col = c(\"gray80\", \"tomato3\"),\n  main = \"Air Pollution - 100 training/test splits\", ylim = c(0, 5000)\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n```\n\n::: {.cell-output-display}\n![](10-test-set-and-cv_files/figure-html/testrain-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n## Leave-one-out cross-validation\n\nA different procedure to estimate the prediction power\nof a model or method is called **leave-one-out CV**. \nOne advantage of using this method is that \nthe model we fit can use a larger training set.\nWe discussed the procedure in class. Here\nwe apply it to estimate the mean squared\nprediction error of the **full** and **reduced**\nmodels. Again, we assume that the reduced model \nwas chosen independently from the training set.\n\n::: {.cell layout-align=\"center\" hash='10-test-set-and-cv_cache/html/loocv_4145e4619130a09b5f5397369abce7e8'}\n\n```{.r .cell-code}\nx <- read.csv(\"data/rutgers-lib-30861_CSV-1.csv\")\nn <- nrow(x)\npr.full <- pr.reduced <- rep(0, n)\nfor (i in 1:n) {\n  full <- lm(MORT ~ ., data = x[-i, ])\n  reduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x[-i, ])\n  pr.full[i] <- predict(full, newdata = x[i, ])\n  pr.reduced[i] <- predict(reduced, newdata = x[i, ])\n}\n```\n:::\n\nNow we have the leave-one-out predictions for each model\nand can compute the corresponding estimated mean squared\nprediction errors:\n\n::: {.cell layout-align=\"center\" hash='10-test-set-and-cv_cache/html/loocv2_3b38332bb3a16e99e71adc2f8b2764e6'}\n\n```{.r .cell-code}\nmean((x$MORT - pr.full)^2)\n#> [1] 2136.785\nmean((x$MORT - pr.reduced)^2)\n#> [1] 1848.375\n```\n:::\n\nNote that here again the reduced model seems to yield better\nprediction errors. \n\n## K-fold cross-validation\n\nLeave-one-out cross-validation can be computationally \nvery demanding (or even unfeasible) when the sample size\nis large and training the predictor is relatively costly. \nOne solution is called **K-fold CV**. We split the data\ninto **K** folds, train the predictor on the data without\na fold, and use it to predict the responses \nin the removed fold. We cycle through the folds, and\nuse the average of the squared prediction errors as\nan estimate of the mean squared prediction error.\nThe following script does **5-fold CV** for the \n`full` and `reduced` linear models on the\npollution dataset, once again assuming that the reduced model \nwas originally chosen without using the data.\n\n::: {.cell layout-align=\"center\" hash='10-test-set-and-cv_cache/html/kfold_ef68c4f9347180c6e6fba73a0554bfee'}\n\n```{.r .cell-code}\nn <- nrow(x)\nk <- 5\npr.full <- pr.reduced <- rep(0, n)\n# Create labels for the \"folds\"\ninds <- (1:n) %% k + 1\n# shuffle the rows of x, this is bad coding!\nset.seed(123)\nxs <- x[sample(n, replace = FALSE), ]\n# loop through the folds\nfor (j in 1:k) {\n  x.tr <- xs[inds != j, ]\n  x.te <- xs[inds == j, ]\n  full <- lm(MORT ~ ., data = x.tr)\n  reduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x.tr)\n  pr.full[inds == j] <- predict(full, newdata = x.te)\n  pr.reduced[inds == j] <- predict(reduced, newdata = x.te)\n}\n```\n:::\n\nWe now compute the estimated mean squared prediction\nerror of each model:\n\n::: {.cell layout-align=\"center\" hash='10-test-set-and-cv_cache/html/kfold2_d924cc90171bda32ae392f6cb2ae4576'}\n\n```{.r .cell-code}\nmean((xs$MORT - pr.full)^2)\n#> [1] 2227.21\nmean((xs$MORT - pr.reduced)^2)\n#> [1] 2003.857\n```\n:::\n\nThis method is clearly faster than leave-one-out CV, but\nthe results may depend on the specific fold partition,\nand on the number **K** of folds used. \n\n* One way to obtain more stable mean squared prediction errors \nusing K-fold CV is to repeat the above procedure\nmany times, and compare the distribution of the\nmean squared prediction errors for each estimator.\nFirst, fit the **full** and **reduced** models using the\nwhole data set as training:\n\n::: {.cell layout-align=\"center\" hash='10-test-set-and-cv_cache/html/fits_35985816ad16a3408de22b4e6627676c'}\n\n```{.r .cell-code}\nm.f <- lm(MORT ~ ., data = x)\nm.r <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data = x)\n```\n:::\n\nWe will use 50 runs of 5-fold CV comparing the **full** and **reduced** models.\nAgain, here we assume that the reduced model was not obtained\nusing the training data. \n\n::: {.cell layout-align=\"center\" hash='10-test-set-and-cv_cache/html/cv10runs_692cd518663a1ccf04a3e391a97c6532'}\n\n```{.r .cell-code}\nN <- 50\nmspe1 <- mspe2 <- vector(\"double\", N)\nii <- (1:(n <- nrow(x))) %% 5 + 1\nset.seed(327)\nfor (i in 1:N) {\n  ii <- sample(ii)\n  pr.f <- pr.r <- vector(\"double\", n)\n  for (j in 1:5) {\n    pr.f[ii == j] <- predict(\n      update(m.f, data = x[ii != j, ]),\n      newdata = x[ii == j, ]\n    )\n    pr.r[ii == j] <- predict(\n      update(m.r, data = x[ii != j, ]),\n      newdata = x[ii == j, ]\n    )\n  }\n  mspe1[i] <- with(x, mean((MORT - pr.f)^2))\n  mspe2[i] <- with(x, mean((MORT - pr.r)^2))\n}\nboxplot(mspe1, mspe2,\n  names = c(\"Full\", \"Reduced\"),\n  col = c(\"gray80\", \"tomato3\"),\n  main = \"Air Pollution - 50 runs 5-fold CV\", ylim = c(0, 5000)\n)\nmtext(expression(hat(MSPE)), side = 2, line = 2.5)\n```\n\n::: {.cell-output-display}\n![](10-test-set-and-cv_files/figure-html/cv10runs-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote that the estimated mean squared prediction \nerror of the **reduced** model has a smaller mean / median\nthan that of the **full** one. This tells us that\nthe conclusion we reached favouring the reduced model \n(in terms of its prediction mean squared error) does\nnot depend on a particular choice of folds. In other\nwords, this provides more evidence to conclude that\nthe reduced model will produce better predictions\nthan the full one. \n\n* A computationally simpler (albeit possibly less precise) way\nto account for the K-fold variability is to run \nK-fold CV once and \nuse the sample standard error of the\n**K** *smaller* mean squared prediction errors to\nconstruct a rough *confidence interval* around\nthe overall mean squared prediction error estimate (that is\nthe average of the mean squared prediction errors\nover the K folds).\n\n* The dependency of this MSPE on **K** is more involved.\nWe will discuss it later. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}