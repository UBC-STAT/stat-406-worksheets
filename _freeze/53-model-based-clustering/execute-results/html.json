{
  "hash": "7e4e052edfd9d92a619a131335fb36a6",
  "result": {
    "markdown": "# Model based clustering\n\n\n\n\n\n\nModel-based clustering methods depend on a probabilistic \nmodel that specifies the distribution of the observed features\n(over the whole population). This distribution is \ntypically modelled as a mixture of several\ndifferent distributions. Given a sample of *n* vectors of\nfeatures $X_1$, $X_2$, ..., $X_n$, the clustering problem then becomes \nthe estimation of the *n* unobserved labels that indicate to which\nsub-population (cluster, group) each $X_i$ belongs. In addition, \none generally also has to estimate the parameters that specify the \ndistribution of $X$ in each assumed group.\n\nGiven that this method is based on a full specificification of \nthe distribution of the observed vector of features, it is not \nsurprising that the parameters are generally estimated using maximum\nlikelihood. The difficulty is that there are *n* unobserved (missing)\nvariables (the group labels) that also need to be estimated (*imputed*).\nThe most commonly used approach uses the EM algorithm to \nperform maximum likelihood estimation with missing observations.\n\n## EM algorithm\n\nThe specifics of the EM algorithm were introduced and discussed in\nclass. Although the algorithm may seem clear at first sight, \nit is fairly subtle, and mistakes and misunderstandings are \nvery (**very**) common. Many applications of the EM algorithm\nfound on-line are either wrong, or wrongly derived. \nFor a more detailed discussion and a different\n(and also very useful) application of the algorithm, see the\nSection  **Imputation via EM** below.\n\n\n## Bivariate Gaussian mixture model via EM \"by hand\"\n\nWe will use a 2-dimensional representation of the\nUN votes data. This lower-dimensional representation\nis obtained using multidimensional scaling, a topic\nwe will cover later in the course. For formulas\nand specific steps of the algorithm please refer \nto your class notes. \nWe first load the data and reduce it to a 2-dimensional\nproblem, in order to be able to plot the results. \nIt will be a very nice exercise for the reader to \nre-do this analysis on the original data set.\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/initial_edd1fe860138d555645d156bc3eee4c4'}\n\n```{.r .cell-code}\nX <- read.table(file = \"data/unvotes.csv\", sep = \",\", row.names = 1, header = TRUE)\n# Compute pairwise distances and use MDS\ndd <- dist(t(X))\ntmp <- cmdscale(dd, k = 2)\n```\n:::\n\n\nThis is the data with which we will work: \n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/scatter_fe9d2a5089f851962fee4eb7e5a97c26'}\n\n```{.r .cell-code}\nplot(tmp, pch = 19, col = \"gray50\", cex = 2, xlab = \"\", ylab = \"\")\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/scatter-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe will now use the EM algorithm to find (Gaussian-ly\ndistributed) clusters in the data. First\nwe find initial maximum likelihood estimators (i.e. initial\nvalues for the EM algorithm), using a random\npartition of the data:\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/init.EM_dc9e9384838355a41fd214bcfe28af44'}\n\n```{.r .cell-code}\nk <- 3\nn <- nrow(tmp)\nset.seed(123456)\nb <- sample((1:n) %% k + 1)\ngammas <- matrix(0, n, k)\nfor (j in 1:k) gammas[b == j, j] <- 1\npis <- colSums(gammas) / n\nmus <- sigmas <- vector(\"list\", k)\nfor (j in 1:k) {\n  mus[[j]] <- colSums(tmp * gammas[, j]) / sum(gammas[, j])\n  sigmas[[j]] <- t(tmp * gammas[, j]) %*% tmp / sum(gammas[, j])\n}\n```\n:::\n\nNote that the above loop could have been computed more efficiently\nusing the fact that at the initial\nstep the gamma coefficients are either 0's or 1's. \nHowever, in the following steps of the EM algorithm we will\nneed to use such *weighted averages* computations, since\nin general the weights are between 0 and 1. \n\nThis is the initial configuration (pure noise):\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/initial.scatter_075ade12bbbc0cd4cd2663312984ff38'}\n\n```{.r .cell-code}\nplot(tmp[, 1], tmp[, 2],\n  pch = 19, cex = 2,\n  col = c(\"black\", \"red\", \"darkblue\")[b], xlab = \"\", ylab = \"\"\n)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/initial.scatter-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe now launch our iterations. Here I run 120 iterations. Can you \nthink of an appropriate convergence criterion? Should\nwe look at the parameter\nestimates, the gammas (posterior class probabilities), \nthe likelihood function?\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/EM.alg_ac948617f3478e8b40e9caf062d58463'}\n\n```{.r .cell-code}\nlibrary(mvtnorm)\nniter <- 120\nfor (i in 1:niter) {\n  # E step\n  # compute posterior probabilites f(x_i, \\theta^k)\n  for (j in 1:k) {\n    gammas[, j] <- apply(tmp, 1, dmvnorm,\n      mean = mus[[j]],\n      sigma = sigmas[[j]]\n    )\n  }\n  # multiply by probs of each class\n  # f(x_i, \\theta^k) * pi_k\n  gammas <- gammas %*% diag(pis)\n  # standardize: f(x_i, \\theta^k) * pi_k / [ sum_s { f(x_i, \\theta^s) * pi_s } ]\n  gammas <- gammas / rowSums(gammas)\n  # M step\n  # the maximizers of the expected likelihood have\n  # a closed form in the Gaussian case, they are\n  # just weighted means and covariance matrices\n  for (j in 1:k) {\n    mus[[j]] <- colSums(tmp * gammas[, j]) / sum(gammas[, j])\n    tmp2 <- scale(tmp, scale = FALSE, center = mus[[j]])\n    sigmas[[j]] <- t(tmp2 * gammas[, j]) %*% tmp2 / sum(gammas[, j])\n  }\n  # update pi's\n  pis <- colSums(gammas) / n # n = sum(colSums(gammas))\n}\n```\n:::\n\nWe now plot the estimated density for X, which is\na combination of 3 gaussian densities.\nWe do this by evaluating the estimated densities\non a relatively fine grid of points and displaying them.\nWe will color the points according to the estimated\ngroup labels (the largest estimated posterior\nprobability for each point). We first compute those\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/displ1_3cc781cb8ed701a56b3a4b71e6d906d3'}\n\n```{.r .cell-code}\n# estimated groups\nemlab <- apply(gammas, 1, which.max)\n# build a 100 x 100 grid\nngr <- 100\nx1 <- seq(-15, 15, length = ngr)\nx2 <- seq(-10, 7, length = ngr)\nxx <- expand.grid(x1, x2)\n# evaluate each density component on each grid point\nm <- matrix(NA, ngr * ngr, k)\nfor (j in 1:k) {\n  m[, j] <- apply(xx, 1, dmvnorm, mean = mus[[j]], sigma = sigmas[[j]])\n}\n# apply weights\nmm <- m %*% pis # apply(m, 1, max)\nfilled.contour(x1, x2, matrix(mm, ngr, ngr),\n  col = terrain.colors(35),\n  xlab = \"\", ylab = \"\",\n  panel.last = {\n    points(tmp[, 1], tmp[, 2], pch = 19, cex = 1, col = c(\"black\", \"red\", \"darkblue\")[emlab])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/displ1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe can also show each separate estimated component:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/sep.dens_7faa71b0f19cfe4c0c2a4f90bb8b8037'}\n\n```{.r .cell-code}\nm2 <- m %*% diag(pis)\nfor (j in 1:k) {\n  filled.contour(x1, x2, matrix(m2[, j], ngr, ngr),\n    col = terrain.colors(35), xlab = \"\", ylab = \"\",\n    panel.last = {\n      points(tmp[, 1], tmp[, 2], pch = 19, cex = 1, col = c(\"black\", \"red\", \"darkblue\")[emlab])\n    }\n  )\n}\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/sep.dens-1.png){fig-align='center' width=90%}\n:::\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/sep.dens-2.png){fig-align='center' width=90%}\n:::\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/sep.dens-3.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## Model assumptions may be important\n\nWe will illustrate the problem with a synthetic data set.\nThere are 3 groups with 300 observations in each, \nand 3 variables / features.\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/noise1_111d12027ad9704b92453945ee4a6ffc'}\n\n```{.r .cell-code}\n# sample size\nn <- 300\n\n# covariance matrices for two of the groups\ns1 <- matrix(c(2, -1, -1, -1, 2, 1, -1, 1, 1), ncol = 3, byrow = TRUE)\ns2 <- matrix(c(4, 0, -1, 0, 4, 3, -1, 3, 5), ncol = 3, byrow = TRUE)\ns1.sqrt <- chol(s1)\ns2.sqrt <- chol(s2)\n\n# easy case, well separated groups\nset.seed(31)\nx1 <- matrix(rnorm(n * 3), n, 3) %*% s1.sqrt\nmu2 <- c(8, 8, 3)\nx2 <- scale(matrix(rnorm(n * 3), n, 3) %*% s2.sqrt, center = -mu2, scale = FALSE)\nmu3 <- c(-5, -5, -10)\nx3 <- scale(matrix(rnorm(n * 3), n, 3), center = -mu3, scale = FALSE)\nx <- rbind(x1, x2, x3)\n```\n:::\n\n\nThis is how the data look\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/disp.noise1_d63bbba5ea911db771870b7ffe2ab71f'}\n\n```{.r .cell-code}\npairs(x, col = \"gray\", pch = 19)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/disp.noise1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nIt is not a surprise that model-based clustering works\nvery well in this case:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/mclust.noise1_60dda883f29070363fcdfec8f8b40509'}\n\n```{.r .cell-code}\nlibrary(mclust)\n# select the number of clusters using likelihood-base criterion\nm <- Mclust(x)\n# show the data, color-coded according to the groups found\npairs(x, col = m$class)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/mclust.noise1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe now create a data set that does not satisfy the model:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/wrong_b90c3f870a966826e0510830634842af'}\n\n```{.r .cell-code}\nset.seed(31)\nx1 <- matrix(rexp(n * 3, rate = .2), n, 3)\nmu2 <- c(10, 20, 20)\nx2 <- scale(matrix(runif(n * 3, min = -6, max = 6), n, 3), center = -mu2, scale = FALSE)\nmu3 <- c(-5, -5, -5)\nx3 <- scale(matrix(rnorm(n * 3, sd = 3), n, 3), center = -mu3, scale = FALSE)\nx.3 <- rbind(x1, x2, x3)\n\n# run model-based clustering,\n# select the number of clusters using likelihood-base criterion\nm3 <- Mclust(x.3)\n\n# show the data, colors according to groups found\npairs(x.3, col = m3$class)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/wrong-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThe problem is with the likelihood-based criterion used by \n`mclust()` to select the number of clusters. Note that the\nfunction increases until k = 3, and it \nalmost stops growing after k = 4. The \nthe maximum is nonetheless attained at k = 8.\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/wrong.bic_d373978a2794b29d0e8012effe8d78e9'}\n\n```{.r .cell-code}\nplot(m3$BIC[, 6], type = \"b\", xlab = \"K\", ylab = \"BIC\", lwd = 2, pch = 19)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/wrong.bic-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nIt is interesting to note that \nK-means would have found\nthe right number of clusters and cluster memberships\nrather easily. Here is the sum-of-squares plot based\non K-means, which indicates that K = 3 is a sensible\nchoice:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/wrong.kmeans.ss_700afda721f45cd3bf31e16dd0ada6c2'}\n\n```{.r .cell-code}\n# run k-means with k = 2, 2, ..., 10\n# to try to identify how many clusters are present\nm3.l <- vector(\"list\", 10)\nss <- rep(0, 10)\nfor (i in 2:10) {\n  ss[i] <- sum((m3.l[[i]] <- kmeans(x.3, centers = i, nstart = 500))$within)\n}\nplot(2:10, ss[-1], xlab = \"K\", ylab = \"W_k\", type = \"b\", lwd = 2, pch = 19)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/wrong.kmeans.ss-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThe clusters found when K-means was run with kK = 3 were:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/wrong.kmeans.clusters_ab44425124b2bd957c0788c0d24e0b75'}\n\n```{.r .cell-code}\npairs(x.3, col = m3.l[[3]]$cluster)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/wrong.kmeans.clusters-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nFurthermore, if you force `mclust()` to use 3 classes\nit works fairly well, even thought the model is wrong. The \nmain problem here is that BIC depends heavily on the \nassumed likelihood / probabilistic model:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/wrong.forced_48fc4a962f57c7a025ec41b84bf43f32'}\n\n```{.r .cell-code}\nm3.3 <- Mclust(x.3, G = 3)\npairs(x.3, col = m3.3$class)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/wrong.forced-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n\n\n## Behaviour when there are noise variables\n\nThe presence of noise variables (i.e. features that\nare non-informative about clusters that may\nbe present in the data) can be quite damaging to\nthese methods (both K-means and mclust)\nWe will create two data sets with \"noise\" features:\none with Gaussian noise, and\none with uniformly distributed noise.\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/noise_09f209bb9aee6d37e353b948d798e728'}\n\n```{.r .cell-code}\nset.seed(31)\nx1 <- matrix(rnorm(n * 3, mean = 3), n, 3) %*% s1.sqrt\nmu2 <- c(9, 9, 3)\nx2 <- scale(matrix(rnorm(n * 3), n, 3) %*% s2.sqrt, center = -mu2, scale = FALSE)\nmu3 <- c(5, 5, -10)\nx3 <- scale(matrix(rnorm(n * 3), n, 3), center = -mu3, scale = FALSE)\nx <- rbind(x1, x2, x3)\n# non-normal \"noise\" features\nx.4 <- cbind(x, matrix(rexp(n * 3 * 3, rate = 1 / 10), n * 3, 3))\n# normal \"noise\" features\nx.5 <- cbind(x, matrix(rnorm(n * 3 * 3, mean = 0, sd = 150), n * 3, 3))\n```\n:::\n\nWe now find clusters using a Gaussian model,\nand select the number of clusters using likelihood-base criterion:\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/noise.mclust_bcf834f15b7dee6f9caab93c87c200ad'}\n\n```{.r .cell-code}\nm4 <- Mclust(x.4)\nm5 <- Mclust(x.5)\n```\n:::\n\nIf we use the first 3\nfeatures (which are the ones that determine the\ncluster structure)\nto\nshow the clusters found by `mclust`\nwhen the noise was not Gaussian, we get:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/unif.noise.mclust_7b98e14e438345def761da7278504321'}\n\n```{.r .cell-code}\npairs(x.4[, 1:3], col = m4$class, pch = 19)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/unif.noise.mclust-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nAnd even when the noise had a Gaussian\ndistribution, we do not identify the ``right'' clusters:\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/normal.noise.mclust_2feab302228b421e8595f16e7a906271'}\n\n```{.r .cell-code}\n# pairs(x.5[,1:3], col=m5$class, pch=19)\ntable(m5$class, rep(1:3, each = n))\n#>    \n#>       1   2   3\n#>   1 300   1   0\n#>   2   0 299   0\n#>   3   0   0 300\n```\n:::\n\n\nIf we force `mclust()` to identify 3 clusters, things look\nmuch better both for Gaussian and non-Gaussian noise:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/force.noise.mclust_c5d9570ace402711154914482a95a4de'}\n\n```{.r .cell-code}\nm4.3 <- Mclust(x.4, G = 3)\nm5.3 <- Mclust(x.5, G = 3)\n# it works well\npairs(x.4[, 1:3], col = m4.3$class, pch = 19)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/force.noise.mclust-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\npairs(x.5[, 1:3], col = m5.3$class, pch = 19)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/force.noise.mclust-2.png){fig-align='center' width=90%}\n:::\n:::\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/force.noise.mclust0_7ad9425782b56cb64eb71ebd68cf38d6'}\n\n```{.r .cell-code}\ntable(m4.3$class, rep(1:3, each = n))\n#>    \n#>       1   2   3\n#>   1 300   5   0\n#>   2   0 295   0\n#>   3   0   0 300\ntable(m5.3$class, rep(1:3, each = n))\n#>    \n#>       1   2   3\n#>   1 300   1   0\n#>   2   0 299   0\n#>   3   0   0 300\n```\n:::\n\n\nNote that noise also affects K-means seriously.\nI refer you to the robust and sparse K-means\nmethod (links on the module's main page).\n\nWithin sum-of-squares plot\nfor K-means with non-Gaussian noise:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/unif.noise.kmeans_4a655c4ac497f10ac6e2ac0ec71296f8'}\n\n```{.r .cell-code}\nm4.l <- vector(\"list\", 10)\nss <- rep(0, 10)\nfor (i in 2:10) {\n  ss[i] <- sum((m4.l[[i]] <- kmeans(x.4, centers = i, nstart = 100, iter.max = 20))$within)\n}\nplot(2:10, ss[-1], xlab = \"K\", ylab = \"W_k\", type = \"b\", lwd = 2, pch = 19)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/unif.noise.kmeans-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWithin sum-of-squares plot\nfor K-means with Gaussian noise:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/gauss.noise.kmeans_4f821d692769cb585daa1b025bcc439d'}\n\n```{.r .cell-code}\nm5.l <- vector(\"list\", 10)\nss <- rep(0, 10)\nfor (i in 2:10) {\n  ss[i] <- sum((m5.l[[i]] <- kmeans(x.5, centers = i, nstart = 100, iter.max = 20))$within)\n}\nplot(2:10, ss[-1], xlab = \"K\", ylab = \"W_k\", type = \"b\", lwd = 2, pch = 19)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/gauss.noise.kmeans-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNot even forcing `k-means` to identify 3 clusters helps when\nthere are noise features:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/force.noise.kmeans_c4a536cb12678342296130e42410d126'}\n\n```{.r .cell-code}\npairs(x.4[, 1:3], col = m4.l[[3]]$cluster, pch = 19)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/force.noise.kmeans-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\npairs(x.5[, 1:3], col = m5.l[[3]]$cluster, pch = 19)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/force.noise.kmeans-2.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## Imputation via EM (a detailed example \"by hand\")\n\nMissing data is a rather prevalent problem,\nand different strategies to replace them by sensible \n\"predictions\" exit. They are collectively\ncalled \"imputation methods\". In these notes we will \nfollow the missing data example discussed in class and\nuse the EM algorithm to impute partially unobserved data points in a\nsynthetic bivariate Gaussian data set. Furthemore, the scripts\nbelow are designed for the case where only one\nentry may be missing in each observation. It is not\ndifficult to extend this to data with more coordinates\nand more than one entry missing. Please refer to your \nclass notes for formulas and details. \n\n### A synthetic example\n\nTo illustrate the method in a simple setting where\nwe can visualize the ideas on a 2-dimensional scatter\nplot, we will work with a *toy* example. \nWe first create a simple synthetic data set with\n50 observations in 2 dimensions, normally distributed with center\nat the point (3,7), and a fairly strong correlation\nbetween its two coordinates:\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/create_d62b88581853660ff4431902f243e67e'}\n\n```{.r .cell-code}\nlibrary(mvtnorm)\n# mean vector\nmu <- c(3, 7)\n# variance/covariance matrix\nsi <- matrix(c(1, 1.2, 1.2, 2), 2, 2)\n# generate data\nset.seed(123)\nx <- rmvnorm(50, mean = mu, sigma = si)\n```\n:::\n\nThis is the data. The larger red point indicates\nthe sample mean (3.13, 7.15):\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/scatter0_b3682c922e3f4ab8a32ff17cbdf9a7df'}\n\n```{.r .cell-code}\nplot(x, pch = 19, col = \"gray30\", xlim = c(0, 8), ylim = c(0, 13), xlab = \"X1\", ylab = \"X2\", cex = 1.5)\nxbar <- colMeans(x)\npoints(xbar[1], xbar[2], pch = 19, col = \"tomato\", cex = 2)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/scatter0-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nAssume we have an observation (5, **NA**) where the \nsecond coordinate is missing, and \nanother one (**NA**, 5.5) with the first coordinate\nmissing. We indicate them with grey lines\nto indicate the uncertainty about their missing\nentries:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/scatter.missing0_43dddc07f3c133301550746e4103d004'}\n\n```{.r .cell-code}\nplot(x, pch = 19, col = \"gray30\", xlim = c(0, 8), ylim = c(0, 13), xlab = \"X1\", ylab = \"X2\", cex = 1.5)\nabline(v = 5, lwd = 6, col = \"gray80\")\nabline(h = 5.5, lwd = 6, col = \"gray80\")\npoints(x, pch = 19, col = \"gray30\", cex = 1.5)\npoints(xbar[1], xbar[2], pch = 19, col = \"tomato\", cex = 2)\ntext(1, 6, \"(NA, 5.5)\")\ntext(6, 2, \"(5, NA)\")\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/scatter.missing0-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nA simple method to impute the missing coordinates would be to\nreplace them by the mean of the missing variable over the\nrest of the data. Hence (5, **NA**) becomes (5, *7.15*) and \n(**NA**, 5.5) becomes (*3.13*, 5.5). The imputed points\nare shown below as blue dots:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/marginal0_1a804680ae495da803c1001091cb6412'}\n\n```{.r .cell-code}\nplot(x, pch = 19, col = \"gray30\", xlim = c(0, 8), ylim = c(0, 13), xlab = \"X1\", ylab = \"X2\", cex = 1.5)\nabline(h = 5.5, lwd = 6, col = \"gray80\")\nabline(v = 5, lwd = 6, col = \"gray80\")\npoints(x, pch = 19, col = \"gray30\", cex = 1.5)\npoints(xbar[1], xbar[2], pch = 19, col = \"tomato\", cex = 2)\npoints(5, xbar[2], pch = 19, col = \"steelblue\", cex = 2)\npoints(xbar[1], 5.5, pch = 19, col = \"steelblue\", cex = 2)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/marginal0-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote that the imputed points are in fact away from the bulk\nof the data, even though this is not\napparent if you look at each \ncoordinate separately. \nA better imputation method uses the EM algorithm. \n\nWe assume that the points in our data can be modelled as\noccurences of a bivariate random vector with a normal / Gaussian\ndistribution. The unknown parameters are its mean vector\nand 2x2 variance/covariance matrix. The EM algorithm will alternate\nbetween computing the expected value of the log-likelihood for the \nfull (non-missing) data set conditional on the actually observed\npoints (even incompletely observed ones), and finding the\nparameters (mean vector and covariance matrix) that maximize\nthis conditional expected log-likelihood. \n\nIt is not trivial to see that the conditional expected log-likelihood\nequals a constant (that depends only on the parameters from the \nprevious iteration) plus the log-likelihood of a data set where the\nmissing coordinates of each observation are\nreplaced by their conditional expectation\n(given the observed entries in the same unit). Refer to the \ndiscussion in class for more details. \n\nWe now implement this imputation method in `R`. First add \nthe two incomplete observations to\nthe data set above, we append them at the \"bottom\" of the \nmatrix `x`:\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/create.data_d58bbdf8a989f6a64d13155c787eed63'}\n\n```{.r .cell-code}\nset.seed(123)\ndat <- rbind(x, c(5, NA), c(NA, 5.5))\n```\n:::\n\nNext, we compute initial values for the estimates of the parameters \nof the model. These can be, for example, the sample mean and \nsample covariance matrix using only the fully observed\ndata points:\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/initial0_56cd9e39046b5b25d5fd56bfef314390'}\n\n```{.r .cell-code}\nmu <- colMeans(dat, na.rm = TRUE)\nsi <- var(dat, na.rm = TRUE)\n```\n:::\n\nBefore we start the EM iterations it will be helpful to\nkeep track of wich observations are missing a coordinate\n(we store their indices in the vector `mi`):\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/missing.cases0_c8ad2ff33cd3c0fb5c78deb2d028e905'}\n\n```{.r .cell-code}\nn <- nrow(dat)\np <- 2\n# find observations with a missing coordinate\nmi <- (1:n)[!complete.cases(dat)]\n```\n:::\n\nOut of the n (52) rows in `x`, the ones with some\nmissing coordinates are: 51, 52.\n\nNow we run 100 iterations of the EM algorithm, although convergence\nis achieved much sooner:\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/em0_5f8adcc08c19a7551893c44d42ee4ae1'}\n\n```{.r .cell-code}\n# For this data we don't need many iterations\nniter <- 100\n# how many observations with missing entries:\nlen.mi <- length(mi)\n# Start the EM iterations\nfor (i in 1:niter) {\n  # E step\n  # impute the data points with missing entries\n  for (h in 1:len.mi) {\n    # which entries are not missing?\n    nm <- !is.na(dat[mi[h], ])\n    dat[mi[h], !nm] <- mu[!nm] + si[!nm, nm] * solve(si[nm, nm], dat[mi[h], nm] - mu[nm])\n  }\n  # M step, luckily we have a closed form for the maximizers of the\n  # conditional expected likelihood\n  mu <- colMeans(dat)\n  si <- var(dat)\n}\n```\n:::\n\nThe imputed data are now much more in line with the\nshape and distribution of the other points in the data set:\n\n\n::: {.cell layout-align=\"center\" hash='53-model-based-clustering_cache/html/em-imputed_bf1da13ef783a9152476c987013964e8'}\n\n```{.r .cell-code}\nplot(x, pch = 19, col = \"gray30\", xlim = c(0, 8), ylim = c(0, 13), xlab = \"X1\", ylab = \"X2\", cex = 1.5)\nabline(h = 5.5, lwd = 6, col = \"gray80\")\nabline(v = 5, lwd = 6, col = \"gray80\")\npoints(x, pch = 19, col = \"gray30\", cex = 1.5)\npoints(xbar[1], xbar[2], pch = 19, col = \"tomato\", cex = 2)\nfor (h in 1:length(mi)) points(dat[mi[h], 1], dat[mi[h], 2], pch = 19, col = \"steelblue\", cex = 2)\n```\n\n::: {.cell-output-display}\n![](53-model-based-clustering_files/figure-html/em-imputed-1.png){fig-align='center' width=90%}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}