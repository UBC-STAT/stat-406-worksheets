{
  "hash": "7c77ae5d6ee142708b12704107505f14",
  "result": {
    "markdown": "# Hierarchical clustering\n\n\n\n\n\n\nHierarchical clustering refers to a class of algorithms that work in a \ndifferent way from the ones we have seen so far. Both k-means and model-based\nclustering try to find a pre-specified number of clusters *simultaneously*. \nHierarchical methods are *agglomerative*--they start with *n* clusters \n(one singleton cluster for each observation in the data set), and form a\nhierarchical \nsequence of clusters of sizes *n-1*, *n-2*, ..., *3*, *2*, and a final \"cluster\"\ncomposed of all the observations. The user then needs to decide where to \n*cut* the sequence, in other words, how many clusters to identify. \nAlgorithms in this class are also called\nagglomerative, for obvious reasons. \n\nThe general algorithm can be described as follows:\n\n1. Set *K = n* (the number of observations in the data), and Start with *n* clusters;\n2. While *K > 1*:\n    a. Merge 2 clusters to form *K-1* clusters;\n    b. Set `K = K - 1` (i.e. decrease *K* by one).\n\nThe different versions (*flavours*) of this method are obtained \nby varying the criteria to decide which 2 clusters to merge at each \nrun of step 2(i) above, which depends on how we measure the distance\n(or dissimilarity)  between clusters.\n\nThere are a few different tools to decide how many clusters may be present\nin the data following a hierarchical clustering algorithm. The most \ncommonly used is a graphical representation of the sequence of \nclusters, called a *dendogram*. \n\nPlease refer to your class notes for details on the different\nmerging criteria (i.e. deciding which clusters to combine\nat each step)\nand the interpretation of a dendogram. Below\nwe will illustrate the use of these algorithms on a few\nexamples. \n\n## Breweries example\n\nBeer drinkers were asked to rate 9 breweries on 26\nattributes. The attributes were, e.g., Brewery has rich tradition; or\nBrewery makes very good Pils beer. Relative to each attribute, the\ninformant had to assign each brewery a score on a 6-point scale\nranging from 1=not true at all to 6=very true.\nWe read the data, and use the function `dist` \nto compute the pairwise *L_1* distances between \nthe 9 breweries. Note that the data are available\ncolumnwise ($p \\times x$) so we first transpose\nit before we compute the distances. We also\nchange the misleading column names assigned\nby `read.table`, which are not features but\nrather observation numbers:\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/breweries_5f7a67387715f3152ac1255052a61599'}\n\n```{.r .cell-code}\nx <- read.table(\"data/breweries.dat\", header = FALSE)\ncolnames(x) <- paste0(\"Brew-\", 1:ncol(x))\nx <- t(x)\nd <- dist(x, method = \"manhattan\")\n```\n:::\n\n\nOne implementation of hierarchical clustering methods in `R` is\nin the function `hclust` in package `cluster`. \nWe first use Ward's information criterion (corrected to \nappropriately use squared distances). The  `plot` method for objects\nof class `hclust` produces the associated dendogram. The function `rect.hclust` \ncomputes the height at which one shuld *cut* the dendogram to\nobtain a desired number *k* of clusters. Below we show \nthe result for *K = 3* clusters:\n\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/breweries.2_bc87cfccdbe4d29e14d5de4a891dca55'}\n\n```{.r .cell-code}\n# hierarchical\nlibrary(cluster)\n# show the dendogram\nplot(cl <- hclust(d, method = \"ward.D2\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\n# identify 3 clusters\nrect.hclust(cl, k = 3, border = \"red\")\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/breweries.2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNow we repeat the analysis but using Euclidean distances and *single linkage*, \nand show *K = 3* clusters:\n\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/breweries2_eb18aec7f8cbeaca5cc73ce91d54bdfb'}\n\n```{.r .cell-code}\nbr.dis <- dist(x) # L2\nbr.hc <- hclust(br.dis, method = \"single\")\nplot(br.hc)\nbr.hc.3 <- rect.hclust(br.hc, k = 3)\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/breweries2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote how these 3 clusters are somewhat different from the \nones found before. However, the *(V1, V4, V7)* cluster\nis present in both partitions, and also the triplet \n*(V3, V6, V8)* stays together as well. \nIt is interesting to compare these clusters with those\nfound by K-means (see previous notes), in particular,\nthese dendograms resemble \nthe information on the silhouette plots to some extent. \n\n\n## Languages example\n\nThe details of this example were discussed in class. Here we \npresent the results of three commonly used merging \ncriteria:\n*single* linkage, *complete* linkage, *average* linkage, and *Ward's* \ncriterion. As usual, we start by reading \nthe data, which in this case \nare the specific dissimilarities between languages\ndiscussed in class, and we arrange them in a\nmatrix that can be used by `hclust`:\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/languages_d2eeb3ec77f1ba67749d6d2bbfa744dd'}\n\n```{.r .cell-code}\ndd <- read.table(\"data/languages.dat\", header = FALSE)\nnames(dd) <- c(\"E\", \"N\", \"Da\", \"Du\", \"G\", \"Fr\", \"S\", \"I\", \"P\", \"H\", \"Fi\")\ndd <- (dd + t(dd) / 2)\nd <- as.dist(dd)\n```\n:::\n\nNow we compute a hierarchical clustering sequence using **single linkage**,\nplot the corresponding dendogram and identify 4 clusters:\n\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/langs2_d1d64eae6e0b4e0455065da595a12c33'}\n\n```{.r .cell-code}\nplot(cl <- hclust(d, method = \"single\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\nrect.hclust(cl, k = 4, border = \"red\")\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/langs2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nCompare the above with the results obtained with **complete linkage**:\n\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/langs3_29da4d1968d1bedf3d0c1c066964c3bd'}\n\n```{.r .cell-code}\nplot(cl <- hclust(d, method = \"complete\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\nrect.hclust(cl, k = 4, border = \"red\")\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/langs3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWith **average linkage** we obtain:\n\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/langs3.1_06dd86fe8265f63493fb709f4afb1634'}\n\n```{.r .cell-code}\nplot(cl <- hclust(d, method = \"average\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\nrect.hclust(cl, k = 4, border = \"red\")\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/langs3.1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\nAnd finally, using **Ward's criterion** results in the following\ndendogram and 4 clusters:\n\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/langs4_74d61fb852d545afba7fb1044236f57b'}\n\n```{.r .cell-code}\nplot(cl <- hclust(d, method = \"ward.D2\"), main = \"\", xlab = \"\", sub = \"\", hang = -1)\nrect.hclust(cl, k = 4, border = \"red\")\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/langs4-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n## Cancer example\n\nHere we revisit the Cancer example discussed before. We use Euclidean\ndistances and Ward's information criterion. Below we show the \nclusters identified when we stop the algorithm at *K = 8*, which\nbased on the dendogram seems to be a reasonable choice:\n\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/cancer_94a6ff3c2e1f9e60c77983632c15cf88'}\n\n```{.r .cell-code}\ndata(nci, package = \"ElemStatLearn\")\nnci.dis <- dist(t(nci), method = \"euclidean\")\nplot(nci.hc.w <- hclust(nci.dis, method = \"ward.D2\"),\n  main = \"\",\n  xlab = \"\", sub = \"\", hang = -1, labels = rownames(nci)\n)\nrect.hclust(nci.hc.w, k = 8, border = \"red\")\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/cancer-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nFor completeness, below we show the results obtained with the other linkage criteria, \nincluding Ward's:\n\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/cancer2_1f16bbcb2bb8b3659fd85332b21d5532'}\n\n```{.r .cell-code}\nnci.hc.s <- hclust(nci.dis, method = \"single\")\nnci.hc.c <- hclust(nci.dis, method = \"complete\")\nnci.hc.a <- hclust(nci.dis, method = \"average\")\n\n# plot them\nplot(nci.hc.s, labels = colnames(nci), cex = .5)\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/cancer2-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\n\nplot(nci.hc.c, labels = colnames(nci), cex = .5)\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/cancer2-2.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\n\nplot(nci.hc.a, labels = colnames(nci), cex = .5)\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/cancer2-3.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote that with these 3 other criteria no clear structure\nseems apparent in the data. \n\n\n\n\n\n## Nations example\n\nThis is a smaller Political Science dataset. Twelve countries\nwere assessed on their perceived \"likeness\" by Political Science \nstudents. Note that (as in the Languages example\nabove) in this example we do not have raw observations (features),\nwe only have access to the already determined parwise dissimilarities. \nBelow we show the results of using hierarchical clustering with complete and average \nlinkage merging criteria, which produce identical clusters. You\nare encouraged to investigate what can be found with other \nmerging criteria. \n\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/nations_b8be04689b81d2b09304bca0dda76af2'}\n\n```{.r .cell-code}\n# read the pairwise dissimilarities\na2 <- read.table(\"data/nations2.dat\", header = FALSE)\n\n# since only the lower triangular matrix is available\n# we need to copy it on the upper half\na2 <- a2 + t(a2)\n\n# create a vector of country names, to be used later\nnams2 <- c(\n  \"BEL\", \"BRA\", \"CHI\", \"CUB\", \"EGY\", \"FRA\",\n  \"IND\", \"ISR\", \"USA\", \"USS\", \"YUG\", \"ZAI\"\n)\n\n# compute hierarchical clustering using complete linkage\nna.hc <- hclust(as.dist(a2), method = \"complete\")\nplot(na.hc, labels = nams2)\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/nations-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\n\n# compute hierarchical clustering using average linkage\nna.hc <- hclust(as.dist(a2), method = \"average\")\nplot(na.hc, labels = nams2)\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/nations-2.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\n## UN Votes\n\nWe revisit here the UN votes example (see Lecture 19). Using Euclidean\ndistances and Ward's criterion we obtain the following 3 clusters:\n\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/unvotes_74c356002324504502bfda0a5995b900'}\n\n```{.r .cell-code}\nX <- read.table(file = \"data/unvotes.csv\", sep = \",\", row.names = 1, header = TRUE)\nun.dis <- dist(t(X), method = \"euclidean\")\nun.hc <- hclust(un.dis, method = \"ward.D2\")\nplot(un.hc, cex = .5)\nun.hc.3 <- rect.hclust(un.hc, k = 3)\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/unvotes-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nlapply(un.hc.3, names)\n#> [[1]]\n#>  [1] \"Australia\"   \"Austria\"     \"Belgium\"     \"Bulgaria\"    \"Canada\"     \n#>  [6] \"Denmark\"     \"Finland\"     \"France\"      \"Greece\"      \"Hungary\"    \n#> [11] \"Iceland\"     \"Ireland\"     \"Israel\"      \"Italy\"       \"Japan\"      \n#> [16] \"Luxembourg\"  \"Netherlands\" \"New.Zealand\" \"Norway\"      \"Poland\"     \n#> [21] \"Portugal\"    \"Spain\"       \"Sweden\"      \"UK\"          \"Ukraine\"    \n#> [26] \"USA\"        \n#> \n#> [[2]]\n#>  [1] \"Argentina\"  \"Bahamas\"    \"Chile\"      \"Colombia\"   \"Costa.Rica\"\n#>  [6] \"Cyprus\"     \"Malta\"      \"Mexico\"     \"Panama\"     \"Paraguay\"  \n#> [11] \"Peru\"       \"Uruguay\"   \n#> \n#> [[3]]\n#>  [1] \"Algeria\"              \"Bangladesh\"           \"Belarus\"             \n#>  [4] \"Bolivia\"              \"Botswana\"             \"Brazil\"              \n#>  [7] \"Brunei.Darussalam\"    \"Burkina.Faso\"         \"China\"               \n#> [10] \"Cuba\"                 \"Ecuador\"              \"Egypt\"               \n#> [13] \"Ethiopia\"             \"Ghana\"                \"Guyana\"              \n#> [16] \"India\"                \"Indonesia\"            \"Jamaica\"             \n#> [19] \"Jordan\"               \"Kenya\"                \"Kuwait\"              \n#> [22] \"Libya\"                \"Malaysia\"             \"Mali\"                \n#> [25] \"Nepal\"                \"Nigeria\"              \"Pakistan\"            \n#> [28] \"Philippines\"          \"Russian.Federation\"   \"Singapore\"           \n#> [31] \"Sri.Lanka\"            \"Sudan\"                \"Syrian.Arab.Republic\"\n#> [34] \"Tanzania\"             \"Thailand\"             \"Togo\"                \n#> [37] \"Trinidad.and.Tobago\"  \"Venezuela\"            \"Zambia\"\n```\n:::\n\n\nIf we repeat the same exercise but using $L_1$ distances we obtain \ndifferent clusters. \n\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/unvotes.l1_2174552490902020a3eb6db8b1d13d77'}\n\n```{.r .cell-code}\nun.dis.l1 <- dist(t(X), method = \"manhattan\")\nun.hc.l1 <- hclust(un.dis.l1, method = \"ward.D2\")\nplot(un.hc.l1, cex = .5)\nun.hc.l1.3 <- rect.hclust(un.hc.l1, k = 3)\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/unvotes.l1-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nlapply(un.hc.l1.3, names)\n#> [[1]]\n#>  [1] \"Australia\"   \"Austria\"     \"Belgium\"     \"Bulgaria\"    \"Canada\"     \n#>  [6] \"Cyprus\"      \"Denmark\"     \"Finland\"     \"France\"      \"Greece\"     \n#> [11] \"Hungary\"     \"Iceland\"     \"Ireland\"     \"Israel\"      \"Italy\"      \n#> [16] \"Japan\"       \"Luxembourg\"  \"Malta\"       \"Netherlands\" \"New.Zealand\"\n#> [21] \"Norway\"      \"Poland\"      \"Portugal\"    \"Spain\"       \"Sweden\"     \n#> [26] \"UK\"          \"Ukraine\"     \"USA\"        \n#> \n#> [[2]]\n#>  [1] \"Algeria\"              \"Bangladesh\"           \"Belarus\"             \n#>  [4] \"Brunei.Darussalam\"    \"China\"                \"Cuba\"                \n#>  [7] \"Egypt\"                \"India\"                \"Indonesia\"           \n#> [10] \"Jordan\"               \"Kuwait\"               \"Libya\"               \n#> [13] \"Malaysia\"             \"Pakistan\"             \"Russian.Federation\"  \n#> [16] \"Sri.Lanka\"            \"Sudan\"                \"Syrian.Arab.Republic\"\n#> [19] \"Venezuela\"           \n#> \n#> [[3]]\n#>  [1] \"Argentina\"           \"Bahamas\"             \"Bolivia\"            \n#>  [4] \"Botswana\"            \"Brazil\"              \"Burkina.Faso\"       \n#>  [7] \"Chile\"               \"Colombia\"            \"Costa.Rica\"         \n#> [10] \"Ecuador\"             \"Ethiopia\"            \"Ghana\"              \n#> [13] \"Guyana\"              \"Jamaica\"             \"Kenya\"              \n#> [16] \"Mali\"                \"Mexico\"              \"Nepal\"              \n#> [19] \"Nigeria\"             \"Panama\"              \"Paraguay\"           \n#> [22] \"Peru\"                \"Philippines\"         \"Singapore\"          \n#> [25] \"Tanzania\"            \"Thailand\"            \"Togo\"               \n#> [28] \"Trinidad.and.Tobago\" \"Uruguay\"             \"Zambia\"\n```\n:::\n\n\nIt is easier to compare these 2 sets of clusters\nif we show them on a map. We first find the \ncluster labels corresponding to 3 clusters \nusing Euclidean and $L_1$ distances:\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/unvotes.labs_d10e16c120557779f3d99ac511587f41'}\n\n```{.r .cell-code}\nlabs <- cutree(un.hc, k = 3)\nlabs.l1 <- cutree(un.hc.l1, k = 3)\n```\n:::\n\nWe can now use these labels to color a map, as\nwe did previously. For the Euclidean distances\nwe obtain:\n\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/unvotes.maps.1_4bbbcf2efef8a6640a41be3cd20d27f7'}\n\n```{.r .cell-code}\nlibrary(rworldmap)\nlibrary(countrycode)\nthese <- countrycode(colnames(X), \"country.name\", \"iso3c\")\nmalDF <- data.frame(country = these, cluster = labs)\nmalMap <- joinCountryData2Map(malDF, joinCode = \"ISO3\", nameJoinColumn = \"country\")\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap,\n  nameColumnToPlot = \"cluster\", catMethod = \"categorical\",\n  missingCountryCol = \"white\", addLegend = FALSE, mapTitle = \"\",\n  colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\", \"blueviolet\", \"yellow\"),\n  oceanCol = \"dodgerblue\"\n)\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/unvotes.maps.1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWhile with the $L_1$ distances we get:\n\n\n\n\n::: {.cell layout-align=\"center\" hash='54-hclust_cache/html/unvotes.maps.2.2_51c1ec50507d280cdf99f71f5875e654'}\n\n```{.r .cell-code}\npar(mai = c(0, 0, 0, 0), xaxs = \"i\", yaxs = \"i\")\nmapCountryData(malMap,\n  nameColumnToPlot = \"cluster\", catMethod = \"categorical\",\n  missingCountryCol = \"white\", addLegend = FALSE, mapTitle = \"\",\n  colourPalette = c(\"darkgreen\", \"hotpink\", \"tomato\", \"blueviolet\", \"yellow\"),\n  oceanCol = \"dodgerblue\"\n)\n```\n\n::: {.cell-output-display}\n![](54-hclust_files/figure-html/unvotes.maps.2.2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nRecall that, as discussed in class, the analyses above may be \nquestionable, because these distance measures do not take into\naccount the actual nature of the available features. \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}