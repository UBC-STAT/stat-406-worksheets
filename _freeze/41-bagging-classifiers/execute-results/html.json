{
  "hash": "9e87a1ea45dc66042a97d1a2458bab3c",
  "result": {
    "markdown": "# Bagging for classification\n\n\n\n\n\n\n\n## Instability of trees (motivation)\n\nJust like in the regression case, classification \ntrees can be highly unstable (specifically: relatively small \nchanges in the training set may result in \ncomparably large changes in the corresponding tree). \nWe illustrate the problem on the very simple graduate\nschool admissions example \n(3-class 2-dimensional covariates) we used in class. First\nread the data:\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/inst1_2748be6c91963ee1f736a8a975f0b00e'}\n\n```{.r .cell-code}\nmm <- read.table(\"data/T11-6.DAT\", header = FALSE)\n```\n:::\n\nWe transform the response variable `V3` into a factor (which is how class labels\nare represented in `R`, and what `rpart()` expects as the values in the response \nvariable to build a classifier): \n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/inst11_df8932dee5da54447ecc77eddd6a3e03'}\n\n```{.r .cell-code}\nmm$V3 <- as.factor(mm$V3)\n```\n:::\n\nTo obtain better looking plots later, we now re-scale one of the features\n(so both explanatory variables have similar ranges):\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/inst12_6e8b0af3b608a6f217771df21d209321'}\n\n```{.r .cell-code}\nmm[, 2] <- mm[, 2] / 150\n```\n:::\n\nWe use the function `rpart` to train a classification tree on these data, using \ndeviance-based (`information`) splits:\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/inst13_a7854dd4d4413218fc97791710a6429e'}\n\n```{.r .cell-code}\nlibrary(rpart)\na.t <- rpart(V3 ~ V1 + V2, data = mm, method = \"class\", parms = list(split = \"information\"))\n```\n:::\n\nTo illustrate the instability of this tree (i.e. how the tree changes when the data are perturbed\nslightly), we create a new training set (`mm2`) that is identical to \nthe original one (in `mm`), except for two observations where \nwe change their responses from class `1` to class `2`:\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/inst20_49e00a969278d1bf15b6f9c4071a6cd7'}\n\n```{.r .cell-code}\nmm2 <- mm\nmm2[1, 3] <- 2\nmm2[7, 3] <- 2\n```\n:::\n\nThe following plot contains the new training set, with the two changed\nobservations (you can find them around the point `(GPA, GMAT) = (3, 4)`) \nhighlighted with a blue dot (their new class) and a red ring around them \n(their old class was \"red\"):\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/inst2_8230a35d56d43a612a9e74f1e9d37a90'}\n\n```{.r .cell-code}\nplot(mm2[, 1:2],\n  pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]],\n  xlab = \"GPA\", \"GMAT\", xlim = c(2, 5), ylim = c(2, 5)\n)\npoints(mm[c(1, 7), -3], pch = \"O\", cex = 1.1, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n```\n\n::: {.cell-output-display}\n![](41-bagging-classifiers_files/figure-html/inst2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nAs we did above, we now train a classification tree on the perturbed data in `mm2`:\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/inst2.5_827a2e1a48947356af810d6ba7547ec1'}\n\n```{.r .cell-code}\na2.t <- rpart(V3 ~ V1 + V2, data = mm2, method = \"class\", parms = list(split = \"information\"))\n```\n:::\n\nTo visualize the differences between the two trees we build a fine grid of points\nand compare the predicted probabilities of each class on each point on the grid. \nFirst, construct the grid:\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/inst2.0_b0566c2b526343cb9f0b465bd6ec266d'}\n\n```{.r .cell-code}\naa <- seq(2, 5, length = 200)\nbb <- seq(2, 5, length = 200)\ndd <- expand.grid(aa, bb)\nnames(dd) <- names(mm)[1:2]\n```\n:::\n\nNow, compute the estimated conditional probabilities of each of the 3 classes\non each of the 40,000 points on the grid `dd`:\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/inst2.1_2ac73c92c5e97fa3aa00d938651a836d'}\n\n```{.r .cell-code}\np.t <- predict(a.t, newdata = dd, type = \"prob\")\np2.t <- predict(a2.t, newdata = dd, type = \"prob\")\n```\n:::\n\nThe next figures show the estimated probabilities of class \"red\" with each of the two\ntrees:\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/inst2.2_4a29374cdf53391c363fbc148791675b'}\n\n```{.r .cell-code}\nfilled.contour(aa, bb, matrix(p.t[, 1], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](41-bagging-classifiers_files/figure-html/inst2.2-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\n\nfilled.contour(aa, bb, matrix(p2.t[, 1], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm2[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]])\n    points(mm[c(1, 7), -3], pch = \"O\", cex = 1.1, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](41-bagging-classifiers_files/figure-html/inst2.2-2.png){fig-align='center' width=90%}\n:::\n:::\n\n\nSimilarly, the estimated conditional probabilities for class \"blue\" at each point of the grid are:\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/kk_680d74d278d5c14e983145e2fbaceff3'}\n\n```{.r .cell-code}\n# blues\nfilled.contour(aa, bb, matrix(p.t[, 2], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](41-bagging-classifiers_files/figure-html/kk-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\n\nfilled.contour(aa, bb, matrix(p2.t[, 2], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  pane.last = {\n    points(mm2[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]])\n    points(mm[c(1, 7), -3], pch = \"O\", cex = 1.1, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](41-bagging-classifiers_files/figure-html/kk-2.png){fig-align='center' width=90%}\n:::\n:::\n\n\nAnd finally, for class \"green\":\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/kk2_6fa5065da93c8879f67b296b06cfb51b'}\n\n```{.r .cell-code}\n# greens\nfilled.contour(aa, bb, matrix(p.t[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  }, panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](41-bagging-classifiers_files/figure-html/kk2-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\n\nfilled.contour(aa, bb, matrix(p2.t[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  pane.last = {\n    points(mm2[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]])\n    points(mm[c(1, 7), -3], pch = \"O\", cex = 1.1, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](41-bagging-classifiers_files/figure-html/kk2-2.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote that, for example, the regions of the feature space (the \nexplanatory variables) that would be classified as \"red\" or \"green\" for the\ntrees trained with the original and the slightly changed training sets \nchange quite noticeably, even though the difference in the training sets is\nrelatively small. Below we show how an ensemble of classifiers \nconstructed via bagging can provide a more stable classifier.\n\n## Bagging trees\n\nJust as we did for regression, bagging consists of building an ensemble of\npredictors (in this case, classifiers) using bootstrap samples. \nIf we using *B* bootstrap samples, we will construct *B* classifiers, and\ngiven a point **x**, we now have *B* estimated conditional probabilities \nfor each of the possible *K* classes. \nUnlike what happens with regression problems, we now have a choice to make\nwhen deciding how to combine the *B* outputs for each point. We can take\neither: a majority vote over the *B* separate decisions, or we can \naverage the *B* estimated probabilities for the *K* classes, to obtain\nbagged estimated conditional probabilities. As discussed and illustrated\nin class, the latter approach is usually preferred. \n\nTo illustrate the increased stability of bagged classification trees, \nwe repeat the experiment above: we build an ensemble of 1000 classification\ntrees trained on the original data, and a second ensemble (also of 1000\ntrees) using the slightly modified data. \nEach ensemble is constructed in exactly the same way we did\nin the regression case. \nFor the first ensemble we train `NB = 1000` trees\nand store them in a list (called `ts`) for future use:\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/bag0_835c5ffc2a572b69db4d98b856751bb4'}\n\n```{.r .cell-code}\nmy.c <- rpart.control(minsplit = 3, cp = 1e-6, xval = 10)\nNB <- 1000\nts <- vector(\"list\", NB)\nset.seed(123)\nn <- nrow(mm)\nfor (j in 1:NB) {\n  ii <- sample(1:n, replace = TRUE)\n  ts[[j]] <- rpart(V3 ~ V1 + V2, data = mm[ii, ], method = \"class\", parms = list(split = \"information\"), control = my.c)\n}\n```\n:::\n\n\n### Using the ensemble\n\nAs discussed in class, there are two possible ways to use this ensemble given\na new observation: we can classify it to the class with most votes among the \n*B* bagged classifiers, or we can compute the average conditional probabilities\nover the *B* classifiers, and use this average as our esimated conditional \nprobability. We illustrate both of these with the point `(GPA, GMAT) = (3.3, 3.0)`.\n\n#### Majority vote\n\nThe simplest, but less elegant way to compute the votes for each class \nacross the *B* trees in the ensemble is to loop over them and \ncount:\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/predbag0_c46427c5f028761daf367626885331ad'}\n\n```{.r .cell-code}\nx0 <- t(c(V1 = 3.3, V2 = 3.0))\nvotes <- vector(\"numeric\", 3)\nnames(votes) <- 1:3\nfor (j in 1:NB) {\n  k <- predict(ts[[j]], newdata = data.frame(x0), type = \"class\")\n  votes[k] <- votes[k] + 1\n}\n(votes)\n#>   1   2   3 \n#> 909   0  91\n```\n:::\n\nAnd we see that the class most voted is 1. \n\nThe above calculation can be made more elegantly with the function `sapply`\n(or `lapply`):\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/predbag1_2bb225ea1df5f53db470c56fd2459eca'}\n\n```{.r .cell-code}\nvotes2 <- sapply(ts, FUN = function(a, newx) predict(a, newdata = newx, type = \"class\"), newx = data.frame(x0))\ntable(votes2)\n#> votes2\n#>   1   2   3 \n#> 909   0  91\n```\n:::\n\n#### Average probabilities (over the ensemble)\n\nIf we wanted to compute the average of the conditional probabilities across\nthe *B* different estimates, we could do it in a very similar way. Here I show how \nto do it using `sapply`. You are strongly encouraged to verify these calculations\nby computing the average of the conditional probabilities using a for-loop. \n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/predbag2_60ec2ec2f6b501afbfe1b621311881c1'}\n\n```{.r .cell-code}\nvotes2 <- sapply(ts, FUN = function(a, newx) predict(a, newdata = newx, type = \"prob\"), newx = data.frame(x0))\n(rowMeans(votes2))\n#> [1] 0.90881555 0.00000000 0.09118445\n```\n:::\n\nAnd again, we see that class `1` has a much higher probability of occuring \nfor this point. \n\n### Increased stability of ensembles\n\nTo illustrate that ensembles of tree-based classifiers tend to be more stable\nthan a single tree, we construct another example, but this time using\nthe slightly modified data. The ensemble is stored in the list `ts2`:\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/bag02_2dcb5067eaa9ff0566e25d6dd86c4c94'}\n\n```{.r .cell-code}\nmm2 <- mm\nmm2[1, 3] <- 2\nmm2[7, 3] <- 2\nNB <- 1000\nts2 <- vector(\"list\", NB)\nset.seed(123)\nn <- nrow(mm)\nfor (j in 1:NB) {\n  ii <- sample(1:n, replace = TRUE)\n  ts2[[j]] <- rpart(V3 ~ V1 + V2, data = mm2[ii, ], method = \"class\", parms = list(split = \"information\"), control = my.c)\n}\n```\n:::\n\nWe use the same fine grid as before to show the \nestimated conditional probabilities, this time\nobtained with the two ensembles.\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/grid0_6db947eb8ef03c9d7f7013cb34f13873'}\n\n```{.r .cell-code}\naa <- seq(2, 5, length = 200)\nbb <- seq(2, 5, length = 200)\ndd <- expand.grid(aa, bb)\nnames(dd) <- names(mm)[1:2]\n```\n:::\n\nTo combine (average) the `NB = 1000` estimated probabilities \nof each of the 3 classes for each of the 40,000 points \nin the grid `dd` I use the function `vapply`\nand store the result in a 3-dimensional array. The\naveraged probabilities over the 1000 bagged trees\ncan then obtained by averaging across the 3rd dimension. \nThis approach may not be intuitively very clear at \nfirst sight. *You are strongly encouraged to ignore my code\nbelow and compute the bagged conditional probabilites for the\n3 classes for each point in the grid in a way that is \nclear to you*. The main goal is to understand the method\nand be able to do it on your own. Efficient and / or elegant code\ncan be written later, but it is not the focus of this course. \nThe ensemble of trees trained with the original data:\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/bag1_4858d522489a0461892a144293afca47'}\n\n```{.r .cell-code}\npp0 <- vapply(ts, FUN = predict, FUN.VALUE = matrix(0, 200 * 200, 3), newdata = dd, type = \"prob\")\npp <- apply(pp0, c(1, 2), mean)\n```\n:::\n\nAnd the ensemble of trees trained with the slightly modified data:\n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/bag010_2ad5078abb339b3ad16fcb9debee0917'}\n\n```{.r .cell-code}\npp02 <- vapply(ts2, FUN = predict, FUN.VALUE = matrix(0, 200 * 200, 3), newdata = dd, type = \"prob\")\npp2 <- apply(pp02, c(1, 2), mean)\n```\n:::\n\nThe plots below show the estimated conditional probabilities for class \"red\"\nin each point of the grid, with each of the two ensembles. Note \nhow similar they are (and contrast this with the results obtained before\nwithout bagging): \n\n::: {.cell layout-align=\"center\" hash='41-bagging-classifiers_cache/html/bag1.plot_a6bd9af4cf5e0f7167042bd66bb097e4'}\n\n```{.r .cell-code}\nfilled.contour(aa, bb, matrix(pp[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](41-bagging-classifiers_files/figure-html/bag1.plot-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nfilled.contour(aa, bb, matrix(pp2[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm2[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm2[, 3]])\n    points(mm[c(1, 7), -3], pch = \"O\", cex = 1.2, col = c(\"red\", \"blue\", \"green\")[mm[c(1, 7), 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](41-bagging-classifiers_files/figure-html/bag1.plot-2.png){fig-align='center' width=90%}\n:::\n:::\n\n\nYou are strongly encouraged to obtain the corresponding plots comparing\nthe estimated conditional probabilities with both ensembles\nfor each of the other 2 classes (\"blue\" and \"green\"). \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}