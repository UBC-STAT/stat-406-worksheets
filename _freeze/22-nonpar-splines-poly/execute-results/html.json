{
  "hash": "5c7f13ca3b726ad15aa720ce2493358e",
  "result": {
    "markdown": "\n# Non-parametric regression\n\n\n\n\n\n\nWe now turn our attention to the situation where the \nregression function E(Y|X) is not necessarily linear.\nFurthermore, we will assume that its *form* is\n**unknown**. If we knew that the regression\nfunction was a linear combination\nof a sine and a cosine function, \"E(Y|X=x) = a + b sin(x)\n+ c cos(x)\", where *a*, *b* and *c* are uknown, \nfor example, then the problem would in fact be a linear\nregression problem. More in general, \nwhen the true regression function is known (or\nassumed) to belong to a family of functions that we can parametrize, then\nthe estimation can be done via standard least squares.\nInstead here we focus on the case where the\nregression function is **completely unknown**.\n\nIn this note and the next one will discuss two ways to estimating\n$E(Y|X)$: \n\n(a) one using bases (e.g. a polynomial basis, or a spline basis); and \n(b) one using kernels (aka local regression).\n\nTo simplify the presentation (but also because of\nan intrinsic limitation of these methods, which will\nbe discussed in more detail later in the course), we will initially only consider\nthe case where there is a single explanatory variable\n(i.e. X above is a scalar, not a vector). \n\n## Polynomial regression\n\nTo illustrate these basis methods, we will consider the \n`lidar` data, available in the package `SemiPar`. More\ninformation is available from the\ncorresponding help page: `help(lidar, package='SemiPar')`. \nWe now load the data and plot it, the response\nvariable is `logratio` and the explanatory one is\n`range`:\n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/nonparam_c87b5aed0a46c6bb5645885709a0cc71'}\n\n```{.r .cell-code}\ndata(lidar, package = \"SemiPar\")\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/nonparam-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nIn class we discussed the formal motivation to look into\na polynomial approximation of the regression function. \nThis argument, however, does not specify which degree\nof the approximating polynomial to use. Here we \nfirst try a 4th degree polynomial and the problem reduces to a linear\nregression one (see the lecture slides). We can use a \ncommand like `lm(logratio ~ range + range^2 + range^3 + range^4)`.\nHowever, this call to `lm`  will not work as we intend it \n(I recommend that you check this and find out the reason why). \nInstead, we would need to use something like\n`lm(logratio ~ range + I(range^2) + I(range^3) + I(range^4))`. \nTo avoid having to type a long formula, we can instead use\nthe function `poly()` in `R` to generate the design matrix \ncontaining the desired powers\nof `range`, and plug that into the call to `lm()`. \nThe code below fits two such approximations (a 3rd degree and a \n4th degree polynomial), plots the data\nand overlays the estimated regression function:\n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/poly4_e659530902ef4122134d025f6ab6482c'}\n\n```{.r .cell-code}\n# Degree 4 polynomials\npm <- lm(logratio ~ poly(range, 4), data = lidar)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(pm)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"blue\")\npm3 <- lm(logratio ~ poly(range, 3), data = lidar)\nlines(predict(pm3)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"hotpink\")\nlegend(\"topright\", legend = c(\"3rd degree\", \"4th degree\"), lwd = 6, col = c(\"hotpink\", \"blue\"))\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/poly4-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote that this fit is reasonable, although there is probably \nroom for improvement. Based on the \nformal motivation discussed in class to use polynomials in the first \nplace, it may seem natural to increase the order\nof the approximating polynomial in order to improve the quality \nof the approximation. However, this is\neasily seen not to be a good idea. Below we compare the \n4th degree approximation used above (in blue) with\na 10th degree one (in red): \n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/poly10_df725a2635ad31d1b4b4f9e13c27a060'}\n\n```{.r .cell-code}\n# Degree 10 polynomials\npm2 <- lm(logratio ~ poly(range, 10), data = lidar)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(pm)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"blue\")\nlines(predict(pm2)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/poly10-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote that the 10th order fit follows the data much more closely, but\nit starts to become \"too adaptive\" and departing quite often from the\nmain (larger scale) trend we associate with the regression (conditional \nmean) function. \n\n(Can you explain the discrepancy between what we observe above and\nthe motivation we used in class, that suggests that higher order \npolynomials provide better approximations?)\n\n## A more stable basis: splines\n\nPart of the problem with global polynomial bases as the ones used \nabove is that they necessarily become more wiggly within the range of the data, and also quickly \nincrease or decrease near the edge of the observations. A more stable\nbut also remarkably flexible basis is given by spline functions, \nas discussed in class. \n\nWe first here show how to build a naive linear spline basis with \n5 knots (placed at the `(1:5)/6` quantiles \n(i.e. the 0.17, 0.33, 0.5, 0.67, 0.83 percentiles) of the\nobserved values of the explanatory variable), and use\nit to estimate the regression function. \nRemember that a linear spline function with knot *w* is\ngiven by `f_w(x) = max( x - w, 0 )`. Given a fixed \nset of pre-selected knots *w_1*, *w_2*, ..., *w_k*, \nwe consider regression functions that are linear combinations \nof the corresponding k linear spline functions. \n\nNote that for higher-order splines (e.g. cubic splines\ndiscussed below), the naive spline basis used above is numerically \nvery unstable, and usually works poorly in practice. \nI include it here simply as an illustration of the\nmethodology and to stress the point that this type of approach \n(that estimates the regression function as a linear combination\nof an explicit basis) is in fact nothing\nmore than slightly more complex linear models. \n\nFirst we find the 5 knots mentioned above that will be used to \nconstruct the spline basis: \n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/knots_060e21ebe547de2e69e310484710fca7'}\n\n```{.r .cell-code}\n# select the knots at 5 specific quantiles\n(kn <- as.numeric(quantile(lidar$range, (1:5) / 6)))\n#> [1] 444.6667 499.6667 555.0000 609.6667 664.6667\n```\n:::\n\nNow we compute the matrix of \"explanatory variables\", that\nis: the matrix that in its columns has each of the 5 basis \nfunctions *f_1*, *f_2*, ..., *f_5* evaluated \nat the n observed values of the (single) explanatory variable *x_1*, ..., *x_n*. In other words, the\nmatrix **X** has in its *(i, j)* cell the value *f_j(x_i)*, \nfor *j=1*, ..., *k*, and *i=1*, ..., *n*. In the code below we use (abuse?) `R`'s *recycling* rules \nwhen operating over vectors and arrays (can you spot it?)\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/linearbasis_be0967ed89fa4f0d61a54a20bc3bd147'}\n\n```{.r .cell-code}\n# prepare the matrix of covariates / explanatory variables\nx <- matrix(0, dim(lidar)[1], length(kn) + 1)\nfor (j in 1:length(kn)) {\n    x[, j] <- pmax(lidar$range - kn[j], 0)\n}\nx[, length(kn) + 1] <- lidar$range\n```\n:::\n\nNow that we have the matrix of our \"explanatory variables\", we can simply use `lm` to estimate \nthe coefficients of the linear combination of the functions in the spline basis \nthat will provide our regression function estimator. We then plot the data and overlay the \nfitted / estimated regression function: \n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/splines1_c75fa5e6948bfa6d504375273d99d9af'}\n\n```{.r .cell-code}\n# Fit the regression model\nppm <- lm(lidar$logratio ~ x)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(ppm)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"hotpink\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/splines1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThere are better (numerically more stable) bases for the same linear \nspace spanned by these spline functions. These bases have different \nnumerical properties and can become cumbersome to describe. Here we\nuse the function `bs` (in package `splines`) to build a B-spline basis. \nFor an accessible discussion, see for example [@wood2017generalized, Section 4.1].\n\nGiven the chosen knots and the degree of the splines\n(linear, quadratic, cubic, etc.) the set (linear space) \nof functions we are using to construct our regression estimate\ndoes not depend on the specific basis we use\n(in other words: these are different bases that span\nthe same linear space of functions). As a consequence,\nthe estimated regression function should be the same regardless of the basis we use \n(provided we do not run into serious numerical issues\nwith our naive basis). To illustrate this fact, \nwe will use a B-spline basis with the same 5 knots as above, \nand compare the estimated regression \nfunction with the one we obtained above \nusing our **poor people naive basis**. The plot below overlays both\nfits (the naive one with a thick pink line as above, and the one\nusing b-splines with a thinner blue line):\n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/bsplines1_f12fb21f8d3410a93a667d72406c9efc'}\n\n```{.r .cell-code}\nlibrary(splines)\nppm2 <- lm(logratio ~ bs(range, degree = 1, knots = kn), data = lidar)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(ppm)[order(range)] ~ sort(range), data = lidar, lwd = 8, col = \"hotpink\")\nlines(predict(ppm2)[order(range)] ~ sort(range), data = lidar, lwd = 3, col = \"darkblue\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/bsplines1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nAs expected, both fits provide the same estimated regression function, although\nits coefficients are naturally different (**but their lengths are the same**, \nis this a coincidence, or will it always happen?)\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/bsplines.coef_248808bdf4961849d9b47d843a253a2a'}\n\n```{.r .cell-code}\nas.vector(coef(ppm))\n#> [1]  0.0269095640  0.0002488169 -0.0003235802 -0.0082773735  0.0063779378\n#> [6]  0.0007385513 -0.0001847752\nas.vector(coef(ppm2))\n#> [1] -0.04515276 -0.01010104 -0.00657875 -0.02093988 -0.48762440 -0.60636798\n#> [7] -0.68496471\n```\n:::\n\nNote that, because we are \nusing a set of linear splines, our estimated regression functions will always be piecewise \nlinear (i.e. linear functions between each pair of knots). To obtain smoother (e.g. differentiable, \ncontinuously differentiable, or even twice continously differentiable) \nregression estimators below we will use higher-order splines. \n\n## Higher order splines (quadratic, cubic, etc.)\n\nIn what follows (as above) we will use the function `bs` to evaluate the desired\nspline basis on the observed values of the explanatory variable (in this case `range`).\n\nWe use the arguments `degree = 2` and `knots = kn` to indicate we want a \nquadratic spline basis with knots located at the elements of the vector `kn`. \nAs before, we then simply use `lm` \nto estimate the coefficients, and overlay the estimated regression \nfunction over the data: \n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/bsplines2_f42abd5c70e04924660d9588f4853fe2'}\n\n```{.r .cell-code}\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nppmq <- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nlines(predict(ppmq)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"steelblue\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/bsplines2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nA useful consequence of the fact that these regression estimators are in fact \njust linear regression estimators\n(but using a richer / more flexible basis than just the straight\npredictors) is that we can easily compute (pointwise) standard errors\nfor the fitted regression curve, as follows. We first fit and \nplot a quadratic \nspline using the same 5 knots as before:\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/bsplines.se1_3383d1e3ac5ec4ef8d926784e75014ff'}\n\n```{.r .cell-code}\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nppmc <- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"gray30\")\n```\n:::\n\nTo compute the estimated standard error of the predicted regression curve on a grid\nof values of the explanatory variable `range`, we first build a grid of 200 equally spaced points \nwithin the observed scope of the variable `range`:\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/bsplines.se2_cd0c2eaa8d14c18035d45dedc7628e3e'}\n\n```{.r .cell-code}\nxx <- seq(min(lidar$range), max(lidar$range), length = 200)\n```\n:::\n\nThe  `predict` method for objects of class `lm` returns estimated standard\nerrors for each fitted value if we set the argument\n`se.fit = TRUE`:\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/bsplines.se3_9a88f5b409b4542ccd5395a797089521'}\n\n```{.r .cell-code}\nppmc <- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nps <- predict(ppmc, newdata = list(range = xx), se.fit = TRUE)\n```\n:::\n\nWe now compute upper and lower confidence bands (I used 2 standard errors) around the fitted regression line:\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/bsplines.se4_267711fe55f229b36c78c42a0a470417'}\n\n```{.r .cell-code}\nup <- (ps$fit + 2 * ps$se.fit)\nlo <- (ps$fit - 2 * ps$se.fit)\n```\n:::\n\nFinally, we display the *confidence bands* we just constructed (using **base R** graphics, \nbut also consider using `ggplot2`):\n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/bsplines.se5_a22321647edad7b54617088e952ee83d'}\n\n```{.r .cell-code}\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 4, col = \"gray30\")\nmyrgb <- col2rgb(\"red\") / 256 # , alpha=TRUE)\nmyrgb <- rgb(red = myrgb[1], green = myrgb[2], blue = myrgb[3], alpha = .3)\npolygon(c(xx, rev(xx)), c(up, rev(lo)), density = NA, col = myrgb) #' lightblue')\nlines(ps$fit ~ xx, data = lidar, lwd = 4, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/bsplines.se5-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n**It is important to note that the above confidence bands were constructed assuming that the knots were fixed (not random), and similarly for the degree of the spline basis.** \n\nIncreasing the degree of the cubic basis yields smoother fits (having higher order continuous derivatives). For example, using cubic splines yields an even smoother fit:\n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/bsplines3_b9bb94966abc63ef2adcdaf544745a7b'}\n\n```{.r .cell-code}\n# cubic splines\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nppmc <- lm(logratio ~ bs(range, degree = 3, knots = kn), data = lidar)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"tomato3\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/bsplines3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNote that the estimated regression function seems to have started to \"twitch\"\nand wiggle, particularly at the upper end of our observations. \n\n## How many knots should we use?\n\nSo far we have used 5 knots, but we could have used any other number of knots. If we consider a quadratic\nspline basis with 10 knots, the fit appears a bit better (at least aesthetically): \n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/bsplines.10knots_85ed17d8a3d84b6ff29b7dc829eacc73'}\n\n```{.r .cell-code}\nk <- 10\nkn <- as.numeric(quantile(lidar$range, (1:k)/(k + 1)))\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nppmc <- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"tomato3\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/bsplines.10knots-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWhat about using more knots? The following plot used 50 knots:\n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/bsplines.50knots_ccbd99cca9108abfb18ed4040a098fab'}\n\n```{.r .cell-code}\nk <- 50\nkn <- as.numeric(quantile(lidar$range, (1:k) / (k + 1)))\nppmc <- lm(logratio ~ bs(range, degree = 2, knots = kn), data = lidar)\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(predict(ppmc)[order(range)] ~ sort(range), data = lidar, lwd = 6, col = \"hotpink\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/bsplines.50knots-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nClearly not a good idea!\n\n## Smoothing splines\n\nIf we were to follow the approach discussed so far we would need to \nfind an \"optimal\" of selecting the number of knots and their\npositions, **plus** the order of the spline basis. Although one\ncould consider using cross-validation for this, we note that \nthis would require considerable computational effort (we would need \nto perform an exhaustive search on a 3-dimensional grid). \n\nWe saw in class that *natural cubic splines* provide a natural\n*optimal* \nspace to look for a good regression estimator. For a formal\nbut surprisingly simple proof of this optimality result, \nsee again Section 4.1 of\n\n> Wood, S. (2006). *Generalized additive models : an introduction with R*.\n> Chapman & Hall/CRC, \n> Boca Raton, FL. [Library link](http://resolve.library.ubc.ca/cgi-bin/catsearch?bid=8140311).\n\nThis result not only justifies using natural cubic splines, \nbut also eliminates many of the unknown \"tuning parameters\" (the degree\nof the spline basis, the number of knots, and their locations). \nIn fact, we only need to select one tuning parameter--the penalty term, which \ncan be done using any cross-validation \"flavour\" (although \nin this setting leave-one-out CV is particularly appealing, as\nwe discussed in class).\n\n\nThe function `smooth.spline` in `R` computes a cubic smoothing spline \n(natural cubic spline). Details on its arguments and different options\nare available from its help page. \n\nWhen applied to the `lidar` data with penalization \nparameter equal to 0.2 (setting the argument `spar = 0.2`) \nwe obtain the following estimated regression function:\n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/smoothing1_af7dbe0d1a1d28b5593d356aa24510a6'}\n\n```{.r .cell-code}\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\ntmp <- smooth.spline(x = lidar$range, y = lidar$logratio, spar = 0.2, cv = FALSE,\n    all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 6, col = \"magenta\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/smoothing1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThis fit is clearly too wiggly and unsatisfactory. To obtain a smoother fit we increase the penalty term to 0.5:\n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/smoothing1.5_1fc3fd492e841d0d1c8ff5a7b9ca91f8'}\n\n```{.r .cell-code}\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\ntmp <- smooth.spline(x = lidar$range, y = lidar$logratio, spar = 0.5, cv = FALSE,\n    all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 6, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/smoothing1.5-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThe larger the penalty parameter, the smoother the fit. Setting it to 0.8 yields:\n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/smoothing2_0c8ef80e0a79d7d371bbc9a0aa722552'}\n\n```{.r .cell-code}\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\ntmp <- smooth.spline(x = lidar$range, y = lidar$logratio, spar = 0.8, cv = FALSE,\n    all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 6, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/smoothing2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nIt is easy to see that the larger the penalty coefficient the closer\nthe resulting natural cubic spline becomes to a linear function (why?). \nFor example, if we use `smooth.spline(spar=2)`:\n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/smoothing3_6bc9e16f5f02593b34937f34ef425d3c'}\n\n```{.r .cell-code}\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\ntmp <- smooth.spline(x = lidar$range, y = lidar$logratio, spar = 2, cv = FALSE, all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 6, col = \"tomato3\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/smoothing3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## Selecting an \"optimal\" penalty parameter\n\nAs discussed in class, an \"optimal\" natural cubic spline can be found using \ncross-validation, and for these linear predictors, leave-one-out cross-validation\nis particularly attractive (in terms of computational cost).\nThe function `smooth.spline` in `R` will compute (and use) an optimal value for the penalty term using \nleave-one-out cross-validation when we set the argument `cv = TRUE`: \n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/smoothing.cv1_db1fc73a86d1c88f7e5da60485be1ea5'}\n\n```{.r .cell-code}\ntmp.cv <- smooth.spline(x = lidar$range, y = lidar$logratio, cv = TRUE, all.knots = TRUE)\n# tmp.cv$spar = 0.974\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(tmp.cv$y ~ tmp.cv$x, lwd = 6, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/smoothing.cv1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n### Sanity check (always a good idea)\n\nNote that the optimal value found for the regularization parameter (`spar`) is\nalso returned in the element \n`$spar` of the object returned by `smooth.spline`. Just as a **sanity check**\nwe can now call `smooth.spline` with `cv = FALSE` and manually set\n`spar` to this optimal value, and verify that we obtain the same fit:\n\n\n::: {.cell layout-align=\"center\" hash='22-nonpar-splines-poly_cache/html/smoothing.cv2_faa4bacd13db5ce7c238a702829b70d0'}\n\n```{.r .cell-code}\nplot(logratio ~ range, data = lidar, pch = 19, col = \"gray\", cex = 1.5)\nlines(tmp.cv$y ~ tmp.cv$x, lwd = 8, col = \"blue\")\ntmp <- smooth.spline(x = lidar$range, y = lidar$logratio, spar = tmp.cv$spar, cv = FALSE,\n    all.knots = TRUE)\nlines(tmp$y ~ tmp$x, lwd = 3, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](22-nonpar-splines-poly_files/figure-html/smoothing.cv2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## The problem of outliers and other model departures\n\nWhen the data may contain outliers and/or other atypical observations,\nthe estimation methods discussed above may be seriously affected, even\nif there are only a few such aberrant data points in the training set \n(possible outliers in the test / validation set are also a concern, but\nwe don't have time to discuss it here). Some robust estimation \nmethods based on splines exist. See for example [@TharmaratnamClaeskens2010]\nand references therein. Software in `R` implementing this method is available\n[here](https://github.com/msalibian/PenalizedS). \n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}