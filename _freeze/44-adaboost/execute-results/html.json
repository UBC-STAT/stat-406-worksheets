{
  "hash": "3583491b2b2f0d8bee1a813c06ccb96a",
  "result": {
    "markdown": "# What is Adaboost doing, *really*? \n\n\n\n\n\n\nFollowing the work of [@FriedmanHastie2000] (see also\nChapter 10 of [ESL]), we saw in class that Adaboost can be \ninterpreted as fitting an *additive model* in a stepwise (greedy) way,\nusing an exponential loss. \nIt is then easy to prove that Adaboost.M1 \nis computing an approximation to the *optimal classifier*\nG( x ) = log[ P( Y = 1 | X = x ) / P( Y = -1 | X = x ) ] / 2,\nwhere *optimal* here is taken with respect to the **exponential loss** \nfunction. More specifically, Adaboost.M1 is using an \nadditive model to approximate that function. In other words, Boosting is\nattempting to find functions $f_1$, $f_2$, ..., $f_N$ such that \n$G(x) = \\sum_i f_i( x^{(i)} )$, where $x^{(i)}$ is a sub-vector\nof $x$ (i.e. the function $f_i$ only depends on *some* of the\navailable features, typically a few of them: 1 or 2, say). Note\nthat each $f_i$ generally depends on a different subset of \nfeatures than the other $f_j$'s. \n\nKnowing the function the boosting algorithm is approximating (even\nif it does it in a greedy and suboptimal way), allows us to \nunderstand when the algorithm is expected to work well,\nand also when it may not work well. \nIn particular, it provides one way to choose the complexity of the \n*weak lerners* used to construct the ensemble. For an example\nyou can refer to the corresponding lab activity. \n\n### A more challenging example, the `email spam` data\n\nThe email spam data set is a relatively classic data set \ncontaining 57 features (potentially explanatory variables) \nmeasured on 4601 email messages. The goal is to predict\nwhether an email is *spam* or not. The 57 features are \na mix of continuous and discrete variables. More information\ncan be found at\n[https://archive.ics.uci.edu/ml/datasets/spambase](https://archive.ics.uci.edu/ml/datasets/spambase).\n\nWe first load the data and randomly separate it into a training and\na test set. A more thorough analysis would be to use \n*full* K-fold cross-validation, but given the computational\ncomplexity, I decided to leave the rest of this \n3-fold CV exercise to the reader. \n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/spam.1_6dd0311aba057e75be80acb2314d1997'}\n\n```{.r .cell-code}\ndata(spam, package = \"ElemStatLearn\")\nn <- nrow(spam)\nset.seed(987)\nii <- sample(n, floor(n / 3))\nspam.te <- spam[ii, ]\nspam.tr <- spam[-ii, ]\n```\n:::\n\nWe now use Adaboost with 500 iterations, using *stumps* (1-split\ntrees) as our\nweak learners / classifiers, and check the performance on\nthe test set:\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/spam.2_869e6c9d7100d5776e705399104f84e4'}\n\n```{.r .cell-code}\nlibrary(adabag)\nonesplit <- rpart.control(cp = -1, maxdepth = 1, minsplit = 0, xval = 0)\nbo1 <- boosting(spam ~ ., data = spam.tr, boos = FALSE, mfinal = 500, control = onesplit)\npr1 <- predict(bo1, newdata = spam.te)\ntable(spam.te$spam, pr1$class) # (pr1$confusion)\n#>        \n#>         email spam\n#>   email   883   39\n#>   spam     45  566\n```\n:::\n\nThe classification error rate on the test set is 0.055. We now\ncompare it with that of a Random Forest and look at the fit:\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/spam.3_a09532fbf1481317b2e7dbe85f9ea0d7'}\n\n```{.r .cell-code}\nlibrary(randomForest)\nset.seed(123)\n(a <- randomForest(spam ~ ., data = spam.tr, ntree = 500))\n#> \n#> Call:\n#>  randomForest(formula = spam ~ ., data = spam.tr, ntree = 500) \n#>                Type of random forest: classification\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 7\n#> \n#>         OOB estimate of  error rate: 5.05%\n#> Confusion matrix:\n#>       email spam class.error\n#> email  1813   53  0.02840300\n#> spam    102 1100  0.08485857\n```\n:::\n\nNote that the OOB estimate of the classification error rate \nis 0.051. \nThe number of trees used seems to be appropriate in terms\nof the stability of the OOB error rate estimate:\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/spam.plot.rf_4b92942ed6069d132d784f8afb9efb1b'}\n\n```{.r .cell-code}\nplot(a)\n```\n\n::: {.cell-output-display}\n![](44-adaboost_files/figure-html/spam.plot.rf-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nNow use the test set to estimate the error rate of the Random Forest \n(for a fair comparison with the one computed with boosting) and obtain\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/spam.error.rate.rf_1d68fb1a5aa3cd69fd9a1b8168025d9e'}\n\n```{.r .cell-code}\npr.rf <- predict(a, newdata = spam.te, type = \"response\")\ntable(spam.te$spam, pr.rf)\n#>        pr.rf\n#>         email spam\n#>   email   886   36\n#>   spam     36  575\n```\n:::\n\nThe performance of Random Forests on this test set is better than that of \nboosting (recall that the estimated classification error rate \nfor 1-split trees-based Adaboost was \n0.055, while for the Random Forest is 0.047 on the test set and 0.051 using OOB). \n\nIs there *any room for improvement* for Adaboost? \nAs we discussed in class, depending on the interactions that may be \npresent in the *true classification function*, we might be able to \nimprove our boosting classifier by slightly increasing the complexity\nof our base ensemble members. Here we try to use 3-split classification\ntrees, instead of the 1-split ones used above:\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/spam.4_548a6afc9aff46540504dc1df18f23db'}\n\n```{.r .cell-code}\nthreesplits <- rpart.control(cp = -1, maxdepth = 3, minsplit = 0, xval = 0)\nbo3 <- boosting(spam ~ ., data = spam.tr, boos = FALSE, mfinal = 500, control = threesplits)\npr3 <- predict(bo3, newdata = spam.te)\n(pr3$confusion)\n#>                Observed Class\n#> Predicted Class email spam\n#>           email   881   34\n#>           spam     41  577\n```\n:::\n\nThe number of elements on the boosting ensemble (500) appears to be\nappropriate when we look at the error rate on the test set as \na function of the number of boosting iterations:\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/spam.5_5822a5a821e0075f58b6d2bf8a985dc5'}\n\n```{.r .cell-code}\nplot(errorevol(bo3, newdata = spam.te))\n```\n\n::: {.cell-output-display}\n![](44-adaboost_files/figure-html/spam.5-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nThere is, in fact, a noticeable improvement in performance on this\ntest set compared to the AdaBoost using *stumps*. \nThe estimated classification error rate of AdaBoost using 3-split trees on this test set is\n0.049. Recall that the estimated classification error rate\nfor the Random Forest was 0.047\n(or 0.051 using OOB). \n\nAs mentioned above you are strongly encouraged to finish this analysis\nby doing a complete K-fold CV analysis in order to compare boosting with random \nforests on these data. \n\n\n### An example on improving Adaboost's performance including interactions\n\n**Some error I can't track happens below**\n\nConsider the data set in the file `boost.sim.csv`. This \nis a synthetic data inspired by the \nwell-known Boston Housing data. The response variable is `class` \nand the two predictors are `lon` and `lat`. We read the data set\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/sim.read_3c557cdb72bc6a7be84c8e43f1730fc6'}\n\n```{.r .cell-code}\nsim <- read.table(\"data/boost.sim.csv\", header = TRUE, sep = \",\", row.names = 1)\n```\n:::\n\nWe split the data randomly into a training and a test set:\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/sim.split_7289f275b32634a1197ff88177e21534'}\n\n```{.r .cell-code}\nset.seed(123)\nii <- sample(nrow(sim), nrow(sim) / 3)\nsim.tr <- sim[-ii, ]\nsim.te <- sim[ii, ]\n```\n:::\n\nAs before, we use *stumps* as our base classifiers\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/boston0_c7a4f9aaa88d50124cee11334d9c56a1'}\n\n```{.r .cell-code}\nlibrary(rpart)\nstump <- rpart.control(cp = -1, maxdepth = 1, minsplit = 0, xval = 0)\n```\n:::\n\nand run 300 iterations of the boosting algorithm:\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/bostonsimu0_9131653047871f668f2fad6488412c27'}\n\n```{.r .cell-code}\nset.seed(17)\nsim1 <- boosting(class ~ ., data = sim.tr, boos = FALSE, mfinal = 300, control = stump)\n```\n:::\n\nWe examine the evolution of our ensemble on the test set:\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/bostonsimu1_5c83d1e9e7a4778af7cce393eddd0d61'}\n\n```{.r .cell-code}\nplot(errorevol(sim1, newdata = sim.te))\n```\n:::\n\n\nand note that the peformance is both disappointing and does not improve with\nthe number of iterations. The error rate on the test set is\n.\nBased on the discussion in class about the effect of the \ncomplexity of the base classifiers, \nwe now increase slightly their complexity: from\nstumps to trees with up to 2 splits:\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/boston00_6c8d76d171204ea6acb9f6f565e36a81'}\n\n```{.r .cell-code}\ntwosplit <- rpart.control(cp = -1, maxdepth = 2, minsplit = 0, xval = 0)\nset.seed(17)\nsim2 <- boosting(class ~ ., data = sim.tr, boos = FALSE, mfinal = 300, control = twosplit)\nplot(errorevol(sim2, newdata = sim.te))\n```\n:::\n\n\nNote that the error rate improves noticeably to \n.\nInterestingly, note as well that increasing the number \nof splits of the base classifiers does not seem to \nhelp much. With 3-split trees:\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/boston00.more_d2d9911daee1cef2b57f0da627f1e21b'}\n\n```{.r .cell-code}\nthreesplit <- rpart.control(cp = -1, maxdepth = 3, minsplit = 0, xval = 0)\nset.seed(17)\nsim3 <- boosting(class ~ ., data = sim.tr, boos = FALSE, mfinal = 300, control = threesplit)\nplot(errorevol(sim3, newdata = sim.te))\n```\n:::\n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/boston001_adbc23a4e57de3d73408feded2187237'}\n\n```{.r .cell-code}\nfoursplit <- rpart.control(cp = -1, maxdepth = 4, minsplit = 0, xval = 0)\nset.seed(17)\nsim4 <- boosting(class ~ ., data = sim.tr, boos = FALSE, mfinal = 300, control = foursplit)\n```\n:::\n\nthe error rate on the test set is \n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/unnamed-chunk-1_bc9ee19f88b8d0f68a4ea45c52a4ea91'}\n\n```{.r .cell-code}\nround(predict(sim3, newdata = sim.te)$error, 4)\n```\n:::\n\nwhile with 4-split trees the error rate is \n\n::: {.cell layout-align=\"center\" hash='44-adaboost_cache/html/unnamed-chunk-2_f2b25be6891dce401c8f1f34a65c8f75'}\n\n```{.r .cell-code}\nround(predict(sim4, newdata = sim.te)$error, 4)\n```\n:::\n\n\n\nThe explanation for this is that the response variables \nin the data set \\texttt{boost.sim.csv} were in fact generated\nthrough the following relationship:\n```\nlog [ P ( Y = 1 | X = x ) / P ( Y = -1 | X = x ) ] / 2\n = [ max( x2 - 2, 0) - max( x1 + 1, 0) ] ( 1- x1 + x2 )\n```\nwhere $x = (x_1, x_2)^\\top$. Since *stumps* (1-split trees) \nare by definition functions of a single\nvariable, boosting will not be able to approximate the above function using\na linear combination of them, regardless of how many terms you use. Two-split\ntrees, on the other hand, are able to model interactions between the two\nexplanatory variables $X_1$ (`lon`) and\n$X_2$ (`lat`), and thus, with sufficient terms in the sum, we are able to \napproximate the above function relatively well. \n\nAs before, note that the analysis above may depend on the specific \ntraining / test split we used, so it is strongly suggested that you\nre-do it using a proper cross-validation setup. \n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}