{
  "hash": "b4c12b729ea225011efbdc7bed135505",
  "result": {
    "markdown": "# Random Forests\n\n\n\n\n\n\nEven though using a *bagged* ensemble of trees usually results in a more\nstable predictor / classifier, a better ensemble can be improved by training \neach of its members in a careful way. The main idea is to try to reduce the \n(conditional) potential correlation among the predictions of the bagged trees, \nas discussed in class. Each of the bootstrap trees in the ensemble is\ngrown using only a randomly selected set of features when partitioning each node.\nMore specifically, at each node only a random subset of explanatory variables\nis considered to determine the optimal split. These randomly chosen features \nare selected independently at each node as the tree is being constructed. \n\nTo train a Random Forest in `R` we use the \nfuntion `randomForest` from the package with the same name.\nThe syntax is the same as that of `rpart`, but the tuning parameters \nfor each of the *trees* in the *forest* are different from `rpart`. \nRefer to the help page if you need to modify them. \n\nWe load and prepare the admissions data as before:\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/init_a21f8b3d9c5eb3ebac192ef7a749161c'}\n\n```{.r .cell-code}\nmm <- read.table(\"data/T11-6.DAT\", header = FALSE)\nmm$V3 <- as.factor(mm$V3)\nmm[, 2] <- mm[, 2] / 150\n```\n:::\n\nand train a Random Forest with 500 trees and using all the default tuning parameters:\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf1_9f0d5aa34bfa59635b288c868259bcc0'}\n\n```{.r .cell-code}\nlibrary(randomForest)\na.rf <- randomForest(V3 ~ V1 + V2, data = mm, ntree = 500)\n```\n:::\n\nPredictions can be obtained using the `predict` method, as usual, when \nyou specify the `newdata` argument. Refer to the help page \nof `predict.randomForest` for details on the different \nbehaviour of `predict` for Random Forest objects when the argument `newdata` is \neither present or missing. \n\nTo visualize the predicted classes obtained with a Random Forest\non our example data, we compute the corresponding\npredicted conditional class probabilities on the \nsame grid used before:\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/inst2.0.2_c5026b9f27cfc877f262d8216058cc5e'}\n\n```{.r .cell-code}\naa <- seq(2, 5, length = 200)\nbb <- seq(2, 5, length = 200)\ndd <- expand.grid(aa, bb)\nnames(dd) <- names(mm)[1:2]\n```\n:::\n\nThe estimated conditional probabilities for class *red* \nare shown in the plot below\n(how are these estimated conditional probabilities computed exactly?)\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf1.1_71d0e8655b39e1969833e0a649362246'}\n\n```{.r .cell-code}\npp.rf <- predict(a.rf, newdata = dd, type = \"prob\")\nfilled.contour(aa, bb, matrix(pp.rf[, 1], 200, 200),\n  col = terrain.colors(20),\n  xlab = \"GPA\", ylab = \"GMAT\",\n  plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3],\n      pch = 19, cex = 1.5,\n      col = c(\"red\", \"blue\", \"green\")[mm[, 3]]\n    )\n  }\n)\n```\n\n::: {.cell-output-display}\n![](42-random-forests_files/figure-html/rf1.1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nAnd the predicted conditional probabilities for the rest of the classes are:\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf2_64d948f6bcafe2387ee7fc302ebc10ba'}\n\n```{.r .cell-code}\nfilled.contour(aa, bb, matrix(pp.rf[, 2], 200, 200),\n  col = terrain.colors(20),\n  xlab = \"GPA\", ylab = \"GMAT\", plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3],\n      pch = 19, cex = 1.5,\n      col = c(\"red\", \"blue\", \"green\")[mm[, 3]]\n    )\n  }\n)\n```\n\n::: {.cell-output-display}\n![](42-random-forests_files/figure-html/rf2-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\nfilled.contour(aa, bb, matrix(pp.rf[, 3], 200, 200),\n  col = terrain.colors(20), xlab = \"GPA\",\n  ylab = \"GMAT\", plot.axes = {\n    axis(1)\n    axis(2)\n  },\n  panel.last = {\n    points(mm[, -3], pch = 19, cex = 1.5, col = c(\"red\", \"blue\", \"green\")[mm[, 3]])\n  }\n)\n```\n\n::: {.cell-output-display}\n![](42-random-forests_files/figure-html/rf2-2.png){fig-align='center' width=90%}\n:::\n:::\n\n\nA very interesting exercise would be to train a Random Forest on the perturbed data\n(in `mm2`) and verify that the predicted conditional probabilities do not change much, \nas was the case for the bagged classifier. \n\n## Another example\n\nWe will now use a more interesting example. The ISOLET data, available \nhere: \n[http://archive.ics.uci.edu/ml/datasets/ISOLET](http://archive.ics.uci.edu/ml/datasets/ISOLET), \ncontains data \non sound recordings of 150 speakers saying each letter of the\nalphabet (twice). See the original source for more details. Since \nthe full data set is rather large, here we only use the subset \ncorresponding to the observations for the letters **C** and **Z**. \n\nWe first load the training and test data sets, and force the response \nvariable to be categorical, so that the `R` implementations of the\ndifferent predictors we will use below will build \nclassifiers and not their regression counterparts:\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.isolet_f0d338420eb475f9414417e974a14a40'}\n\n```{.r .cell-code}\nxtr <- read.table(\"data/isolet-train-c-z.data\", sep = \",\")\nxte <- read.table(\"data/isolet-test-c-z.data\", sep = \",\")\nxtr$V618 <- as.factor(xtr$V618)\nxte$V618 <- as.factor(xte$V618)\n```\n:::\n\nTo train a Random Forest we use the function `randomForest` in the\npackage of the same name. The code underlying this package was originally \nwritten by Leo Breiman. We first train a Random Forest, using all the default parameters\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.isolet02_1a26aad5d28fdcc8fe2bd702506423ce'}\n\n```{.r .cell-code}\nlibrary(randomForest)\nset.seed(123)\n(a.rf <- randomForest(V618 ~ ., data = xtr, ntree = 500))\n#> \n#> Call:\n#>  randomForest(formula = V618 ~ ., data = xtr, ntree = 500) \n#>                Type of random forest: classification\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 24\n#> \n#>         OOB estimate of  error rate: 2.29%\n#> Confusion matrix:\n#>      3  26 class.error\n#> 3  234   6  0.02500000\n#> 26   5 235  0.02083333\n```\n:::\n\nWe now check its performance on the test set:\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.isolet.te_886b5553505b92490e49de8549d44338'}\n\n```{.r .cell-code}\np.rf <- predict(a.rf, newdata = xte, type = \"response\")\ntable(p.rf, xte$V618)\n#>     \n#> p.rf  3 26\n#>   3  60  1\n#>   26  0 59\n```\n:::\n\nNote that the Random Forest only makes one mistake out of 120 (approx 0.8%) observations\nin the test set. However, the OOB error rate estimate is slightly over 2%. \nThe next plot shows the evolution of the OOB error rate estimate as a function of the \nnumber of classifiers in the ensemble (trees in the forest). Note that 500 trees \nappears to be a reasonable forest size, in the sense thate the OOB error rate estimate is stable. \n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf0.oob_c885473b743f07e1e8a377cf7ebf5d50'}\n\n```{.r .cell-code}\nplot(a.rf, lwd = 3, lty = 1)\n```\n\n::: {.cell-output-display}\n![](42-random-forests_files/figure-html/rf0.oob-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nConsider again the ISOLET data, available\nhere:\n[http://archive.ics.uci.edu/ml/datasets/ISOLET](http://archive.ics.uci.edu/ml/datasets/ISOLET).\nHere we only use a subset\ncorresponding to the observations for the letters **C** and **Z**.\n\nWe first load the training and test data sets, and force the response\nvariable to be categorical, so that the `R` implementations of the\ndifferent predictors we will use below will build\nclassifiers and not their regression counterparts:\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.isolet00_e32bd39c1f2c7ae7746f81833e511204'}\n\n```{.r .cell-code}\nxtr <- read.table(\"data/isolet-train-c-z.data\", sep = \",\")\nxte <- read.table(\"data/isolet-test-c-z.data\", sep = \",\")\nxtr$V618 <- as.factor(xtr$V618)\nxte$V618 <- as.factor(xte$V618)\n```\n:::\n\n\nTo train a Random Forest we use the function `randomForest` in the\npackage of the same name. The code underlying this package was originally\nwritten by Leo Breiman. We train a RF leaving all\nparamaters at their default values, and check\nits performance on the test set:\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.isolet2_a4ece2c362345e557bcc149ae3f8f82a'}\n\n```{.r .cell-code}\nlibrary(randomForest)\nset.seed(123)\na.rf <- randomForest(V618 ~ ., data = xtr, ntree = 500)\np.rf <- predict(a.rf, newdata = xte, type = \"response\")\ntable(p.rf, xte$V618)\n#>     \n#> p.rf  3 26\n#>   3  60  1\n#>   26  0 59\n```\n:::\n\nNote that the Random Forest only makes one mistake out of 120 observations\nin the test set. The OOB error rate estimate is slightly over 2%,\nand we see that 500 trees is a reasonable forest size:\n\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.oob_53d28529cb3207ad0de8bed119558c12'}\n\n```{.r .cell-code}\nplot(a.rf, lwd = 3, lty = 1)\n```\n\n::: {.cell-output-display}\n![](42-random-forests_files/figure-html/rf.oob-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\na.rf\n#> \n#> Call:\n#>  randomForest(formula = V618 ~ ., data = xtr, ntree = 500) \n#>                Type of random forest: classification\n#>                      Number of trees: 500\n#> No. of variables tried at each split: 24\n#> \n#>         OOB estimate of  error rate: 2.29%\n#> Confusion matrix:\n#>      3  26 class.error\n#> 3  234   6  0.02500000\n#> 26   5 235  0.02083333\n```\n:::\n\n\n## Using a test set instead of OBB\n\nGiven that in this case we do have a test set, we can use it \nto monitor the error rate (instead of using the OOB error estimates):\n\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.isolet.test_99873a80994ba0008704001c59cf6ca2'}\n\n```{.r .cell-code}\nx.train <- model.matrix(V618 ~ ., data = xtr)\ny.train <- xtr$V618\nx.test <- model.matrix(V618 ~ ., data = xte)\ny.test <- xte$V618\nset.seed(123)\na.rf <- randomForest(x = x.train, y = y.train, xtest = x.test, ytest = y.test, ntree = 500)\ntest.err <- a.rf$test$err.rate\nma <- max(c(test.err))\nplot(test.err[, 2], lwd = 2, lty = 1, col = \"red\", type = \"l\", ylim = c(0, max(c(0, ma))))\nlines(test.err[, 3], lwd = 2, lty = 1, col = \"green\")\nlines(test.err[, 1], lwd = 2, lty = 1, col = \"black\")\n```\n\n::: {.cell-output-display}\n![](42-random-forests_files/figure-html/rf.isolet.test-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nAccording to the help page for the `plot` method for objects of class\n`randomForest`, the following plot should show both error rates (OOB plus\nthose on the test set):\n\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.isolet.test.plot_6225e267e191dc1ca85afa10a86ea8b2'}\n\n```{.r .cell-code}\nplot(a.rf, lwd = 2)\n```\n\n::: {.cell-output-display}\n![](42-random-forests_files/figure-html/rf.isolet.test.plot-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## Feature sequencing / Variable ranking\n\nTo explore which variables were used in the forest,\nand also, their importance rank as discussed in\nclass, we can use the function `varImpPlot`:\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.isolet3_94c53b7b78746bcf94cbd75456c9f198'}\n\n```{.r .cell-code}\nvarImpPlot(a.rf, n.var = 20)\n```\n\n::: {.cell-output-display}\n![](42-random-forests_files/figure-html/rf.isolet3-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n## Comparing RF with other classifiers\n\nWe now compare the Random Forest with some of the other classifiers we saw in class,\nusing their classification error rate on the test set as our comparison measure. \nWe first start with K-NN:\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.isolet4_6e1c3ff002c872a8ae984e80beb89415'}\n\n```{.r .cell-code}\nlibrary(class)\nu1 <- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 1)\ntable(u1, xte$V618)\n#>     \n#> u1    3 26\n#>   3  57  9\n#>   26  3 51\n\nu5 <- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 5)\ntable(u5, xte$V618)\n#>     \n#> u5    3 26\n#>   3  58  5\n#>   26  2 55\n\nu10 <- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 10)\ntable(u10, xte$V618)\n#>     \n#> u10   3 26\n#>   3  58  6\n#>   26  2 54\n\nu20 <- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 20)\ntable(u20, xte$V618)\n#>     \n#> u20   3 26\n#>   3  58  5\n#>   26  2 55\n\nu50 <- knn(train = xtr[, -618], test = xte[, -618], cl = xtr[, 618], k = 50)\ntable(u50, xte$V618)\n#>     \n#> u50   3 26\n#>   3  58  7\n#>   26  2 53\n```\n:::\n\nTo use logistic regression we first create a new variable that is 1\nfor the letter **C** and 0 for the letter **Z**, and use it as\nour response variable. \n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.isoletglm_6bb08d471c3abb8970cfc65c2bb87cad'}\n\n```{.r .cell-code}\nxtr$V619 <- as.numeric(xtr$V618 == 3)\nd.glm <- glm(V619 ~ . - V618, data = xtr, family = binomial)\npr.glm <- as.numeric(predict(d.glm, newdata = xte, type = \"response\") > 0.5)\ntable(pr.glm, xte$V618)\n#>       \n#> pr.glm  3 26\n#>      0 25 33\n#>      1 35 27\n```\n:::\n\nQuestion for the reader: why do you think this classifier's performance\nis so disappointing? \n\nIt is interesting to see how a simple LDA classifier does:\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.isolet5_ef4ac18e11c0e6d4e40ccf2abf5ba7ab'}\n\n```{.r .cell-code}\nlibrary(MASS)\nxtr$V619 <- NULL\nd.lda <- lda(V618 ~ ., data = xtr)\npr.lda <- predict(d.lda, newdata = xte)$class\ntable(pr.lda, xte$V618)\n#>       \n#> pr.lda  3 26\n#>     3  58  3\n#>     26  2 57\n```\n:::\n\nFinally, note that a carefully built classification tree \nperforms remarkably well, only using 3 features:\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.isolet6_2e9d82c144e8613cc97c6ba643c7d28a'}\n\n```{.r .cell-code}\nlibrary(rpart)\nmy.c <- rpart.control(minsplit = 5, cp = 1e-8, xval = 10)\nset.seed(987)\na.tree <- rpart(V618 ~ ., data = xtr, method = \"class\", parms = list(split = \"information\"), control = my.c)\ncp <- a.tree$cptable[which.min(a.tree$cptable[, \"xerror\"]), \"CP\"]\na.tp <- prune(a.tree, cp = cp)\np.t <- predict(a.tp, newdata = xte, type = \"vector\")\ntable(p.t, xte$V618)\n#>    \n#> p.t  3 26\n#>   1 59  0\n#>   2  1 60\n```\n:::\n\nFinally, note that if you train a single classification tree with the\ndefault values for the stopping criterion tuning parameters, the \ntree also uses only 3 features, but its classification error rate\non the test set is larger than that of the pruned one:\n\n::: {.cell layout-align=\"center\" hash='42-random-forests_cache/html/rf.isolet7_dd2e36501bfb2c9b1e74a7b4d72d9e7f'}\n\n```{.r .cell-code}\nset.seed(987)\na2.tree <- rpart(V618 ~ ., data = xtr, method = \"class\", parms = list(split = \"information\"))\np2.t <- predict(a2.tree, newdata = xte, type = \"vector\")\ntable(p2.t, xte$V618)\n#>     \n#> p2.t  3 26\n#>    1 57  2\n#>    2  3 58\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}