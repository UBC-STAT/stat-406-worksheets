{
  "hash": "8845bb981b029e36ad36c1df593e9b84",
  "result": {
    "markdown": "# QDA\n\n\n\n\n\n\nSimilarly to the way we derived the LDA classifier in class, if one relaxes the assumption\nthat the conditional distribution of the vector of features **X** in each class has\nthe same covariance matrix (*shape*) (but still assumes that these distributions are\nGaussian), then it is (again) easy to find a closed form for the conditional probability\nof each class (conditional on a vector of features **X**). As in the LDA case,\nthese conditional class probabilities (aka *posterior probabilities*) depend on the\nparameters of the assumed model for the conditional distributions of **X** in each \nclass. So, again, we estimate those parameters from the training set (usin the observations\nin each group) and plug them in to compute the conditional class probabilities. \n\nSimilarly to what we did for LDA, it is easy to see that in this case the class\nboundaries are quadratic functions of the vector of features **X**. \n\nWe illustrate QDA on the same `vaso` data we used before. We first load the \ndata, and train a QDA classifier using the function `qda` in package `MASS`\n(this can also be written as `MASS::qda()`). \n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/qda1_4cd9a9320f46fb57d2082337b1c59c2c'}\n\n```{.r .cell-code}\ndata(vaso, package = \"robustbase\")\nlibrary(MASS)\na.qda <- qda(Y ~ ., data = vaso)\n```\n:::\n\nWe now build a relatively fine grid of points in the domain of our\n2-dimensional vector of features and use the `predict` method\nassociated with a `qda` object to predict the conditional probability of\nclass `blue`:\n\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/qda2_45d3132fbcff2a89bc8a5f10ec60eb82'}\n\n```{.r .cell-code}\nxvol <- seq(0, 4, length = 200)\nxrat <- seq(0, 4, length = 200)\nxx <- expand.grid(xvol, xrat)\nnames(xx) <- c(\"Volume\", \"Rate\")\npr.qda <- predict(a.qda, newdata = xx)$posterior[, 2]\nimage(xrat, xvol, matrix(pr.qda, 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", cex.lab = 1.5, cex.axis = 1.5\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\ncontour(xrat, xvol, matrix(pr.qda, 200, 200),\n  col = \"gray30\", levels = .5,\n  drawlabels = FALSE, lwd = 3, add = TRUE\n)\n```\n\n::: {.cell-output-display}\n![](31-qda-knn_files/figure-html/qda2-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe used the function `contour` above to draw the boundary between\nclasses (the set of points where the probability of blue is equal to\nthe probability of red). \n\n## Sensitivity to the Gaussian assumption\n\nWe discussed in class (with the help of a simple example) the sensitivity of\nQDA to the assumed specific conditional distribution of the \nfeatures within each class. It is very easy to see that LDA may also\nbe affected by similar problems. This is not at all surprising--in many\ncases optimal methods obtained under certain conditions are very\nsensitive to the vailidity of the assumptions used in their derivation.\n\nIt is interesting to note (as discussed in class) that logistic\nregression was not affected by the \"good outliers\" we included in \nthe data. Considering where these \"good outliers\" are (in terms\nof their corresponding likelihood values), this is probably \nnot surprising. Note, furthermore, that both QDA (and LDA) and logistic regression\nare classifiers that require the estimation of parameters (maybe we\ncan call them *parametric classifiers*?), and in all cases considered so far\nthe parameters were estimated using maximum likelihood. However their\nsensitivity to this kind of outliers is very different. \n\n## More than 2 classes -- The handwritten digit recognition data\n\nAs you may have noted, all the classification methods we have seen so far can \nbe used in applications with an arbitrary number of classes. We will now\nillustrate them on the well-known Handwritten Digit Recognition Data\n(as usual, see `help(zip.train, package='ElemStatLearn')`). We first\nload the data, and extract the images corresponding to digits 0, 1 and\n8. These should be challenging enough to discriminate given their\nsimilar shapes. \n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/zip1_879d318c9c95bff47c6434257d0703ec'}\n\n```{.r .cell-code}\ndata(zip.train, package = \"ElemStatLearn\")\ndata(zip.test, package = \"ElemStatLearn\")\nx.tr <- zip.train[zip.train[, 1] %in% c(0, 1, 8), ]\nx.te <- zip.test[zip.test[, 1] %in% c(0, 1, 8), ]\n```\n:::\n\nThe values of the pixes of each image are in the rows of the\ncorresponding matrix (columns 2:256), and the true class of each image is in \nthe first column. Note that there are relatively few 8's in this training set:\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/zip10_f02cce29dc4e7a713537e36ec219f3aa'}\n\n```{.r .cell-code}\ntable(x.tr[, 1])\n#> \n#>    0    1    8 \n#> 1194 1005  542\n```\n:::\n\nTo display these 16x16 images we adapt a simple function to plot matrices:\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/ziplotfun_ec34ab2cf80e84aee843d5eda0c9ebdc'}\n\n```{.r .cell-code}\n# ----- Define a function for plotting a matrix ----- #\n# modified from: http://www.phaget4.org/R/image_matrix.html\nmyImagePlot <- function(x) {\n  min <- min(x)\n  max <- max(x)\n  ColorRamp <- grey(seq(1, 0, length = 256))\n  ColorLevels <- seq(min, max, length = length(ColorRamp))\n  # Reverse Y axis\n  reverse <- nrow(x):1\n  x <- x[reverse, ]\n  image(1:ncol(x), 1:nrow(x), t(x),\n    col = ColorRamp, xlab = \"\",\n    ylab = \"\", axes = FALSE, zlim = c(min, max)\n  )\n}\n```\n:::\n\n\nNext we choose 9 images at random from the training set, \nand display them in a 3x3 array of images:\n\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/ziplot9_4eed37fb8ce717dc518418980f362216'}\n\n```{.r .cell-code}\na <- x.tr\nset.seed(987)\nsa <- sample(dim(a)[1], 9)\npar(mfrow = c(3, 3))\nfor (j in 1:9) {\n  myImagePlot(t(matrix(unlist(a[sa[j], -1]), 16, 16)))\n}\n```\n\n::: {.cell-output-display}\n![](31-qda-knn_files/figure-html/ziplot9-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\nWe can also show the \"average 8\" in the training set:\n\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/zipav_5d385ab90e973c12bdc1b29c7ddcaaa2'}\n\n```{.r .cell-code}\nmyImagePlot(t(matrix(colMeans(subset(x.tr, subset = (x.tr[, 1] == 8), select = -1)), 16, 16)))\n```\n\n::: {.cell-output-display}\n![](31-qda-knn_files/figure-html/zipav-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\n# alternatively: myImagePlot(t(matrix(colMeans(a[a[,1]==8,-1]), 16, 16)))\n```\n:::\n\n\nWe will now use LDA, QDA and a multinomial logistic model. The \nlatter is the natural extension of logistic regression to more than\n2 classes. You can easily derive it yourself by assuming that the response\nvariable has a multinomial distribution and modeling each \nconditional probability as a (different) logistic function of the\nvector **X** of features. Note that if there are *K* classes you only need\nto model *K-1* of these conditional class probabilities. The derivation\nis left as an easy exercise for you. \n\nNote that the data is stored in a matrix, but the use of \n`lda()`, `qda()`, etc. is clearer when you have your data\nin a `data frame` (as you can then refer to features by their\nnames and use the `data` argument). So, we first transform\nour matrix into a data frame, and name the resulting variables\n*V1*, *V2*, ..., *V257*:\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/zipnames_634fa2f6cddef994eb2a4f240723e90d'}\n\n```{.r .cell-code}\nx.tr <- data.frame(x.tr)\nx.te <- data.frame(x.te)\nnames(x.te) <- names(x.tr) <- paste(\"V\", 1:257, sep = \"\")\n```\n:::\n\nNow we use `lda` and `multinom` (this last one from package `nnet`) to train \nan LDA and a multinomial classifier to these 3-class data:\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/zip2_57dcae80a160765ee3a1fac41ccd2a15'}\n\n```{.r .cell-code}\na <- lda(V1 ~ . - V257, data = x.tr) # x.tr[,1] ~ x[, 2:256])\nlibrary(nnet)\na.log <- multinom(V1 ~ . - V257, data = x.tr, maxit = 5000)\n#> # weights:  771 (512 variable)\n#> initial  value 3011.296283 \n#> iter  10 value 27.327939\n#> iter  20 value 8.491334\n#> iter  30 value 2.640128\n#> iter  40 value 1.228798\n#> iter  50 value 0.663474\n#> iter  60 value 0.391984\n#> iter  70 value 0.212952\n#> iter  80 value 0.114876\n#> iter  90 value 0.053465\n#> iter 100 value 0.026628\n#> iter 110 value 0.014534\n#> iter 120 value 0.009281\n#> iter 130 value 0.006623\n#> iter 140 value 0.004210\n#> iter 150 value 0.002723\n#> iter 160 value 0.001851\n#> iter 170 value 0.001318\n#> iter 180 value 0.001036\n#> iter 190 value 0.000580\n#> iter 200 value 0.000516\n#> iter 210 value 0.000304\n#> iter 220 value 0.000249\n#> iter 230 value 0.000218\n#> final  value 0.000090 \n#> converged\n```\n:::\n\n(Question: *Why do I remove variable `V257` from the models above?*)\n\nAs a side commment: note how slow is the convergence of `multinom`. This is not unusual,\nand it has to do with how neural networks are trained. Refer to \nthe corresponding help page for more information. We will probably\ndiscuss this further later in the course. \n\nFor now we obtain the predictions on the test set and build a \nmatrix of classification errors for each classifier. For LDA we have:\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/zipmisclass_6922a356875c15be8b4e7561139c2107'}\n\n```{.r .cell-code}\npr.lda <- predict(a, newdata = x.te)$class\ntable(pr.lda, x.te$V1)\n#>       \n#> pr.lda   0   1   8\n#>      0 353   2   9\n#>      1   0 258   0\n#>      8   6   4 157\n```\n:::\n\nFor the logistic multinomial classifier we have: \n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/zipmisclass2_cc5c057603d11213ca0d5c7bce6ce9b3'}\n\n```{.r .cell-code}\npr.log <- predict(a.log, newdata = x.te)\ntable(pr.log, x.te$V1)\n#>       \n#> pr.log   0   1   8\n#>      0 342   3  13\n#>      1  12 258  10\n#>      8   5   3 143\n```\n:::\n\n\nWe now attempt to train a QDA classifier:\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/zipqdafails_0119bf1e9ae845a7fb59977484b511bc'}\n\n```{.r .cell-code}\na.qda <- try(qda(V1 ~ . - V257, data = x.tr))\n#> Error in qda.default(x, grouping, ...) : rank deficiency in group 0\nclass(a.qda)\n#> [1] \"try-error\"\n```\n:::\n\nThis classifier cannot be trained on these data. The problem is that\nthe training set for at least one class is rank deficient (which can\nbe found by looking at the error message stored in the returned\nobject `a.qda`\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/zipqdaerrormsg_dedf1759696fa871675bb68f2433c2f9'}\n\n```{.r .cell-code}\na.qda\n#> [1] \"Error in qda.default(x, grouping, ...) : rank deficiency in group 0\\n\"\n#> attr(,\"class\")\n#> [1] \"try-error\"\n#> attr(,\"condition\")\n#> <simpleError in qda.default(x, grouping, ...): rank deficiency in group 0>\n```\n:::\n\nIndeed, we have:\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/ziprank_0bfc8d26802a140b72072d0984442fc6'}\n\n```{.r .cell-code}\nx1 <- x.tr[x.tr$V1 == 0, ]\ndim(x1)\n#> [1] 1194  257\nqr(x1)$rank\n#> [1] 254\n```\n:::\n\nThe questions for you are:\n\n* why is this rank deficiency a problem for QDA, but not for LDA, or a multinomial model?\n* can we do anything to train a (possibly different) QDA classifier to these data?\n\n\n<!-- #### (Optional section) Sensitivity & Specificity -->\n\n<!-- **This section is still under revision, read at your own risk.** -->\n\n<!-- Any binary decision-making process has two important features, generally called -->\n<!-- senstivity and specificity. Intuitively these measure: -->\n\n<!-- * how often it makes correct decisions (how many *cats* are correctly classified as *cats*?) (sensitivity); and -->\n<!-- * how often it makes correct positive calls (how many objects classified as *cats* are indeed *cats*?) (equivalentely, how many *not cats* are **not called cats**?) (specificity).  -->\n\n<!-- We refer back to the `vaso` data. We train both an LDA and a QDA classifier. We can  -->\n<!-- derive the associated sensitivity and specificity from the misclassification table. Note -->\n<!-- that since there is no independent test set these figures may be misleading.  -->\n\n<!-- ```{r sens1, fig.width=6, fig.height=6, message=FALSE, warning=FALSE} -->\n<!-- a <- lda(Y ~ . , data=vaso) -->\n<!-- a.qda <- qda(Y ~ . , data=vaso) -->\n<!-- pr.lda <- as.numeric(predict(a)$class) -->\n<!-- pr.qda <- as.numeric(predict(a.qda)$class) -->\n<!-- table(pr.lda, vaso$Y) -->\n<!-- table(pr.qda, vaso$Y) -->\n<!-- ``` -->\n\n<!-- Hence we can estimate the sensitivities of LDA and QDA as 17/19 and 16/19  -->\n<!-- respectively. Their specificities are both 16/20.  -->\n\n<!-- # sensitivity  -->\n<!-- # LDA: 16/20 = 4/5 -->\n<!-- # QDA: 16/20  -->\n\n<!-- # specificity -->\n<!-- # LDA: 17/19 -->\n<!-- # QDA: 16/19 -->\n\n\n<!-- For the zip code data: -->\n\n<!-- ```{r zip3, fig.width=6, fig.height=6, message=FALSE, warning=FALSE} -->\n<!-- data(zip.train, package='ElemStatLearn')  -->\n<!-- data(zip.test, package='ElemStatLearn') -->\n<!-- x.tr <- data.frame( zip.train[ zip.train[, 1] %in% c(3, 8), ] ) -->\n<!-- x.te <- data.frame( zip.test[ zip.test[, 1] %in% c(3, 8), ] ) -->\n<!-- names( x.te ) <- names( x.tr  ) <- paste('V', 1:257, sep='') -->\n<!-- a <- lda(V1 ~ . - V257, data=x.tr) -->\n<!-- te.lda <- as.numeric(predict(a, newdata=x.te)$class) -->\n<!-- table(te.lda, x.te$V1) -->\n<!-- ``` -->\n<!-- # sensitivity - test -->\n<!-- # 350/ 359 = 97.4% -->\n\n<!-- # specificity - test -->\n<!-- # 160 / 166 = 96.4% -->\n\n<!-- # build the ROC curve -->\n\n<!-- te.lda <- predict(a, newdata=x.te)$posterior[,1] -->\n<!-- sens <- spec <- rep(0, 50) -->\n<!-- als <- seq(0, 1, length=51) -->\n<!-- for(i in 1:50) { -->\n<!--   npr.1 <- (te.lda > als[i]) -->\n<!--   npr.2 <- !npr.1 -->\n<!--   sens[i] <- sum( (as.numeric(as.factor(x.te$V1)) == 1) & npr.1 ) -->\n<!--   spec[i] <- sum( (as.numeric(as.factor(x.te$V1)) == 2) & npr.2 ) -->\n<!-- } -->\n<!-- sens <- sens / sum(as.numeric(as.factor(x.te$V1)) == 1) -->\n<!-- spec <- spec / sum(as.numeric(as.factor(x.te$V1)) == 2) -->\n<!-- plot(1-spec, sens, type='b', ylim=c(0,1), xlim=c(0,1)) -->\n\n\n## K-Nearest Neighbours (K-NN)\n\nPerhaps the intuitively simplest model-free estimator for conditional class probabilities \nfor a given set of feature values **X** is the one based on nearest neighbours\n(as discussed in class). It is similar (in *spirit*) to the kernel regression\nestimator in the continuous-response regression setting. More specifically,\nit can be thought of as a variable-bandwidth kernel estimator. For a \npoint **X** in the feature space we look at the proportion of observations\nin each class among **X**'s K-th closest neighbours. That is, of course, equivalent\nto looking at all points $(Y_i, \\mathbf{X}_i)$ in the training set such that \n$\\left\\| \\mathbf{X}_i - \\mathbf{X} \\right\\| \\le h_k$, where $h_k$ is the \ndistance from **X** to the K-th closest neighbour in the training set.\nRefer to the discussion in class for more details. \n\nHere we will illustrate K-NN classifiers on the toy `vaso` example (to be able to\nvisualize the results more easily), and also on the hand written digits data. \nWe will use the function `knn` in package `class`. This function takes a \ntraining set, and also a *test* set (i.e. a different data set containing the\nobservations to be predicted). In the example below we first create (as \nwe have done before) a 200 x 200 grid of points and display the resulting\npredicted probabilities (or the corresponding class with highest conditional\nprobability). \n\nWe first we use a trivial 1-NN classifier: the estimated conditional probabilities\nfor each class at a point **X**, will simply be 0 or 1 depending on the class of the closest\nneighbour to **X** in the training set. \n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/knn1_12962759d814dfa8ec12e551a6cd496c'}\n\n```{.r .cell-code}\nlibrary(class)\ndata(vaso, package = \"robustbase\")\nx1 <- seq(0, 4, length = 200)\nx2 <- seq(0, 4, length = 200)\nxx <- expand.grid(x1, x2)\nu1 <- knn(train = vaso[, c(2, 1)], cl = vaso[, 3], test = xx, k = 1)\nu1 <- as.numeric(u1)\nimage(x1, x2, matrix(u1, 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", main = \"1-NN\"\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\n```\n\n::: {.cell-output-display}\n![](31-qda-knn_files/figure-html/knn1-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\nWe repeat the analysis with a 5-NN classifier. Now the estimated \nconditional probabilities \nfor each **X** in the grid can be 0, 0.20, 0.40, 0.60, 0.80 or 1 (why?)\nThe function `knn` returns the estimated probabilities in the \n`'prob'` attribute of the returned object, so we need to use the\nfunction `attr` to extract it (as usual, the R help pages are \na good source of information if you have any questions about the \ncode below): \n\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/knn5_5bc9f375d059225fce9ce181d1a5a0b5'}\n\n```{.r .cell-code}\nu5 <- attr(\n  knn(train = vaso[, c(2, 1)], cl = vaso[, 3], test = xx, k = 5, prob = TRUE),\n  \"prob\"\n)\nimage(x1, x2, matrix(u5, 200, 200),\n  col = terrain.colors(100),\n  ylab = \"Volume\", xlab = \"Rate\", main = \"5-NN\"\n)\npoints(Volume ~ Rate,\n  data = vaso, pch = 19, cex = 1.5,\n  col = c(\"red\", \"blue\")[Y + 1]\n)\n```\n\n::: {.cell-output-display}\n![](31-qda-knn_files/figure-html/knn5-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n\nWe now turn to the digits data. We now look at the images \nfor digits 1, 3 and 8 and create the corresponding \ntraining and test sets:\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/knndig1_ebd3f9179cd2b70d0499d61e54b34e7b'}\n\n```{.r .cell-code}\ndata(zip.train, package = \"ElemStatLearn\")\ndata(zip.test, package = \"ElemStatLearn\")\nx.tr <- data.frame(zip.train[zip.train[, 1] %in% c(1, 3, 8), ])\nx.te <- data.frame(zip.test[zip.test[, 1] %in% c(1, 3, 8), ])\nnames(x.te) <- names(x.tr) <- paste(\"V\", 1:257, sep = \"\")\n```\n:::\n\nWe now train 1-, 5-, 10- and 50-NN classifiers and evaluate them on\nthe test set. We report the misclassification rate on the test set, \nalong with the corresponding tables:\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/knndig2_2135ec3654d7d385d03e3978e12bd2ab'}\n\n```{.r .cell-code}\nu1 <- knn(train = x.tr[, -1], cl = x.tr[, 1], test = x.te[, -1], k = 1)\ntable(u1, x.te$V1)\n#>    \n#> u1    1   3   8\n#>   1 261   0   0\n#>   3   3 162   9\n#>   8   0   4 157\nmean(u1 != x.te$V1)\n#> [1] 0.02684564\n\nu5 <- knn(train = x.tr[, -1], cl = x.tr[, 1], test = x.te[, -1], k = 5)\ntable(u5, x.te$V1)\n#>    \n#> u5    1   3   8\n#>   1 261   1   0\n#>   3   3 161   7\n#>   8   0   4 159\nmean(u5 != x.te$V1)\n#> [1] 0.02516779\n\nu10 <- knn(train = x.tr[, -1], cl = x.tr[, 1], test = x.te[, -1], k = 10)\ntable(u10, x.te$V1)\n#>    \n#> u10   1   3   8\n#>   1 261   1   3\n#>   3   3 163  12\n#>   8   0   2 151\nmean(u10 != x.te$V1)\n#> [1] 0.0352349\n\nu50 <- knn(train = x.tr[, -1], cl = x.tr[, 1], test = x.te[, -1], k = 50)\ntable(u50, x.te$V1)\n#>    \n#> u50   1   3   8\n#>   1 261   2   7\n#>   3   3 159  18\n#>   8   0   5 141\nmean(u50 != x.te$V1)\n#> [1] 0.05872483\n```\n:::\n\nNote how the performance of the K-NN classifier in this case \nstops improving when K is larger than 5. Since the number *K* of \nnearest neighbours is in fact a tuning constant that needs to\nbe chosen by the user, how would do it in an objective way? \nWhat would you do if you didn't have a test set available? \n\n\n## Challenges for K-NN classifiers\n\n* It is easy to see that they suffer from the *curse of dimensionality*.\n* Factor or binary features need to be treated with care.\n* Euclidean distances do not reflect *shape* of features in each class (i.e. the\nconditional distribution of **X** in each class). Class-wise pre-standardization \n(whitening) might be useful. \n\nTo illustrate the last point, consider this toy synthetic example we discussed in class:\n\n\n::: {.cell layout-align=\"center\" hash='31-qda-knn_cache/html/knntrouble_75b58612bdcbe562d3753bee8e5800d4'}\n\n```{.r .cell-code}\n# create example\nset.seed(123)\nx <- matrix(runif(250 * 2, min = -1, max = 1), 250, 2)\nnorm2 <- function(a) sqrt(sum(a^2))\nr <- apply(x, 1, norm2)\na <- (r > .4) & (r < .7)\nx <- x[a, ]\n# plot(x, xlim=c(-1,1), ylim=c(-1,1))\nl1 <- (x[, 1] > 0)\nl2 <- (x[, 2] > 0)\na <- l1 & !l2\nb <- l1 & l2\nd <- !l1 & l2\nla <- rep(\"C\", nrow(x))\nla[a] <- \"A\"\nla[b] <- \"B\"\nla[d] <- \"D\"\n# plot(x, pch=la)\nx2 <- x\nx2[, 1] <- x2[, 1] * 1e5\n\n\n# plot(x2, pch=la, cex=1.5)\n#\n# # pick a point\n# points(x2[26,1], x2[26, 2], pch='A', col='red', cex=1.9)\n\n# find closest neighbour\nx0 <- x2[26, ]\nd <- apply(scale(x2, center = x0, scale = FALSE), 1, norm2)\nh <- sort(d)[2]\ne <- (1:nrow(x2))[d == h]\nplot(x2, pch = la, cex = 1.5, xlab = expression(X[1]), ylab = expression(X[2]))\npoints(x2[26, 1], x2[26, 2], pch = \"A\", col = \"red\", cex = 1.9)\npoints(x2[e, 1], x2[e, 2], pch = \"O\", col = \"red\", cex = 1.9)\ntext(-5000, 0, labels = \"Closest neighbour\", cex = 1.5, col = \"red\")\narrows(x2[26, 1], x2[26, 2] + .1, x2[e, 1], x2[e, 2] - .1, lwd = 5, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](31-qda-knn_files/figure-html/knntrouble-1.png){fig-align='center' width=90%}\n:::\n\n```{.r .cell-code}\n\n# pdf('knn-challenge.pdf', bg='transparent')\n# plot(x2, pch=la, cex=1.5, col='gray30', xlab='', ylab='')\n# points(x2[26,1], x2[26, 2], pch='A', col='red', cex=1.9)\n# points(x2[e,1], x2[e, 2], pch=19, col='red', cex=3)\n# arrows(x2[26, 1], x2[26,2] + .15, x2[e,1], x2[e,2]-.15, lwd=7, col='red')\n# text(-5000, 0, labels='Closest neighbour', cex=1.5, col='red')\n# dev.off()\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}