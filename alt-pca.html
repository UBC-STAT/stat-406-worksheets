<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>A PCA and alternating regression | UBC Stat 406 Worksheets</title>
<meta name="author" content="Daniel J. McDonald and Matías Salibián-Barrera">
<meta name="description" content="Let \(X_1, \ldots, X_n \in \mathbb{R}^p\) be the observations for which we want to compute the corresponding PCA. Without loss of generality we can always assume that \[ \frac{1}{n} \sum_{i=1}^n...">
<meta name="generator" content="bookdown 0.35 with bs4_book()">
<meta property="og:title" content="A PCA and alternating regression | UBC Stat 406 Worksheets">
<meta property="og:type" content="book">
<meta property="og:url" content="https://ubc-stat.github.io/stat-406-worksheets/alt-pca.html">
<meta property="og:description" content="Let \(X_1, \ldots, X_n \in \mathbb{R}^p\) be the observations for which we want to compute the corresponding PCA. Without loss of generality we can always assume that \[ \frac{1}{n} \sum_{i=1}^n...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A PCA and alternating regression | UBC Stat 406 Worksheets">
<meta name="twitter:description" content="Let \(X_1, \ldots, X_n \in \mathbb{R}^p\) be the observations for which we want to compute the corresponding PCA. Without loss of generality we can always assume that \[ \frac{1}{n} \sum_{i=1}^n...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.1/transition.js"></script><script src="libs/bs3compat-0.5.1/tabs.js"></script><script src="libs/bs3compat-0.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">UBC Stat 406 Worksheets</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Stat 406 Worksheets</a></li>
<li><a class="" href="preface.html">Preface</a></li>
<li class="book-part">Module 0 – Review</li>
<li><a class="" href="predictions-using-a-linear-model.html"><span class="header-section-number">1</span> Predictions using a linear model</a></li>
<li class="book-part">Module 1 – Model Selection</li>
<li><a class="" href="predictions-using-a-linear-model-continued.html"><span class="header-section-number">2</span> Predictions using a linear model (continued)</a></li>
<li><a class="" href="cross-validation-concerns.html"><span class="header-section-number">3</span> Cross-validation concerns</a></li>
<li><a class="" href="comparing-models.html"><span class="header-section-number">4</span> Comparing models</a></li>
<li class="book-part">Module 2 – Regression</li>
<li><a class="" href="ridge-regression.html"><span class="header-section-number">5</span> Ridge regression</a></li>
<li><a class="" href="lasso.html"><span class="header-section-number">6</span> LASSO</a></li>
<li><a class="" href="non-parametric-regression.html"><span class="header-section-number">7</span> Non-parametric regression</a></li>
<li><a class="" href="kernel-regression-local-regression.html"><span class="header-section-number">8</span> Kernel regression / local regression</a></li>
<li><a class="" href="regression-trees.html"><span class="header-section-number">9</span> Regression trees</a></li>
<li><a class="" href="pruning-regression-trees-with-rpart.html"><span class="header-section-number">10</span> Pruning regression trees with rpart</a></li>
<li class="book-part">Module 3 – Classification</li>
<li><a class="" href="parametric-classifiers.html"><span class="header-section-number">11</span> Parametric classifiers</a></li>
<li><a class="" href="qda.html"><span class="header-section-number">12</span> QDA</a></li>
<li><a class="" href="classification-trees.html"><span class="header-section-number">13</span> Classification Trees</a></li>
<li class="book-part">Module 4 – Modern techniques</li>
<li><a class="" href="bagging-for-regression.html"><span class="header-section-number">14</span> Bagging for regression</a></li>
<li><a class="" href="bagging-for-classification.html"><span class="header-section-number">15</span> Bagging for classification</a></li>
<li><a class="" href="random-forests.html"><span class="header-section-number">16</span> Random Forests</a></li>
<li><a class="" href="boosting-a-statistical-learning-perspective.html"><span class="header-section-number">17</span> Boosting (a Statistical Learning perspective)</a></li>
<li><a class="" href="what-is-adaboost-doing-really.html"><span class="header-section-number">18</span> What is Adaboost doing, really?</a></li>
<li><a class="" href="single-layer-neural-network.html"><span class="header-section-number">19</span> Single layer neural network</a></li>
<li class="book-part">Module 5 – Unsupervised learning</li>
<li><a class="" href="introduction.html"><span class="header-section-number">20</span> Introduction</a></li>
<li><a class="" href="clustering.html"><span class="header-section-number">21</span> Clustering</a></li>
<li><a class="" href="model-based-clustering.html"><span class="header-section-number">22</span> Model based clustering</a></li>
<li><a class="" href="hierarchical-clustering.html"><span class="header-section-number">23</span> Hierarchical clustering</a></li>
<li><a class="" href="references.html">References</a></li>
<li class="book-part">Appendix</li>
<li><a class="active" href="alt-pca.html"><span class="header-section-number">A</span> PCA and alternating regression</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/ubc-stat/stat-406-worksheets">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="alt-pca" class="section level1" number="24">
<h1>
<span class="header-section-number">A</span> PCA and alternating regression<a class="anchor" aria-label="anchor" href="#alt-pca"><i class="fas fa-link"></i></a>
</h1>
<p>Let <span class="math inline">\(X_1, \ldots, X_n \in \mathbb{R}^p\)</span> be the observations for which we
want to compute the corresponding PCA. Without loss of generality we can
always assume that
<span class="math display">\[
\frac{1}{n} \sum_{i=1}^n X_i \ = (0,\ldots,0)^\top \, ,
\]</span>
so that the sample covariance matrix <span class="math inline">\(S_n\)</span> is
<span class="math display">\[
S_n \ = \ \frac{1}{n-1} \, \sum_{i=1}^n X_i \, X_i^\top \, .
\]</span>
We saw in class that if <span class="math inline">\(B \in \mathbb{R}^{p \times k}\)</span> has in its
columns the eigenvectors of <span class="math inline">\(S_n\)</span> associated with its <span class="math inline">\(k\)</span> largest
eigenvalues, then
<span class="math display">\[
\frac{1}{n} \, \sum_{i=1}^n \left\| X_i - P( L_{B}, X_i
) \right\|^2 \ \le \
\frac{1}{n} \, \sum_{i=1}^n \left\| X_i - P( L, X_i
) \right\|^2 \, ,
\]</span>
for any <span class="math inline">\(k\)</span>-dimensional linear subspace <span class="math inline">\(L \subset \mathbb{R}^p\)</span>
where <span class="math inline">\(P( L, X)\)</span> denotes the orthogonal projection of <span class="math inline">\(X\)</span> onto the
subspace <span class="math inline">\(L\)</span>, <span class="math inline">\(P( L_{B}, X) = {B} {B}^\top X\)</span> (whenever <span class="math inline">\({B}\)</span> is chosen
so that <span class="math inline">\({B}^\top {B} = I\)</span>) and <span class="math inline">\(L_{B}\)</span> denotes the subspace spanned by
the columns of <span class="math inline">\(B\)</span>.</p>
<p>We will show now that, instead of finding the spectral decomposition of
<span class="math inline">\(S_n\)</span>, principal components can also be computed via a sequence of
“alternating least squares” problems. To fix ideas we will consider the
case <span class="math inline">\(k=1\)</span>, but the method is trivially extended to arbitrary values of
<span class="math inline">\(k\)</span>.</p>
<p>When <span class="math inline">\(k=1\)</span> we need to solve the following problem
<span class="math display" id="eq:pca1">\[\begin{equation}
\min_{\left\| a \right\|=1, v \in \mathbb{R}^n} \
\sum_{i=1}^n \left\| X_i - a \, v_i \right\|^2,
\tag{A.1}
\end{equation}\]</span>\end{equation}
where <span class="math inline">\(v = (v_1, \ldots v_n)^\top\)</span> (in general, for any <span class="math inline">\(k\)</span> we have
<span class="math display">\[
\min_{ A^\top A = I, v_1, \ldots, v_n \in \mathbb{R}^k} \
\sum_{i=1}^n \left\| X_i - A \, v_i \right\|^2 \, ).
\]</span>
The objective function in Equation <a href="alt-pca.html#eq:pca1">(A.1)</a> can also be written
as
<span class="math display" id="eq:pca2">\[\begin{equation}
\sum_{i=1}^n \sum_{j=1}^p \left( X_{i,j} - a_j \, v_i \right)^2 \, , \tag{A.2}
\end{equation}\]</span>
and hence, for a given vector <span class="math inline">\(a\)</span>, the minimizing values of
<span class="math inline">\(v_1, \ldots, v_n\)</span> in Equation <a href="alt-pca.html#eq:pca2">(A.2)</a> can be found solving <span class="math inline">\(n\)</span>
separate least squares problems:
<span class="math display">\[
v_\ell \, = \, \arg \,  \min_{d \in \mathbb{R}}
\sum_{j=1}^p \left( X_{\ell,j} - a_j \, d \right)^2 \, , \qquad
\ell = 1, \ldots, n \, .
\]</span>
Similarly, for a given set <span class="math inline">\(v_1, \ldots, v_n\)</span> the entries of <span class="math inline">\(a\)</span> can
be found solving <span class="math inline">\(p\)</span> separate least squares problems: <span class="math display">\[
a_r \, = \, \arg \,  \min_{d \in \mathbb{R}}
\sum_{i=1}^n \left( X_{i, r} - d \, v_i \right)^2 \, , \qquad
r = 1, \ldots, p \, .
\]</span> We can then set <span class="math inline">\(a \leftarrow a / \| a \|\)</span> and iterate to find new
<span class="math inline">\(v\)</span>’s, then a new <span class="math inline">\(a\)</span>, etc.</p>

</div>
  <div class="chapter-nav">
<div class="prev"><a href="references.html">References</a></div>
<div class="empty"></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav"><li><a class="nav-link" href="#alt-pca"><span class="header-section-number">A</span> PCA and alternating regression</a></li></ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/ubc-stat/stat-406-worksheets/blob/main/90-pca-alternating-regression.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/ubc-stat/stat-406-worksheets/edit/main/90-pca-alternating-regression.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>UBC Stat 406 Worksheets</strong>" was written by Daniel J. McDonald and Matías Salibián-Barrera. It was last built on 2023-08-16.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
